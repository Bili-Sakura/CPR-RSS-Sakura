<?xml version="1.0" encoding="utf8"?>
<rss version="2.0">
<channel>
    <title>interspeech 2017</title>
    
    <item>
        <title>ISCA Medal for Scientific Achievement</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/3001.PDF</link>
        <description>The ISCA Medal for Scientific Achievement 2017 will be awarded to Professor
Fumitada Itakura by the President of ISCA during the opening ceremony.
</description>
    </item>
    
    <item>
        <title>The ASVspoof 2017 Challenge: Assessing the Limits of Replay Spoofing Attack Detection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1111.PDF</link>
        <description>The ASVspoof initiative was created to promote the development of countermeasures
which aim to protect automatic speaker verification (ASV) from spoofing
attacks. The first community-led, common evaluation held in 2015 focused
on countermeasures for speech synthesis and voice conversion spoofing
attacks. Arguably, however, it is replay attacks which pose the greatest
threat. Such attacks involve the replay of recordings collected from
enrolled speakers in order to provoke false alarms and can be mounted
with greater ease using everyday consumer devices. ASVspoof 2017, the
second in the series, hence focused on the development of replay attack
countermeasures. This paper describes the database, protocols and initial
findings. The evaluation entailed highly heterogeneous acoustic recording
and replay conditions which increased the equal error rate (EER) of
a baseline ASV system from 1.76% to 31.46%. Submissions were received
from 49 research teams, 20 of which improved upon a baseline replay
spoofing detector EER of 24.77%, in terms of replay/non-replay discrimination.
While largely successful, the evaluation indicates that the quest for
countermeasures which are resilient in the face of variable replay
attacks remains very much alive.
</description>
    </item>
    
    <item>
        <title>Experimental Analysis of Features for Replay Attack Detection &amp;#8212; Results on the ASVspoof 2017 Challenge</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0450.PDF</link>
        <description>This paper presents an experimental comparison of different features
for the detection of replay spoofing attacks in Automatic Speaker Verification
systems. We evaluate the proposed countermeasures using two recently
introduced databases, including the dataset provided for the ASVspoof
2017 challenge. This challenge provides researchers with a common framework
for the evaluation of replay attack detection systems, with a particular
focus on the generalization to new, unknown conditions (for instance,
replay devices different from those used during system training). Our
cross-database experiments show that, although achieving this level
of generalization is indeed a challenging task, it is possible to train
classifiers that exhibit stable and consistent results across different
experiments. The proposed approach for the ASVspoof 2017 challenge
consists in the score-level fusion of several base classifiers using
logistic regression. These base classifiers are 2-class Gaussian Mixture
Models (GMMs) representing  genuine and  spoofed speech respectively.
Our best system achieves an Equal Error Rate of 10.52% on the challenge
evaluation set. As a result of this set of experiments, we provide
some general conclusions regarding feature extraction for replay attack
detection and identify which features show the most promising results.
</description>
    </item>
    
    <item>
        <title>Novel Variable Length Teager Energy Separation Based Instantaneous Frequency Features for Replay Detection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1362.PDF</link>
        <description>Replay attacks presents a great risk for Automatic Speaker Verification
(ASV) system. In this paper, we propose a novel replay detector based
on Variable length Teager Energy Operator-Energy Separation Algorithm-Instantaneous
Frequency Cosine Coefficients (VESA-IFCC) for the ASV spoof 2017 challenge.
The key idea here is to exploit the contribution of IF in each subband
energy via ESA to capture possible changes in spectral envelope (due
to transmission and channel characteristics of replay device) of replayed
speech. The IF is computed from narrowband components of speech signal,
and DCT is applied in IF to get proposed feature set. We compare the
performance of the proposed VESA-IFCC feature set with the features
developed for detecting synthetic and voice converted speech. This
includes the CQCC, CFCCIF and prosody-based features. On the development
set, the proposed VESA-IFCC features when fused at score-level with
a variant of CFCCIF and prosody-based features gave the least EER of
0.12%. On the evaluation set, this combination gave an EER of 18.33%.
However, post-evaluation results of challenge indicate that VESA-IFCC
features alone gave the relatively least EER of 14.06% (i.e., relatively
16.11% less compared to baseline CQCC) and hence, is a very useful
countermeasure to detect replay attacks.
</description>
    </item>
    
    <item>
        <title>Countermeasures for Automatic Speaker Verification Replay Spoofing Attack : On Data Augmentation, Feature Representation, Classification and Fusion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0906.PDF</link>
        <description>The ongoing ASVspoof 2017 challenge aims to detect replay attacks for
text dependent speaker verification. In this paper, we propose multiple
replay spoofing countermeasure systems, with some of them boosting
the CQCC-GMM baseline system after score level fusion. We investigate
different steps in the system building pipeline, including data augmentation,
feature representation, classification and fusion. First, in order
to augment training data and simulate the unseen replay conditions,
we converted the raw genuine training data into replay spoofing data
with parametric sound reverberator and phase shifter. Second, we employed
the original spectrogram rather than CQCC as input to explore the end-to-end
feature representation learning methods. The spectrogram is randomly
cropped into fixed size segments, and then fed into a deep residual
network (ResNet). Third, upon the CQCC features, we replaced the subsequent
GMM classifier with deep neural networks including fully-connected
deep neural network (FDNN) and Bidirectional Long Short Term Memory
neural network (BLSTM). Experiments showed that data augmentation strategy
can significantly improve the system performance. The final fused system
achieves to 16.39% EER on the test set of ASVspoof 2017 for the common
task.
</description>
    </item>
    
    <item>
        <title>Spoof Detection Using Source, Instantaneous Frequency and Cepstral Features</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0930.PDF</link>
        <description>This work describes the techniques used for spoofed speech detection
for the ASVspoof 2017 challenge. The main focus of this work is on
exploiting the differences in the speech-specific nature of genuine
speech signals and spoofed speech signals generated by replay attacks.
This is achieved using glottal closure instants, epoch strength, and
the peak to side lobe ratio of the Hilbert envelope of linear prediction
residual. Apart from these source features, the instantaneous frequency
cosine coefficient feature, and two cepstral features namely, constant
Q cepstral coefficients and mel frequency cepstral coefficients are
used. A combination of all these features is performed to obtain a
high degree of accuracy for spoof detection. Initially, efficacy of
these features are tested on the development set of the ASVspoof 2017
database with Gaussian mixture model based systems. The systems are
then fused at score level which acts as the final combined system for
the challenge. The combined system is able to outperform the individual
systems by a significant margin. Finally, the experiments are repeated
on the evaluation set of the database and the combined system results
in an equal error rate of 13.95%.
</description>
    </item>
    
    <item>
        <title>Audio Replay Attack Detection Using High-Frequency Features</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0776.PDF</link>
        <description>This paper presents our contribution to the ASVspoof 2017 Challenge.
It addresses a replay spoofing attack against a speaker recognition
system by detecting that the analysed signal has passed through multiple
analogue-to-digital (AD) conversions. Specifically, we show that most
of the cues that enable to detect the replay attacks can be found in
the high-frequency band of the replayed recordings. The described anti-spoofing
countermeasures are based on (1) modelling the subband spectrum and
(2) using the proposed features derived from the linear prediction
(LP) analysis. The results of the investigated methods show a significant
improvement in comparison to the baseline system of the ASVspoof 2017
Challenge. A relative equal error rate (EER) reduction by 70% was achieved
for the development set and a reduction by 30% was obtained for the
evaluation set.
</description>
    </item>
    
    <item>
        <title>Feature Selection Based on CQCCs for Automatic Speaker Verification Spoofing</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0304.PDF</link>
        <description>The ASVspoof 2017 challenge aims to assess spoofing and countermeasures
attack detection accuracy for automatic speaker verification. It has
been proven that constant Q cepstral coefficients (CQCCs) processes
speech in different frequencies with variable resolution and performs
much better than traditional features. When coupled with a Gaussian
mixture model (GMM), it is an excellently effective spoofing countermeasure.
The baseline CQCC+GMM system considers short-term impacts while ignoring
the whole influence of channel. In the meanwhile, dimension of the
feature is relatively higher than the traditional feature and usually
with a higher variance. This paper explores different features for
ASVspoof 2017 challenge. The mean and variance of the CQCC features
of an utterance is used as the representation of the whole utterance.
Feature selection method is introduced to avoid high variance and overfitting
for spoofing detection. Experimental results on ASVspoof 2017 dataset
show that feature selection followed by Support Vector Machine (SVM)
gets an improvement compared to the baseline. It is also shown that
pitch feature contributes to the performance improvement, and it obtains
a relative improvement of 37.39% over the baseline CQCC+GMM system.
</description>
    </item>
    
    <item>
        <title>Longitudinal Speaker Clustering and Verification Corpus with Code-Switching Frisian-Dutch Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0301.PDF</link>
        <description>In this paper, we present a new longitudinal and bilingual broadcast
database designed for speaker clustering and text-independent verification
research. The broadcast data is extracted from the archives of Omrop
Frysl&amp;#226;n which is the regional broadcaster in the province of Frysl&amp;#226;n,
located in the north of the Netherlands. Two speaker verification tasks
are provided in a standard enrollment-test setting with language consistent
trials. The first task contains target trials from all speakers available
appearing in at least two different programs, while the second task
contains target trials from a subgroup of speakers appearing in programs
recorded in multiple years. The second task is designed to investigate
the effects of ageing on the accuracy of speaker verification systems.
This database also contains unlabeled spoken segments from different
radio programs for speaker clustering research. We provide the output
of an existing speaker diarization system for baseline verification
experiments. Finally, we present the baseline speaker verification
results using the Kaldi GMM- and DNN-UBM speaker verification system.
This database will be an extension to the recently presented open source
Frisian data collection and it is publicly available for research purposes.
</description>
    </item>
    
    <item>
        <title>Exploiting Untranscribed Broadcast Data for Improved Code-Switching Detection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0391.PDF</link>
        <description>We have recently presented an automatic speech recognition (ASR) system
operating on Frisian-Dutch code-switched speech. This type of speech
requires careful handling of unexpected language switches that may
occur in a single utterance. In this paper, we extend this work by
using some raw broadcast data to improve multilingually trained deep
neural networks (DNN) that have been trained on 11.5 hours of manually
annotated bilingual speech. For this purpose, we apply the initial
ASR to the untranscribed broadcast data and automatically create transcriptions
based on the recognizer output using different language models for
rescoring. Then, we train new acoustic models on the combined data,
i.e., the manually and automatically transcribed bilingual broadcast
data, and investigate the automatic transcription quality based on
the recognition accuracies on a separate set of development and test
data. Finally, we report code-switching detection performance elaborating
on the correlation between the ASR and the code-switching detection
performance.
</description>
    </item>
    
    <item>
        <title> Jee haan, I&amp;#8217;d like both, por favor: Elicitation of a Code-Switched Corpus of Hindi&amp;#8211;English and Spanish&amp;#8211;English Human&amp;#8211;Machine Dialog</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1198.PDF</link>
        <description>We present a database of code-switched conversational human&amp;#8211;machine
dialog in English&amp;#8211;Hindi and English&amp;#8211;Spanish. We leveraged
HALEF, an open-source standards-compliant cloud-based dialog system
to capture audio and video of bilingual crowd workers as they interacted
with the system. We designed conversational items with  intra-sentential
code-switched machine prompts, and examine its efficacy in eliciting
code-switched speech in a total of over 700 dialogs. We analyze various
characteristics of the code-switched corpus and discuss some considerations
that should be taken into account while collecting and processing such
data. Such a database can be leveraged for a wide range of potential
applications, including automated processing, recognition and understanding
of code-switched speech and language learning applications for new
language learners.
</description>
    </item>
    
    <item>
        <title>On Building Mixed Lingual Speech Synthesis Systems</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1244.PDF</link>
        <description>Codemixing &amp;#8212; phenomenon where lexical items from one language
are embedded in the utterance of another &amp;#8212; is relatively frequent
in multilingual communities. However, TTS systems today are not fully
capable of effectively handling such mixed content despite achieving
high quality in the monolingual case. In this paper, we investigate
various mechanisms for building mixed lingual systems which are built
using a mixture of monolingual corpora and are capable of synthesizing
such content. First, we explore the possibility of manipulating the
phoneme representation: using target word to source phone mapping with
the aim of emulating the native speaker intuition. We then present
experiments at the acoustic stage investigating training techniques
at both spectral and prosodic levels. Subjective evaluation shows that
our systems are capable of generating high quality synthesis in codemixed
scenarios.
</description>
    </item>
    
    <item>
        <title>Speech Synthesis for Mixed-Language Navigation Instructions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1259.PDF</link>
        <description>Text-to-Speech (TTS) systems that can read navigation instructions
are one of the most widely used speech interfaces today. Text in the
navigation domain may contain named entities such as location names
that are not in the language that the TTS database is recorded in.
Moreover, named entities can be compound words where individual lexical
items belong to different languages. These named entities may be transliterated
into the script that the TTS system is trained on. This may result
in incorrect pronunciation rules being used for such words. We describe
experiments to extend our previous work in generating code-mixed speech
to synthesize navigation instructions, with a mixed-lingual TTS system.
We conduct subjective listening tests with two sets of users, one being
students who are native speakers of an Indian language and very proficient
in English, and the other being drivers with low English literacy,
but familiarity with location names. We find that in both sets of users,
there is a significant preference for our proposed system over a baseline
system that synthesizes instructions in English.
</description>
    </item>
    
    <item>
        <title>Addressing Code-Switching in French/Algerian Arabic Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1373.PDF</link>
        <description>This study focuses on code-switching (CS) in French/Algerian Arabic
bilingual communities and investigates how speech technologies, such
as automatic data partitioning, language identification and automatic
speech recognition (ASR) can serve to analyze and classify this type
of bilingual speech. A preliminary study carried out using a corpus
of Maghrebian broadcast data revealed a relatively high presence of
CS Algerian Arabic as compared to the neighboring countries Morocco
and Tunisia. Therefore this study focuses on code switching produced
by bilingual Algerian speakers who can be considered native speakers
of both Algerian Arabic and French. A specific corpus of four hours
of speech from 8 bilingual French Algerian speakers was collected.
This corpus contains read speech and conversational speech in both
languages and includes stretches of code-switching. We provide a linguistic
description of the code-switching stretches in terms of intra-sentential
and inter-sentential switches, the speech duration in each language.
We report on some initial studies to locate French, Arabic and the
code-switched stretches, using ASR system word posteriors for this
pair of languages.
</description>
    </item>
    
    <item>
        <title>Metrics for Modeling Code-Switching Across Corpora</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1429.PDF</link>
        <description>In developing technologies for code-switched speech, it would be desirable
to be able to predict how much language mixing might be expected in
the signal and the regularity with which it might occur. In this work,
we offer various metrics that allow for the classification and visualization
of multilingual corpora according to the ratio of languages represented,
the probability of switching between them, and the time-course of switching.
Applying these metrics to corpora of different languages and genres,
we find that they display distinct probabilities and periodicities
of switching, information useful for speech processing of mixed-language
data.
</description>
    </item>
    
    <item>
        <title>Synthesising isiZulu-English Code-Switch Bigrams Using Word Embeddings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1437.PDF</link>
        <description>Code-switching is prevalent among South African speakers, and presents
a challenge to automatic speech recognition systems. It is predominantly
a spoken phenomenon, and generally does not occur in textual form.
Therefore a particularly serious challenge is the extreme lack of training
material for language modelling. We investigate the use of word embeddings
to synthesise isiZulu-to-English code-switch bigrams with which to
augment such sparse language model training data. A variety of word
embeddings are trained on a monolingual English web text corpus, and
subsequently queried to synthesise code-switch bigrams. Our evaluation
is performed on language models trained on a new, although small, English-isiZulu
code-switch corpus compiled from South African soap operas. This data
is characterised by fast, spontaneously spoken speech containing frequent
code-switching. We show that the augmentation of the training data
with code-switched bigrams synthesised in this way leads to a reduction
in perplexity.
</description>
    </item>
    
    <item>
        <title>Crowdsourcing Universal Part-of-Speech Tags for Code-Switching</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1663.PDF</link>
        <description>Code-switching is the phenomenon by which bilingual speakers switch
between multiple languages during communication. The importance of
developing language technologies for code-switching data is immense,
given the large populations that routinely code-switch. High-quality
linguistic annotations are extremely valuable for any NLP task, and
performance is often limited by the amount of high-quality labeled
data. However, little such data exists for code-switching. In this
paper, we describe crowd-sourcing universal part-of-speech tags for
the Miami Bangor Corpus of Spanish-English code-switched speech. We
split the annotation task into three subtasks: one in which a subset
of tokens are labeled automatically, one in which questions are specifically
designed to disambiguate a subset of high frequency words, and a more
general cascaded approach for the remaining data in which questions
are displayed to the worker following a decision tree structure. Each
subtask is extended and adapted for a multilingual setting and the
universal tagset. The quality of the annotation process is measured
using hidden check questions annotated with gold labels. The overall
agreement between gold standard labels and the majority vote is between
0.95 and 0.96 for just three labels and the average recall across part-of-speech
tags is between 0.87 and 0.99, depending on the task.
</description>
    </item>
    
    <item>
        <title>Audio Replay Attack Detection with Deep Learning Frameworks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0360.PDF</link>
        <description>Nowadays spoofing detection is one of the priority research areas in
the field of automatic speaker verification. The success of Automatic
Speaker Verification Spoofing and Countermeasures (ASVspoof) Challenge
2015 confirmed the impressive perspective in detection of unforeseen
spoofing trials based on speech synthesis and voice conversion techniques.
However, there is a small number of researches addressed to replay
spoofing attacks which are more likely to be used by non-professional
impersonators. This paper describes the Speech Technology Center (STC)
anti-spoofing system submitted for ASVspoof 2017 which is focused on
replay attacks detection. Here we investigate the efficiency of a deep
learning approach for solution of the mentioned-above task. Experimental
results obtained on the Challenge corpora demonstrate that the selected
approach outperforms current state-of-the-art baseline systems in terms
of spoofing detection quality. Our primary system produced an EER of
6.73% on the evaluation part of the corpora which is 72% relative improvement
over the ASVspoof 2017 baseline system.
</description>
    </item>
    
    <item>
        <title>Ensemble Learning for Countermeasure of Audio Replay Spoofing Attack in ASVspoof2017</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1246.PDF</link>
        <description>To enhance the security and reliability of automatic speaker verification
(ASV) systems, ASVspoof 2017 challenge focuses on the detection problem
of known and unknown audio replay attacks. We proposed an ensemble
learning classifier for CNCB team&amp;#8217;s submitted system scores,
which across uses a variety of acoustic features and classifiers. An
effective post-processing method is studied to improve the performance
of Constant Q cepstral coefficients (CQCC) and to form a base feature
set with some other classical acoustic features. We also proposed using
an ensemble classifier set, which includes multiple Gaussian Mixture
Model (GMM) based classifiers and two novel GMM mean supervector-Gradient
Boosting Decision Tree (GSV-GBDT) and GSV-Random Forest (GSV-RF) classifiers.
Experimental results have shown that the proposed ensemble learning
system can provide substantially better performance than baseline.
On common training condition of the challenge, Equal Error Rate (EER)
of primary system on development set is 1.5%, compared to baseline
10.4%. EER of primary system (S02 in ASVspoof 2017 board) on evaluation
data set are 12.3% (with only train dataset) and 10.8% (with train+dev
dataset), which are also much better than baseline 30.6% and 24.8%,
given by ASVSpoof 2017 organizer, with 59.7% and 56.4% relative performance
improvement.
</description>
    </item>
    
    <item>
        <title>A Study on Replay Attack and Anti-Spoofing for Automatic Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0456.PDF</link>
        <description>For practical automatic speaker verification (ASV) systems, replay
attack poses a true risk. By replaying a pre-recorded speech signal
of the genuine speaker, ASV systems tend to be easily fooled. An effective
replay detection method is therefore highly desirable. In this study,
we investigate a major difficulty in replay detection: the over-fitting
problem caused by variability factors in speech signal. An F-ratio
probing tool is proposed and three variability factors are investigated
using this tool: speaker identity, speech content and playback &amp;amp;
recording device. The analysis shows that device is the most influential
factor that contributes the highest over-fitting risk. A frequency
warping approach is studied to alleviate the over-fitting problem,
as verified on the ASV-spoof 2017 database.
</description>
    </item>
    
    <item>
        <title>Replay Attack Detection Using DNN for Channel Discrimination</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1377.PDF</link>
        <description>Voice is projected to be the next input interface for portable devices.
The increased use of audio interfaces can be mainly attributed to the
success of speech and speaker recognition technologies. With these
advances comes the risk of criminal threats where attackers are reportedly
trying to access sensitive information using diverse voice spoofing
techniques. Among them, replay attacks pose a real challenge to voice
biometrics. This paper addresses the problem by proposing a deep learning
architecture in tandem with low-level cepstral features. We investigate
the use of a deep neural network (DNN) to discriminate between the
different channel conditions available in the ASVSpoof 2017 dataset,
namely recording, playback and session conditions. The high-level feature
vectors derived from this network are used to discriminate between
genuine and spoofed audio. Two kinds of low-level features are utilized:
state-of-the-art constant-Q cepstral coefficients (CQCC), and our proposed
high-frequency cepstral coefficients (HFCC) that derive from the high-frequency
spectrum of the audio. The fusion of both features proved to be effective
in generalizing well across diverse replay attacks seen in the evaluation
of the ASVSpoof 2017 challenge, with an equal error rate of 11.5%,
that is 53% better than the baseline Gaussian Mixture Model (GMM) applied
on CQCC.
</description>
    </item>
    
    <item>
        <title>ResNet and Model Fusion for Automatic Spoofing Detection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1085.PDF</link>
        <description>Speaker verification systems have achieved great progress in recent
years. Unfortunately, they are still highly prone to different kinds
of spoofing attacks such as speech synthesis, voice conversion, and
fake audio recordings etc. Inspired by the success of ResNet in image
recognition, we investigated the effectiveness of using ResNet for
automatic spoofing detection. Experimental results on the ASVspoof2017
data set show that ResNet performs the best among all the single-model
systems. Model fusion is a good way to further improve the system performance.
Nevertheless, we found that if the same feature is used for different
fused models, the resulting system can hardly be improved. By using
different features and models, our best fused model further reduced
the Equal Error Rate (EER) by 18% relatively, compared with the best
single-model system.
</description>
    </item>
    
    <item>
        <title>SFF Anti-Spoofer: IIIT-H Submission for Automatic Speaker Verification Spoofing and Countermeasures Challenge 2017</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0676.PDF</link>
        <description>The ASVspoof 2017 challenge is about the detection of replayed speech
from human speech. The proposed system makes use of the fact that when
the speech signals are replayed, they pass through multiple channels
as opposed to original recordings. This channel information is typically
embedded in low signal to noise ratio regions. A speech signal processing
method with high spectro-temporal resolution is required to extract
robust features from such regions. The single frequency filtering (SFF)
is one such technique, which we propose to use for replay attack detection.
While SFF based feature representation was used at front-end, Gaussian
mixture model and bi-directional long short-term memory models are
investigated at the backend as classifiers. The experimental results
on ASVspoof 2017 dataset reveal that, SFF based representation is very
effective in detecting replay attacks. The score level fusion of back
end classifiers further improved the performance of the system which
indicates that both classifiers capture complimentary information.
</description>
    </item>
    
    <item>
        <title>Improved Single System Conversational Telephone Speech Recognition with VGG Bottleneck Features</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1513.PDF</link>
        <description>On small datasets, discriminatively trained bottleneck features from
deep networks commonly outperform more traditional spectral or cepstral
features. While these features are typically trained with small, fully-connected
networks, recent studies have used more sophisticated networks with
great success. We use the recent deep CNN (VGG) network for bottleneck
feature extraction &amp;#8212; previously used only for low-resource tasks
&amp;#8212; and apply it to the Switchboard English conversational telephone
speech task. Unlike features derived from traditional MLP networks,
the VGG features outperform cepstral features even when used with BLSTM
acoustic models trained on large amounts of data. We achieve the best
BBN single system performance when combining the VGG features with
a BLSTM acoustic model. When decoding with an n-gram language model,
which are used for deployable systems, we have a realistic production
system with a WER of 7.4%. This result is competitive with the current
state-of-the-art in the literature. While our focus is on realistic
single system performance, we further reduce the WER to 6.1% through
system combination and using expensive neural network language model
rescoring.
</description>
    </item>
    
    <item>
        <title>Student-Teacher Training with Diverse Decision Tree Ensembles</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0145.PDF</link>
        <description>Student-teacher training allows a large teacher model or ensemble of
teachers to be compressed into a single student model, for the purpose
of efficient decoding. However, current approaches in automatic speech
recognition assume that the state clusters, often defined by Phonetic
Decision Trees (PDT), are the same across all models. This limits the
diversity that can be captured within the ensemble, and also the flexibility
when selecting the complexity of the student model output. This paper
examines an extension to student-teacher training that allows for the
possibility of having different PDTs between teachers, and also for
the student to have a different PDT from the teacher. The proposal
is to train the student to emulate the logical context dependent state
posteriors of the teacher, instead of the frame posteriors. This leads
to a method of mapping frame posteriors from one PDT to another. This
approach is evaluated on three speech recognition tasks: the Tok Pisin
and Javanese low resource conversational telephone speech tasks from
the IARPA Babel programme, and the HUB4 English broadcast news task.
</description>
    </item>
    
    <item>
        <title>Embedding-Based Speaker Adaptive Training of Deep Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0460.PDF</link>
        <description>An embedding-based speaker adaptive training (SAT) approach is proposed
and investigated in this paper for deep neural network acoustic modeling.
In this approach, speaker embedding vectors, which are a constant given
a particular speaker, are mapped through a control network to layer-dependent
element-wise affine transformations to canonicalize the internal feature
representations at the output of hidden layers of a main network. The
control network for generating the speaker-dependent mappings are jointly
estimated with the main network for the overall speaker adaptive acoustic
modeling. Experiments on large vocabulary continuous speech recognition
(LVCSR) tasks show that the proposed SAT scheme can yield superior
performance over the widely-used speaker-aware training using i-vectors
with speaker-adapted input features.
</description>
    </item>
    
    <item>
        <title>Improving Deliverable Speech-to-Text Systems with Multilingual Knowledge Transfer</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>English Conversational Telephone Speech Recognition by Humans and Machines</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0405.PDF</link>
        <description>Word error rates on the Switchboard conversational corpus that just
a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently
5.8%, and are now believed to be within striking range of human performance.
This then raises two issues: what is human performance, and how far
down can we still drive speech recognition error rates? In trying to
assess human performance, we performed an independent set of measurements
on the Switchboard and CallHome subsets of the Hub5 2000 evaluation
and found that human accuracy may be considerably better than what
was earlier reported, giving the community a significantly harder goal
to achieve. We also report on our own efforts in this area, presenting
a set of acoustic and language modeling techniques that lowered the
WER of our system to 5.5%/10.3% on these subsets, which is a new performance
milestone (albeit not at what we measure to be human performance).
On the acoustic side, we use a score fusion of one LSTM with multiple
feature inputs, a second LSTM trained with speaker-adversarial multi-task
learning and a third convolutional residual net (ResNet). On the language
modeling side, we use word and character LSTMs and convolutional WaveNet-style
language models.
</description>
    </item>
    
    <item>
        <title>Comparing Human and Machine Errors in Conversational Speech Transcription</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1544.PDF</link>
        <description>Recent work in automatic recognition of conversational telephone speech
(CTS) has achieved accuracy levels comparable to human transcribers,
although there is some debate how to precisely quantify human performance
on this task, using the NIST 2000 CTS evaluation set. This raises the
question what systematic differences, if any, may be found differentiating
human from machine transcription errors. In this paper we approach
this question by comparing the output of our most accurate CTS recognition
system to that of a standard speech transcription vendor pipeline.
We find that the most frequent substitution, deletion and insertion
error types of both outputs show a high degree of overlap. The only
notable exception is that the automatic recognizer tends to confuse
filled pauses (&amp;#8220;uh&amp;#8221;) and backchannel acknowledgments (&amp;#8220;uhhuh&amp;#8221;).
Human tend not to make this error, presumably due to the distinctive
and opposing pragmatic functions attached to these words. Furthermore,
we quantify the correlation between human and machine errors at the
speaker level, and investigate the effect of speaker overlap between
training and test data. Finally, we report on an informal &amp;#8220;Turing
test&amp;#8221; asking humans to discriminate between automatic and human
transcription error cases.
</description>
    </item>
    
    <item>
        <title>Multimodal Markers of Persuasive Speech: Designing a Virtual Debate Coach</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0098.PDF</link>
        <description>The study presented in this paper is carried out to support debate
performance assessment in the context of debate skills training. The
perception of good performance as a debater is influenced by how believable
and convincing the debater&amp;#8217;s argumentation is. We identified
a number of features that are useful for explaining perceived properties
of persuasive speech and for defining rules and strategies to produce
and assess debate performance. We collected and analysed multimodal
and multisensory data of the trainees debate behaviour, and contrasted
it with those of skilled professional debaters. Observational, correlation
and machine learning studies were performed to identify multimodal
markers of persuasive speech and link them to experts&amp;#8217; assessments.
A combination of multimodal in- and out-of-domain debate data, and
various non-verbal, prosodic, lexical, linguistic and structural features
has been computed based on our analysis and assessed used to , and
several classification procedures has been applied achieving an accuracy
of 0.79 on spoken debate data.
</description>
    </item>
    
    <item>
        <title>Acoustic-Prosodic and Physiological Response to Stressful Interactions in Children with Autism Spectrum Disorder</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0179.PDF</link>
        <description>Social anxiety is a prevalent condition affecting individuals to varying
degrees. Research on autism spectrum disorder (ASD), a group of neurodevelopmental
disorders marked by impairments in social communication, has found
that social anxiety occurs more frequently in this population. Our
study aims to further understand the multimodal manifestation of social
stress for adolescents with ASD versus neurotypically developing (TD)
peers. We investigate this through objective measures of speech behavior
and physiology (mean heart rate) acquired during three tasks: a low-stress
conversation, a medium-stress interview, and a high-stress presentation.
Measurable differences are found to exist for speech behavior and heart
rate in relation to task-induced stress. Additionally, we find the
acoustic measures are particularly effective for distinguishing between
diagnostic groups. Individuals with ASD produced higher prosodic variability,
agreeing with previous reports. Moreover, the most informative features
captured an individual&amp;#8217;s vocal changes between low and high social-stress,
suggesting an interaction between vocal production and social stressors
in ASD.
</description>
    </item>
    
    <item>
        <title>A Stepwise Analysis of Aggregated Crowdsourced Labels Describing Multimodal Emotional Behaviors</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1278.PDF</link>
        <description>Affect recognition is a difficult problem that most often relies on
human annotated data to train automated systems. As humans perceive
emotion differently based on personality, cognitive state and past
experiences, it is important to collect rankings from multiple individuals
to assess the emotional content in corpora, which are later aggregated
with rules such as majority vote. With the increased use of crowdsourcing
services for perceptual evaluations, collecting large amount of data
is now feasible. It becomes important to question the amount of data
needed to create well-trained classifiers. How different are the aggregated
labels collected from five raters compared to the ones obtained from
twenty evaluators? Is it worthwhile to spend resources to increase
the number of evaluators beyond those used in conventional/laboratory
studies? This study evaluates the consensus labels obtained by incrementally
adding new evaluators during perceptual evaluations. Using majority
vote over categorical emotional labels, we compare the changes in the
aggregated labels starting with one rater, and finishing with 20 raters.
The large number of evaluators in a subset of the MSP-IMPROV database
and the ability to filter annotators by quality allows us to better
understand label aggregation as a function of the number of annotators.
</description>
    </item>
    
    <item>
        <title>An Information Theoretic Analysis of the Temporal Synchrony Between Head Gestures and Prosodic Patterns in Spontaneous Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0999.PDF</link>
        <description>We analyze the temporal co-ordination between head gestures and prosodic
patterns in spontaneous speech in a data-driven manner. For this study,
we consider head motion and speech data from 24 subjects while they
tell a fixed set of five stories. The head motion, captured using a
motion capture system, is converted to Euler angles and translations
in X, Y and Z-directions to represent head gestures. Pitch and short-time
energy in voiced segments are used to represent the prosodic patterns.
To capture the statistical relationship between head gestures and prosodic
patterns, mutual information (MI) is computed at various delays between
the two using data from 24 subjects in six native languages. The estimated
MI, averaged across all subjects, is found to be maximum when the head
gestures lag the prosodic patterns by 30msec. This is found to be true
when subjects tell stories in English as well as in their native language.
We observe a similar pattern in the root mean squared error of predicting
head gestures from prosodic patterns using Gaussian mixture model.
These results indicate that there could be an asynchrony between head
gestures and prosody during spontaneous speech where head gestures
follow the corresponding prosodic patterns.
</description>
    </item>
    
    <item>
        <title>Multimodal Prediction of Affective Dimensions via Fusing Multiple Regression Techniques</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1088.PDF</link>
        <description>This paper presents a multimodal approach to predict affective dimensions,
that makes full use of features from audio, video, Electrodermal Activity
(EDA) and Electrocardiogram (ECG) using three regression techniques
such as support vector regression (SVR), partial least squares regression
(PLS), and a deep bidirectional long short-term memory recurrent neural
network (DBLSTM-RNN) regression. Each of the three regression techniques
performs multimodal affective dimension prediction followed by a fusion
of different models on features of four modalities using a support
vector regression. A support vector regression is also applied for
a final fusion of the three regression systems. Experiments show that
our proposed approach obtains promising results on the AVEC 2015 benchmark
dataset for prediction of multimodal affective dimensions. For the
development set, the concordance correlation coefficient (CCC) achieves
results of 0.856 for arousal and 0.720 for valence, which increases
3.88% and 4.66% of the top-performer of AVEC 2015 in arousal and valence,
respectively.
</description>
    </item>
    
    <item>
        <title>Co-Production of Speech and Pointing Gestures in Clear and Perturbed Interactive Tasks: Multimodal Designation Strategies</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1329.PDF</link>
        <description>Designation consists in attracting an interlocutor&amp;#8217;s attention
on a specific object and/or location. It is most often achieved using
both speech (e.g., demonstratives) and gestures (e.g., manual pointing).
This study aims at analyzing how speech and pointing gestures are co-produced
in a semi-directed interactive task involving designation. 20 native
speakers of French were involved in a cooperative task in which they
provided instructions to a partner for her to reproduce a model she
could not see on a grid both of them saw. They had to use only sentences
of the form &amp;#8216;The [target word] goes there.&amp;#8217;. They did this
in two conditions: silence and noise. Their speech and articulatory/hand
movements (motion capture) were recorded. The analyses show that the
participants&amp;#8217; speech features were modified in noise (Lombard
effect). They also spoke slower and made more pauses and errors. Their
pointing gestures lasted longer and started later showing an adaptation
of gesture production to speech. The condition did not influence speech/gesture
coordination. The apex (part of the gesture that shows) mainly occurred
at the same time as the target word and not as the demonstrative showing
that speakers group speech and gesture carrying complementary rather
than redundant information.
</description>
    </item>
    
    <item>
        <title>Improving Speaker Verification for Reverberant Conditions with Deep Neural Network Dereverberation Processing</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0461.PDF</link>
        <description>We present an improved method for training Deep Neural Networks for
dereverberation and show that it can improve performance for the speech
processing tasks of speaker verification and speech enhancement. We
replicate recently proposed methods for dereverberation using Deep
Neural Networks and present our improved method, highlighting important
aspects that influence performance. We then experimentally evaluate
the capabilities and limitations of the method with respect to speech
quality and speaker verification to show that ours achieves better
performance than other proposed methods.
</description>
    </item>
    
    <item>
        <title>Stepsize Control for Acoustic Feedback Cancellation Based on the Detection of Reverberant Signal Periods and the Estimated System Distance</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0046.PDF</link>
        <description>A new approach for acoustic feedback cancellation is presented. The
challenge in acoustic feedback cancellation is a strong correlation
between the local speech and the loudspeaker signal. Due to this correlation,
the convergence rate of adaptive algorithms is limited. Therefore,
a novel stepsize control of the adaptive filter is presented. The stepsize
control exploits reverberant signal periods to update the adaptive
filter. As soon as local speech stops, the reverberation energy of
the system decays exponentially. This means that during reverberation
there is only excitation of the filter but no local speech. Thus, signals
are not correlated and the filter can converge without correlation
problems. Consequently, the stepsize control accelerates the adaption
process during reverberation and slows it down at the beginning of
speech activity. It is shown, that with a particular gain control,
the reverb-based stepsize control can be interpreted as the theoretical
optimum stepsize. However, for this purpose a precise estimation of
the system distance is required. One estimation method is presented.
The proposed estimator has a rescue mechanism to detect enclosure dislocations.
Both, simulations and real world testing show that the acoustic feedback
canceler is capable of improving stability and convergence rate, even
at high system gains.
</description>
    </item>
    
    <item>
        <title>A Delay-Flexible Stereo Acoustic Echo Cancellation for DFT-Based In-Car Communication (ICC) Systems</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1084.PDF</link>
        <description>In-car communication (ICC) systems supporting speech communication
in noise by reproducing amplified speech from the car cabin in the
car cabin ask for low-delay acoustic echo cancellation (AEC). In this
paper we propose a delay-flexible DFT-based stereo AEC capable of cancelling
also the echoes stemming from the audio player or FM radio. For the
price of a somewhat higher complexity we are able to reduce the 32
ms delay of the baseline down to 4 ms, loosing only 1 dB in ERLE while
even preserving system distance properties.
</description>
    </item>
    
    <item>
        <title>Speech Enhancement Based on Harmonic Estimation Combined with MMSE to Improve Speech Intelligibility for Cochlear Implant Recipients</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0078.PDF</link>
        <description>In this paper, a speech enhancement algorithm is proposed to improve
the speech intelligibility for cochlear implant recipients. Our method
is based on combination of harmonic estimation and traditional statistical
method. Traditional statistical based speech enhancement method is
effective only for stationary noise suppression, but not non-stationary
noise. To address more complex noise scenarios, we explore the harmonic
structure of target speech to obtain a more accurate noise estimation.
The estimated noise is then employed in the MMSE framework to obtain
the gain function for recovering the target speech. Listening test
experiments show a substantial speech intelligibility improvement for
cochlear implant recipients in noisy environments.
</description>
    </item>
    
    <item>
        <title>Improving Speech Intelligibility in Binaural Hearing Aids by Estimating a Time-Frequency Mask with a Weighted Least Squares Classifier</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0771.PDF</link>
        <description>An efficient algorithm for speech enhancement in binaural hearing aids
is proposed. The algorithm is based on the estimation of a time-frequency
mask using supervised machine learning. The standard least-squares
linear classifier is reformulated to optimize a metric related to speech/noise
separation. The method is energy-efficient in two ways: the computational
complexity is limited and the wireless data transmission optimized.
The ability of the algorithm to enhance speech contaminated with different
types of noise and low SNR has been evaluated. Objective measures of
speech intelligibility and speech quality demonstrate that the algorithm
increments both the hearing comfort and speech understanding of the
user. These results are supported by subjective listening tests.
</description>
    </item>
    
    <item>
        <title>Simulations of High-Frequency Vocoder on Mandarin Speech Recognition for Acoustic Hearing Preserved Cochlear Implant</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0858.PDF</link>
        <description>Vocoder simulations are generally adopted to simulate the electrical
hearing induced by the cochlear implant (CI). Our research group is
developing a new four-electrode CI microsystem which induces high-frequency
electrical hearing while preserving low-frequency acoustic hearing.
To simulate the functionality of this CI, a previously developed hearing-impaired
(HI) hearing model is combined with a 4-channel vocoder in this paper
to respectively mimic the perceived acoustic hearing and electrical
hearing. Psychoacoustic experiments are conducted on Mandarin speech
recognition for determining parameters of electrodes for this CI. Simulation
results show that initial consonants of Mandarin are more difficult
to recognize than final vowels of Mandarin via acoustic hearing of
HI patients. After electrical hearing being induced through logarithmic-frequency
distributed electrodes, speech intelligibility of HI patients is boosted
for all Mandarin phonemes, especially for initial consonants. Similar
results are consistently observed in clean and noisy test conditions.
</description>
    </item>
    
    <item>
        <title>Phonetic Correlates of Pharyngeal and Pharyngealized Consonants in Saudi, Lebanese, and Jordanian Arabic: An rt-MRI Study</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Glottal Opening and Strategies of Production of Fricatives</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1039.PDF</link>
        <description>This work investigates the influence of the gradual opening of the
glottis along its length during the production of fricatives in intervocalic
contexts. Acoustic simulations reveal the existence of a transient
zone in the articulatory space where the frication noise level is very
sensitive to small perturbations of the glottal opening. This corresponds
to the configurations where both frication noise and voiced contributions
are present in the speech signal. To avoid this unstability, speakers
may adopt different strategies to ensure the voiced/voiceless contrast
of fricatives. This is evidenced by experimental data of simultaneous
glottal opening measurements, performed with ePGG, and audio recordings
of vowel-fricative-vowel pseudowords. Voiceless fricatives are usually
longer, in order to maximize the number of voiceless time frames over
voiced frames due to the crossing of the transient regime. For voiced
fricatives, the speaker may avoid the unstable regime by keeping low
frication noise level, and thus by favoring the voicing characteristic,
or by doing very short crossings into the unstable regime. It is also
shown that when speakers are asked to sustain voiced fricatives longer
than in natural speech, they adopt the strategy of keeping low frication
noise level to avoid the unstable regime.
</description>
    </item>
    
    <item>
        <title>Acoustics and Articulation of Medial versus Final Coronal Stop Gemination Contrasts in Moroccan Arabic</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1292.PDF</link>
        <description>This paper presents results of a simultaneous acoustic and articulatory
investigation of word-medial and word-final geminate/singleton coronal
stop contrasts in Moroccan Arabic (MA). The acoustic analysis revealed
that, only for the word-medial contrast, the two MA speakers adopted
comparable strategies in contrasting geminates with singletons, mainly
by significantly lengthening closure duration in geminates, relative
to singletons. In word-final position, two speaker-specific contrasting
patterns emerged. While one speaker also lengthened the closure duration
for final geminates, the other speaker instead lengthened only the
release duration for final geminates, relative to singletons. Consonant
closure and preceding vowel were significantly longer for the geminate
only in medial position, not in final position. These temporal differences
were even more clearly delineated in the articulatory signal, captured
via ultrasound, to which we applied the novel approach of using TRACTUS
[Temporally Resolved Articulatory Configuration Tracking of UltraSound:
15] to index temporal properties of closure gestures for these geminate/singleton
contrasts.
</description>
    </item>
    
    <item>
        <title>How are Four-Level Length Distinctions Produced? Evidence from Moroccan Arabic</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1553.PDF</link>
        <description>We investigate the durational properties of Moroccan Arabic identical
consonant sequences contrasting singleton (S) and geminate (G) dental
fricatives, in six combinations of four-level length contrasts across
word boundaries (#) (one timing slot for #S, two for #G and S#S, three
for S#G and G#S, and four for G#G). The aim is to determine the nature
of the mapping between discrete phonological timing units and phonetic
durations. Acoustic results show that the largest and most systematic
jump in duration is displayed between the singleton fricative on the
one hand and the other sequences on the other hand. Looking at these
sequences, S#S is shown to have the same duration as #G. When a geminate
is within the sequence, a temporal reorganization is observed: G#S
is not significantly longer than S#S and #G; and G#G is only slightly
longer than S#G. Instead of a four-way hierarchy, our data point towards
a possible upper limit of three-way length contrasts for consonants:
S &amp;#60; G=S#S=G#S &amp;#60; S#G=G#G. The interplay of a number of factors
resulting in this mismatch between phonological length and phonetic
duration are discussed, and a working hypothesis is provided for why
duration contrasts are rarely ternary, and almost never quaternary.
</description>
    </item>
    
    <item>
        <title>Vowels in the Barunga Variety of North Australian Kriol</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Nature of Contrast and Coarticulation: Evidence from Mizo Tones and Assamese Vowel Harmony</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>The Influence of Synthetic Voice on the Evaluation of a Virtual Character</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0325.PDF</link>
        <description>Graphical realism and the naturalness of the voice used are important
aspects to consider when designing a virtual agent or character. In
this work, we evaluate how synthetic speech impacts people&amp;#8217;s
perceptions of a rendered virtual character. Using a controlled experiment,
we focus on the role that speech, in particular voice expressiveness
in the form of personality, has on the assessment of voice level and
character level perceptions. We found that people rated a real human
voice as more expressive, understandable and likeable than the expressive
synthetic voice we developed. Contrary to our expectations, we found
that the voices did not have a significant impact on the character
level judgments; people in the voice conditions did not significantly
vary on their ratings of appeal, credibility, human-likeness and voice
matching the character. The implications this has for character design
and how this compares with previous work are discussed.
</description>
    </item>
    
    <item>
        <title>Articulatory Text-to-Speech Synthesis Using the Digital Waveguide Mesh Driven by a Deep Neural Network</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0900.PDF</link>
        <description>Following recent advances in direct modeling of the speech waveform
using a deep neural network, we propose a novel method that directly
estimates a physical model of the vocal tract from the speech waveform,
rather than magnetic resonance imaging data. This provides a clear
relationship between the model and the size and shape of the vocal
tract, offering considerable flexibility in terms of speech characteristics
such as age and gender. Initial tests indicate that despite a highly
simplified physical model, intelligible synthesized speech is obtained.
This illustrates the potential of the combined technique for the control
of physical models in general, and hence the generation of more natural-sounding
synthetic speech.
</description>
    </item>
    
    <item>
        <title>An HMM/DNN Comparison for Synchronized Text-to-Speech and Tongue Motion Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0936.PDF</link>
        <description>We present an end-to-end text-to-speech (TTS) synthesis system that
generates audio and synchronized tongue motion directly from text.
This is achieved by adapting a statistical shape space model of the
tongue surface to an articulatory speech corpus and training a speech
synthesis system directly on the tongue model parameter weights. We
focus our analysis on the application of two standard methodologies,
based on Hidden Markov Models (HMMs) and Deep Neural Networks (DNNs),
respectively, to train both acoustic models and the tongue model parameter
weights. We evaluate both methodologies at every step by comparing
the predicted articulatory movements against the reference data. The
results show that even with less than 2h of data, DNNs already outperform
HMMs.
</description>
    </item>
    
    <item>
        <title>VCV Synthesis Using Task Dynamics to Animate a Factor-Based Articulatory Model</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1410.PDF</link>
        <description>This paper presents an initial architecture for articulatory synthesis
which combines a dynamical system for the control of vocal tract shaping
with a novel MATLAB implementation of an articulatory synthesizer.
The dynamical system controls a speaker-specific vocal tract model
derived by factor analysis of mid-sagittal real-time MRI data and provides
input to the articulatory synthesizer, which simulates the propagation
of sound waves in the vocal tract. First, parameters of the dynamical
system are estimated from real-time MRI data of human speech production.
Second, vocal-tract dynamics is simulated for vowel-consonant-vowel
utterances using a sequence of two dynamical systems: the first one
starts from a vowel vocal-tract configuration and achieves a vocal-tract
closure; the second one starts from the closure and achieves the target
configuration of the second vowel. Third, vocal-tract dynamics is converted
to area function dynamics and is input to the synthesizer to generate
the acoustic signal. Synthesized vowel-consonant-vowel examples demonstrate
the feasibility of the method.
</description>
    </item>
    
    <item>
        <title>Beyond the Listening Test: An Interactive Approach to TTS Evaluation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1438.PDF</link>
        <description>Traditionally, subjective text-to-speech (TTS) evaluation is performed
through audio-only listening tests, where participants evaluate unrelated,
context-free utterances. The ecological validity of these tests is
questionable, as they do not represent real-world end-use scenarios.
In this paper, we examine a novel approach to TTS evaluation in an
imagined end-use, via a complex interaction with an avatar. 6 different
voice conditions were tested: Natural speech, Unit Selection and Parametric
Synthesis, in neutral and expressive realizations. Results were compared
to a traditional audio-only evaluation baseline. Participants in both
studies rated the voices for naturalness and expressivity. The baseline
study showed canonical results for naturalness: Natural speech scored
highest, followed by Unit Selection, then Parametric synthesis. Expressivity
was clearly distinguishable in all conditions. In the avatar interaction
study, participants rated naturalness in the same order as the baseline,
though with smaller effect size; expressivity was not distinguishable.
Further, no significant correlations were found between cognitive or
affective responses and any voice conditions. This highlights 2 primary
challenges in designing more valid TTS evaluations: in real-world use-cases
involving interaction, listeners generally interact with a single voice,
making comparative analysis unfeasible, and in complex interactions,
the context and content may confound perception of voice quality.
</description>
    </item>
    
    <item>
        <title>Integrating Articulatory Information in Deep Learning-Based Text-to-Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1762.PDF</link>
        <description>Articulatory information has been shown to be effective in improving
the performance of hidden Markov model (HMM)-based text-to-speech (TTS)
synthesis. Recently, deep learning-based TTS has outperformed HMM-based
approaches. However, articulatory information has rarely been integrated
in deep learning-based TTS. This paper investigated the effectiveness
of integrating articulatory movement data to deep learning-based TTS.
The integration of articulatory information was achieved in two ways:
(1) direct integration, where articulatory and acoustic features were
the output of a deep neural network (DNN), and (2) direct integration
plus forward-mapping, where the output articulatory features were mapped
to acoustic features by an additional DNN; These forward-mapped acoustic
features were then combined with the output acoustic features to produce
the final acoustic features. Articulatory (tongue and lip) and acoustic
data collected from male and female speakers were used in the experiment.
Both objective measures and subjective judgment by human listeners
showed the approaches integrated articulatory information outperformed
the baseline approach (without using articulatory information) in terms
of naturalness and speaker voice identity (voice similarity).
</description>
    </item>
    
    <item>
        <title>Approaches for Neural-Network Language Model Adaptation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1310.PDF</link>
        <description>Language Models (LMs) for Automatic Speech Recognition (ASR) are typically
trained on large text corpora from news articles, books and web documents.
These types of corpora, however, are unlikely to match the test distribution
of ASR systems, which expect spoken utterances. Therefore, the LM is
typically adapted to a smaller held-out in-domain dataset that is drawn
from the test distribution. We propose three LM adaptation approaches
for Deep NN and Long Short-Term Memory (LSTM): (1) Adapting the softmax
layer in the Neural Network (NN); (2) Adding a non-linear adaptation
layer before the softmax layer that is trained only in the adaptation
phase; (3) Training the extra non-linear adaptation layer in pre-training
and adaptation phases. Aiming to improve upon a hierarchical Maximum
Entropy (MaxEnt) second-pass LM baseline, which factors the model into
word-cluster and word models, we build an NN LM that predicts only
word clusters. Adapting the LSTM LM by training the adaptation layer
in both training and adaptation phases (Approach 3), we reduce the
cluster perplexity by 30% on a held-out dataset compared to an unadapted
LSTM LM. Initial experiments using a state-of-the-art ASR system show
a 2.3% relative reduction in WER on top of an adapted MaxEnt LM.
</description>
    </item>
    
    <item>
        <title>A Batch Noise Contrastive Estimation Approach for Training Large Vocabulary Language Models</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0818.PDF</link>
        <description>Training large vocabulary Neural Network Language Models (NNLMs) is
a difficult task due to the explicit requirement of the output layer
normalization, which typically involves the evaluation of the full
softmax function over the complete vocabulary. This paper proposes
a Batch Noise Contrastive Estimation (B-NCE) approach to alleviate
this problem. This is achieved by reducing the vocabulary, at each
time step, to the target words in the batch and then replacing the
softmax by the noise contrastive estimation approach, where these words
play the role of targets and noise samples at the same time. In doing
so, the proposed approach can be fully formulated and implemented using
optimal dense matrix operations. Applying B-NCE to train different
NNLMs on the Large Text Compression Benchmark (LTCB) and the One Billion
Word Benchmark (OBWB) shows a significant reduction of the training
time with no noticeable degradation of the models performance. This
paper also presents a new baseline comparative study of different standard
NNLMs on the large OBWB on a single Titan-X GPU.
</description>
    </item>
    
    <item>
        <title>Investigating Bidirectional Recurrent Neural Network Language Models for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0513.PDF</link>
        <description>Recurrent neural network language models (RNNLMs) are powerful language
modeling techniques. Significant performance improvements have been
reported in a range of tasks including speech recognition compared
to n-gram language models. Conventional n-gram and neural network language
models are trained to predict the probability of the next word given
its preceding context history. In contrast, bidirectional recurrent
neural network based language models consider the context from future
words as well. This complicates the inference process, but has theoretical
benefits for tasks such as speech recognition as additional context
information can be used. However to date, very limited or no gains
in speech recognition performance have been reported with this form
of model. This paper examines the issues of training bidirectional
recurrent neural network language models (bi-RNNLMs) for speech recognition.
A bi-RNNLM probability smoothing technique is proposed, that addresses
the very sharp posteriors that are often observed in these models.
The performance of the bi-RNNLMs is evaluated on three speech recognition
tasks: broadcast news; meeting transcription (AMI); and low-resource
systems (Babel data). On all tasks gains are observed by applying the
smoothing technique to the bi-RNNLM. In addition consistent performance
gains can be obtained by combining bi-RNNLMs with n-gram and uni-directional
RNNLMs.
</description>
    </item>
    
    <item>
        <title>Fast Neural Network Language Model Lookups at N-Gram Speeds</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0564.PDF</link>
        <description>Feed forward Neural Network Language Models (NNLM) have shown consistent
gains over backoff word n-gram models in a variety of tasks. However,
backoff n-gram models still remain dominant in applications with real
time decoding requirements as word probabilities can be computed orders
of magnitude faster than the NNLM. In this paper, we present a combination
of techniques that allows us to speed up the probability computation
from a neural net language model to make it comparable to the word
n-gram model without any approximations. We present results on state
of the art systems for Broadcast news transcription and conversational
speech which demonstrate the speed improvements in real time factor
and probability computation while retaining the WER gains from NNLM.
</description>
    </item>
    
    <item>
        <title>Empirical Exploration of Novel Architectures and Objectives for Language Models</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0723.PDF</link>
        <description>While recurrent neural network language models based on Long Short
Term Memory (LSTM) have shown good gains in many automatic speech recognition
tasks, Convolutional Neural Network (CNN) language models are relatively
new and have not been studied in-depth. In this paper we present an
empirical comparison of LSTM and CNN language models on English broadcast
news and various conversational telephone speech transcription tasks.
We also present a new type of CNN language model that leverages dilated
causal convolution to efficiently exploit long range history. We propose
a novel criterion for training language models that combines word and
class prediction in a multi-task learning framework. We apply this
criterion to train word and character based LSTM language models and
CNN language models and show that it improves performance. Our results
also show that CNN and LSTM language models are complementary and can
be combined to obtain further gains.
</description>
    </item>
    
    <item>
        <title>Residual Memory Networks in Language Modeling: Improving the Reputation of Feed-Forward Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1442.PDF</link>
        <description>We introduce the Residual Memory Network (RMN) architecture to language
modeling. RMN is an architecture of feed-forward neural networks that
incorporates residual connections and time-delay connections that allow
us to naturally incorporate information from a substantial time context.
As this is the first time RMNs are applied for language modeling, we
thoroughly investigate their behaviour on the well studied Penn Treebank
corpus. We change the model slightly for the needs of language modeling,
reducing both its time and memory consumption. Our results show that
RMN is a suitable choice for small-sized neural language models: With
test perplexity 112.7 and as few as 2.3M parameters, they out-perform
both a much larger vanilla RNN (PPL 124, 8M parameters) and a similarly
sized LSTM (PPL 115, 2.08M parameters), while being only by less than
3 perplexity points worse than twice as big LSTM.
</description>
    </item>
    
    <item>
        <title>Dominant Distortion Classification for Pre-Processing of Vowels in Remote Biomedical Voice Analysis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0378.PDF</link>
        <description>Advances in speech signal analysis facilitate the development of techniques
for remote biomedical voice assessment. However, the performance of
these techniques is affected by noise and distortion in signals. In
this paper, we focus on the vowel /a/ as the most widely-used voice
signal for pathological voice assessments and investigate the impact
of four major types of distortion that are commonly present during
recording or transmission in voice analysis, namely: background noise,
reverberation, clipping and compression, on Mel-frequency cepstral
coefficients (MFCCs) &amp;#8212; the most widely-used features in biomedical
voice analysis. Then, we propose a new distortion classification approach
to detect the most dominant distortion in such voice signals. The proposed
method involves MFCCs as frame-level features and a support vector
machine as classifier to detect the presence and type of distortion
in frames of a given voice signal. Experimental results obtained from
the healthy and Parkinson&amp;#8217;s voices show the effectiveness of
the proposed approach in distortion detection and classification.
</description>
    </item>
    
    <item>
        <title>Automatic Paraphasia Detection from Aphasic Speech: A Preliminary Study</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0626.PDF</link>
        <description>Aphasia is an acquired language disorder resulting from brain damage
that can cause significant communication difficulties. Aphasic speech
is often characterized by errors known as paraphasias, the analysis
of which can be used to determine an appropriate course of treatment
and to track an individual&amp;#8217;s recovery progress. Being able to
detect paraphasias automatically has many potential clinical benefits;
however, this problem has not previously been investigated in the literature.
In this paper, we perform the first study on detecting phonemic and
neologistic paraphasias from scripted speech samples in AphasiaBank.
We propose a speech recognition system with task-specific language
models to transcribe aphasic speech automatically. We investigate features
based on speech duration, Goodness of Pronunciation, phone edit distance,
and Dynamic Time Warping on phoneme posteriorgrams. Our results demonstrate
the feasibility of automatic paraphasia detection and outline the path
toward enabling this system in real-world clinical applications.
</description>
    </item>
    
    <item>
        <title>Evaluation of the Neurological State of People with Parkinson&amp;#8217;s Disease Using i-Vectors</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0819.PDF</link>
        <description>The i-vector approach is used to model the speech of PD patients with
the aim of assessing their condition. Features related to the articulation,
phonation, and prosody dimensions of speech were used to train different
i-vector extractors. Each i-vector extractor is trained using utterances
from both PD patients and healthy controls. The i-vectors of the healthy
control (HC) speakers are averaged to form a single i-vector that represents
the HC group, i.e., the reference i-vector. A similar process is done
to create a reference of the group with PD patients. Then the i-vectors
of test speakers are compared to these reference i-vectors using the
cosine distance. Three analyses are performed using this distance:
classification between PD patients and HC, prediction of the neurological
state of PD patients according to the MDS-UPDRS-III scale, and prediction
of a modified version of the Frenchay Dysarthria Assessment. The Spearman&amp;#8217;s
correlation between this cosine distance and the MDS-UPDRS-III scale
was 0.63. These results show the suitability of this approach to monitor
the neurological state of people with Parkinson&amp;#8217;s Disease.
</description>
    </item>
    
    <item>
        <title>Objective Severity Assessment from Disordered Voice Using Estimated Glottal Airflow</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0138.PDF</link>
        <description>In clinical practice, the severity of disordered voice is typically
rated by a professional with auditory-perceptual judgment. The present
study aims to automate this assessment procedure, in an attempt to
make the assessment objective and less labor-intensive. In the automated
analysis, glottal airflow is estimated from the analyzed voice signal
with an inverse filtering algorithm. Automatic assessment is realized
by a regressor that predicts from temporal and spectral features of
the glottal airflow. A regressor trained on overtone amplitudes and
harmonic richness factors extracted from a set of continuous-speech
utterances was applied to a set of sustained-vowel utterances, giving
severity predictions (on a scale of ratings from 0 to 100) with an
average error magnitude of 14.
</description>
    </item>
    
    <item>
        <title>Earlier Identification of Children with Autism Spectrum Disorder: An Automatic Vocalisation-Based Approach</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1007.PDF</link>
        <description>Autism spectrum disorder (ASD) is a neurodevelopmental disorder usually
diagnosed in or beyond toddlerhood. ASD is defined by repetitive and
restricted behaviours, and deficits in social communication. The early
speech-language development of individuals with ASD has been characterised
as delayed. However, little is known about ASD-related characteristics
of pre-linguistic vocalisations at the feature level. In this study,
we examined pre-linguistic vocalisations of 10-month-old individuals
later diagnosed with ASD and a matched control group of typically developing
individuals (N = 20). We segmented 684 vocalisations from parent-child
interaction recordings. All vocalisations were annotated and signal-analytically
decomposed. We analysed ASD-related vocalisation specificities on the
basis of a standardised set (eGeMAPS) of 88 acoustic features selected
for clinical speech analysis applications. 54 features showed evidence
for a differentiation between vocalisations of individuals later diagnosed
with ASD and controls. In addition, we evaluated the feasibility of
automated, vocalisation-based identification of individuals later diagnosed
with ASD. We compared linear kernel support vector machines and a 1-layer
bidirectional long short-term memory neural network. Both classification
approaches achieved an accuracy of 75% for subject-wise identification
in a subject-independent 3-fold cross-validation scheme. Our promising
results may be an important contribution en-route to facilitate earlier
identification of ASD.
</description>
    </item>
    
    <item>
        <title>Convolutional Neural Network to Model Articulation Impairments in Patients with Parkinson&amp;#8217;s Disease</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1078.PDF</link>
        <description>Speech impairments are one of the earliest manifestations in patients
with Parkinson&amp;#8217;s disease. Particularly, articulation deficits
related to the capability of the speaker to start/stop the vibration
of the vocal folds have been observed in the patients. Those difficulties
can be assessed by modeling the transitions between voiced and unvoiced
segments from speech. A robust strategy to model the articulatory deficits
related to the starting or stopping vibration of the vocal folds is
proposed in this study. The transitions between voiced and unvoiced
segments are modeled by a convolutional neural network that extracts
suitable information from two time-frequency representations: the short
time Fourier transform and the continuous wavelet transform. The proposed
approach improves the results previously reported in the literature.
Accuracies of up to 89% are obtained for the classification of Parkinson&amp;#8217;s
patients vs. healthy speakers. This study is a step towards the robust
modeling of the speech impairments in patients with neuro-degenerative
disorders.
</description>
    </item>
    
    <item>
        <title>Phone Classification Using a Non-Linear Manifold with Broad Phone Class Dependent DNNs</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1179.PDF</link>
        <description>Most state-of-the-art automatic speech recognition (ASR) systems use
a single deep neural network (DNN) to map the acoustic space to the
decision space. However, different phonetic classes employ different
production mechanisms and are best described by different types of
features. Hence it may be advantageous to replace this single DNN with
several phone class dependent DNNs. The appropriate mathematical formalism
for this is a manifold. This paper assesses the use of a non-linear
manifold structure with multiple DNNs for phone classification. The
system has two levels. The first comprises a set of broad phone class
(BPC) dependent DNN-based mappings and the second level is a fusion
network. Various ways of designing and training the networks in both
levels are assessed, including varying the size of hidden layers, the
use of the bottleneck or softmax outputs as input to the fusion network,
and the use of different broad class definitions. Phone classification
experiments are performed on TIMIT. The results show that using the
BPC-dependent DNNs provides small but significant improvements in phone
classification accuracy relative to a single global DNN. The paper
concludes with visualisations of the structures learned by the local
and global DNNs and discussion of their interpretations.
</description>
    </item>
    
    <item>
        <title>An Investigation of Crowd Speech for Room Occupancy Estimation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0070.PDF</link>
        <description>Room occupancy estimation technology has been shown to reduce building
energy cost significantly. However speech-based occupancy estimation
has not been well explored. In this paper, we investigate energy mode
and babble speaker count methods for estimating both small and large
crowds in a party-mode room setting. We also examine how distance between
speakers and microphone affects their estimation accuracies. Then we
propose a novel entropy-based method, which is invariant to different
speakers and their different positions in a room. Evaluations on synthetic
crowd speech generated using the TIMIT corpus show that acoustic volume
features are less affected by distance, and our proposed method outperforms
existing methods across a range of different conditions.
</description>
    </item>
    
    <item>
        <title>Time-Frequency Coherence for Periodic-Aperiodic Decomposition of Speech Signals</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0726.PDF</link>
        <description>Decomposing speech signals into periodic and aperiodic components is
an important task, finding applications in speech synthesis, coding,
denoising, etc. In this paper, we construct a time-frequency coherence
function to analyze spectro-temporal signatures of speech signals for
distinguishing between deterministic and stochastic components of speech.
The narrowband speech spectrogram is segmented into patches, which
are represented as 2-D cosine carriers modulated in amplitude and frequency.
Separation of carrier and amplitude/frequency modulations is achieved
by 2-D demodulation using Riesz transform, which is the 2-D extension
of Hilbert transform. The demodulated AM component reflects contributions
of the vocal tract to spectrogram. The frequency modulated carrier
(FM-carrier) signal exhibits properties of the excitation. The time-frequency
coherence is defined with respect to FM-carrier and a coherence map
is constructed, in which highly coherent regions represent nearly periodic
and deterministic components of speech, whereas the incoherent regions
correspond to unstructured components. The coherence map shows a clear
distinction between deterministic and stochastic components in speech
characterized by jitter, shimmer, lip radiation, type of excitation,
etc. Binary masks prepared from the time-frequency coherence function
are used for periodic-aperiodic decomposition of speech. Experimental
results are presented to validate the efficiency of the proposed method.
</description>
    </item>
    
    <item>
        <title>Musical Speech: A New Methodology for Transcribing Speech Prosody</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0316.PDF</link>
        <description>Musical Speech is a new methodology for transcribing speech prosody
using musical notation. The methodology presented in this paper is
an updated version of our work [12]. Our work is situated in a historical
context with a brief survey of the literature of speech melodies, in
which we highlight the pioneering works of John Steele, Leo&amp;#353; Jan&amp;#225;vcek,
Engelbert Humperdinck, and Arnold Schoenberg, followed by a linguistic
view of musical notation in the analysis of speech. Finally, we present
the current state-of-the-art of our innovative methodology that uses
a quarter-tone scale for transcribing speech, and shows some initial
results of the application of this methodology to prosodic transcription.
</description>
    </item>
    
    <item>
        <title>Estimation of Place of Articulation of Fricatives from Spectral Characteristics for Speech Training</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1074.PDF</link>
        <description>A visual feedback of the place of articulation is considered to be
useful for speech training aids for hearing-impaired children and for
learners of second languages in helping them in improving pronunciation.
For such applications, the relation between place of articulation of
fricatives and their spectral characteristics is investigated using
English fricatives available in the XRMB database, which provides simultaneously
acquired speech signal and articulogram. Place of articulation is estimated
from the articulogram as the position of maximum constriction in the
oral cavity, using an automated graphical technique. The magnitude
spectrum is smoothed by critical band based median and mean filters
for improving the consistency of the spectral parameters. Out of several
spectral parameters investigated, spectral moments and spectral slope
appear to be related to the place of articulation of the fricative
segment of the utterances as measured from articulogram. The data are
used to train and test a Gaussian mixture model to estimate the place
of articulation with spectral parameters as the inputs. The estimated
values showed a good match with those obtained from the articulograms.
</description>
    </item>
    
    <item>
        <title>Estimation of the Probability Distribution of Spectral Fine Structure in the Speech Source</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>End-to-End Acoustic Feedback in Language Learning for Correcting Devoiced French Final-Fricatives</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1031.PDF</link>
        <description>This work aims at providing an end-to-end acoustic feedback framework
to help learners of French to pronounce voiced fricatives. A classifier
ensemble detects voiced/unvoiced utterances, then a correction method
is proposed to improve the perception and production of voiced fricatives
in a word-final position. Realizations of voiced fricatives contained
in French sentences uttered by French and German speakers were analyzed
to find out the deviations between the acoustic cues realized by the
two groups of speakers. The correction method consists in substituting
the erroneous devoiced fricative by TD-PSOLA concatenative synthesis
that uses exemplars of voiced fricatives chosen from a French speaker
corpus. To achieve a seamless concatenation the energy of the replacement
fricative was adjusted with respect to the energy levels of the learner&amp;#8217;s
and French speaker&amp;#8217;s preceding vowels. Finally, a perception
experiment with the corrected stimuli has been carried out with French
native speakers to check the appropriateness of the fricative revoicing.
The results showed that the proposed revoicing strategy proved to be
very efficient and can be used as an acoustic feedback.
</description>
    </item>
    
    <item>
        <title>Dialect Perception by Older Children</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0018.PDF</link>
        <description>The acquisition of regional dialect variation is an inherent part of
the language learning process that takes place in the specific environments
in which the child participates. This study examined dialect perception
by 9&amp;#8211;12-year-olds who grew up in two very diverse dialect regions
in the United States, Western North Carolina (NC) and Southeastern
Wisconsin (WI). In a dialect identification task, each group of children
responded to 120 talkers from the same dialects representing three
generations, ranging in age from old adults to children. There was
a robust discrepancy in the children&amp;#8217;s dialect identification
performance: WI children were able to identify talker dialect quite
well (although still not as well as the adults) whereas NC children
were at chance level. WI children were also more sensitive to cross-generational
changes in both dialects as a function of diachronic sound change.
It is concluded that both groups of children demonstrated their sociolinguistic
awareness in very different ways, corresponding to relatively stable
(WI) and changing (NC) socio-cultural environments in their respective
speech communities.
</description>
    </item>
    
    <item>
        <title>Perception of Non-Contrastive Variations in American English by Japanese Learners: Flaps are Less Favored Than Stops</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0207.PDF</link>
        <description>Alveolar flaps are non-contrastive allophonic variants of alveolar
stops in American English. A lexical decision experiment was conducted
with Japanese learners of English (JE) to investigate whether second-language
(L2) learners are sensitive to such allophonic variations when recognizing
words in L2. The stimuli consisted of 36 isolated bisyllabic English
words containing word-medial /t/, half of which were flap-favored words,
e.g.  city, and the other half were [t]-favored words, e.g.  faster.
All stimuli were recorded with two surface forms: /t/ as a flap, e.g.
 city with a flap, or as [t], e.g.  city with [t]. The stimuli were
counterbalanced so that participants only heard one of the two surface
forms of each word. The accuracy data indicated that flap-favored words
pronounced with a flap, e.g.  city with a flap, were recognized significantly
less accurately than flap-favored words with [t], e.g.  city with [t],
and [t]-favored words with [t], e.g.  faster with [t]. These results
suggest that JE learners prefer canonical forms over frequent forms
produced with context-dependent allophonic variations. These results
are inconsistent with previous studies that found native speakers&amp;#8217;
preference for frequent forms, and highlight differences in the effect
of allophonic variations on the perception of native-language and L2
speech.
</description>
    </item>
    
    <item>
        <title>L1 Perceptions of L2 Prosody: The Interplay Between Intonation, Rhythm, and Speech Rate and Their Contribution to Accentedness and Comprehensibility</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Effects of Pitch Fall and L1 on Vowel Length Identification in L2 Japanese</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0763.PDF</link>
        <description>This study investigated whether and how the role of pitch fall in the
first language (L1) interacts with its use as a cue for Japanese phonological
vowel length in the second language (L2). Native listeners of Japanese
(NJ) and L2 learners of Japanese with L1 backgrounds in Mandarin Chinese
(NC), Seoul Korean (NK), American English (NE), and French (NFr) participated
in a perception experiment. The results showed that the proportion
of &amp;#8220;long&amp;#8221; responses increased as a function of vowel duration
for all groups, giving s-shaped curves. Meanwhile, the presence or
absence of a pitch fall within a syllable affected only NJ and NC&amp;#8217;s
perception. Their category boundary occurred at a shorter duration
for vowels with a pitch fall than without a pitch fall. Among the four
groups of L2 learners, only NC use pitch fall to distinguish words
in the L1. Thus, it is possible to think that the role of pitch fall
as an L1 cue relates to its use as a cue for L2 length identification.
L2 learners tend to attend to an important phonetic feature as a cue
for perceiving an L1 category differentiating L1 words even in the
L2 as implied by the Feature Hypothesis.
</description>
    </item>
    
    <item>
        <title>A Preliminary Study of Prosodic Disambiguation by Chinese EFL Learners</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1210.PDF</link>
        <description>This study investigated whether Chinese learners of English as a foreign
language (EFL learners hereafter) could use prosodic cues to resolve
syntactically ambiguous sentences in English. 8 sentences with 3 types
of syntactic ambiguity were adopted. They were far/near PP attachment,
left/right word attachment and wide/narrow scope. In the production
experiment, 15 Chinese college students who passed the annual national
examination CET (College English Test) Band 4 and 5 native English
speakers from America were recruited. They were asked to read the 8
target sentences after hearing the contexts spoken by a Native American
speaker, which clarified the intended meaning of the ambiguous sentences.
The preliminary results showed that, as the native speakers did, Chinese
EFL learners employed different durational patterns to express the
alternative meanings of the ambiguous sentences by altering prosodic
phrasing. That is, the duration of the pre-boundary items were lengthened
and pause were inserted at the boundary. But the perception experiment
showed that the utterances produced by Chinese EFL learners couldn&amp;#8217;t
be effectively perceived by the native speakers due to their different
use of pre-boundary lengthening and pause. The conclusion is that Chinese
EFL learners find prosodic disambiguation difficult.
</description>
    </item>
    
    <item>
        <title>Generation of Large-Scale Simulated Utterances in Virtual Rooms to Train Deep-Neural Networks for Far-Field Speech Recognition in Google Home</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1510.PDF</link>
        <description>We describe the structure and application of an acoustic room simulator
to generate large-scale simulated data for training deep neural networks
for far-field speech recognition. The system simulates millions of
different room dimensions, a wide distribution of reverberation time
and signal-to-noise ratios, and a range of microphone and sound source
locations. We start with a relatively clean training set as the source
and artificially create simulated data by randomly sampling a noise
configuration for every new training example. As a result, the acoustic
model is trained using examples that are virtually never repeated.
We evaluate performance of this approach based on room simulation using
a factored complex Fast Fourier Transform (CFFT) acoustic model introduced
in our earlier work, which uses CFFT layers and LSTM AMs for joint
multichannel processing and acoustic modeling. Results show that the
simulator-driven approach is quite effective in obtaining large improvements
not only in simulated test conditions, but also in real / rerecorded
conditions. This room simulation system has been employed in training
acoustic models including the ones for the recently released Google
Home.
</description>
    </item>
    
    <item>
        <title>Neural Network-Based Spectrum Estimation for Online WPE Dereverberation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0733.PDF</link>
        <description>In this paper, we propose a novel speech dereverberation framework
that utilizes deep neural network (DNN)-based spectrum estimation to
construct linear inverse filters. The proposed dereverberation framework
is based on the state-of-the-art inverse filter estimation algorithm
called weighted prediction error (WPE) algorithm, which is known to
effectively reduce reverberation and greatly boost the ASR performance
in various conditions. In WPE, the accuracy of the inverse filter estimation,
and thus the dereverberation performance, is largely dependent on the
estimation of the power spectral density (PSD) of the target signal.
Therefore, the conventional WPE iteratively performs the inverse filter
estimation, actual dereverberation and the PSD estimation to gradually
improve the PSD estimate. However, while such iterative procedure works
well when sufficiently long acoustically-stationary observed signals
are available, WPE&amp;#8217;s performance degrades when the duration of
observed/accessible data is short, which typically is the case for
real-time applications using online block-batch processing with small
batches. To solve this problem, we incorporate the DNN-based spectrum
estimator into the framework of WPE, because a DNN can estimate the
PSD robustly even from very short observed data. We experimentally
show that the proposed framework outperforms the conventional WPE,
and improves the ASR performance in real noisy reverberant environments
in both single-channel and multichannel cases.
</description>
    </item>
    
    <item>
        <title>Factorial Modeling for Effective Suppression of Directional Noise</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0852.PDF</link>
        <description>The assumed scenario is transcription of a face-to-face conversation,
such as in the financial industry when an agent and a customer talk
over a desk with microphones placed between the speakers and then it
is transcribed. From the automatic speech recognition (ASR) perspective,
one of the speakers is the target speaker, and the other speaker is
a directional noise source. When the number of microphones is small,
we often accept microphone intervals that are larger than the spatial
aliasing limit because the performance of the beamformer is better.
Unfortunately, such a configuration results in significant leakage
of directional noise in certain frequency bands because the spatial
aliasing makes the beamformer and post-filter inaccurate there. Thus,
we introduce a factorial model to compensate only the degraded bands
with information from the reliable bands in a probabilistic framework
integrating our proposed metrics and speech model. In our experiments,
the proposed method reduced the errors from 29.8% to 24.9%.
</description>
    </item>
    
    <item>
        <title>On Design of Robust Deep Models for CHiME-4 Multi-Channel Speech Recognition with Multiple Configurations of Array Microphones</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0853.PDF</link>
        <description>We design a novel deep learning framework for multi-channel speech
recognition in two aspects. First, for the front-end, an iterative
mask estimation (IME) approach based on deep learning is presented
to improve the beamforming approach based on the conventional complex
Gaussian mixture model (CGMM). Second, for the back-end, deep convolutional
neural networks (DCNNs), with augmentation of both noisy and beamformed
training data, are adopted for acoustic modeling while the forward
and backward long short-term memory recurrent neural networks (LSTM-RNNs)
are used for language modeling. The proposed framework can be quite
effective to multi-channel speech recognition with random combinations
of fixed microphones. Testing on the CHiME-4 Challenge speech recognition
task with a single set of acoustic and language models, our approach
achieves the best performance of all three tracks (1-channel, 2-channel,
and 6-channel) among submitted systems.
</description>
    </item>
    
    <item>
        <title>Acoustic Modeling for Google Home</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0234.PDF</link>
        <description>This paper describes the technical and system building advances made
to the Google Home multichannel speech recognition system, which was
launched in November 2016. Technical advances include an adaptive dereverberation
frontend, the use of neural network models that do multichannel processing
jointly with acoustic modeling, and Grid-LSTMs to model frequency variations.
On the system level, improvements include adapting the model using
Google Home specific data. We present results on a variety of multichannel
sets. The combination of technical and system advances result in a
reduction of WER of 8&amp;#8211;28% relative compared to the current production
system.
</description>
    </item>
    
    <item>
        <title>On Multi-Domain Training and Adaptation of End-to-End RNN Acoustic Models for Distant Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0398.PDF</link>
        <description>Recognition of distant (far-field) speech is a challenge for ASR due
to mismatch in recording conditions resulting from room reverberation
and environment noise. Given the remarkable learning capacity of deep
neural networks, there is increasing interest to address this problem
by using a large corpus of reverberant far-field speech to train robust
models. In this study, we explore how an end-to-end RNN acoustic model
trained on speech from different rooms and acoustic conditions (different
domains) achieves robustness to environmental variations. It is shown
that the first hidden layer acts as a domain separator, projecting
the data from different domains into different subspaces. The subsequent
layers then use this encoded domain knowledge to map these features
to final representations that are invariant to domain change. This
mechanism is closely related to noise-aware or room-aware approaches
which append manually-extracted domain signatures to the input features.
Additionally, we demonstrate how this understanding of the learning
procedure provides useful guidance for model adaptation to new acoustic
conditions. We present results based on AMI corpus to demonstrate the
propagation of domain information in a deep RNN, and perform recognition
experiments which indicate the role of encoded domain knowledge on
training and adaptation of RNN acoustic models.
</description>
    </item>
    
    <item>
        <title>Low-Dimensional Representation of Spectral Envelope Without Deterioration for Full-Band Speech Analysis/Synthesis System</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0067.PDF</link>
        <description>A speech coding for a full-band speech analysis/synthesis system is
described. In this work, full-band speech is defined as speech with
a sampling frequency above 40 kHz, whose Nyquist frequency covers the
audible frequency range. In prior works, speech coding has generally
focused on the narrow-band speech with a sampling frequency below 16
kHz. On the other hand, statistical parametric speech synthesis currently
uses the full-band speech, and low-dimensional representation of speech
parameters is being used. The purpose of this study is to achieve speech
coding without deterioration for full-band speech. We focus on a high-quality
speech analysis/synthesis system and mel-cepstral analysis using frequency
warping. In the frequency warping function, we directly use three auditory
scales. We carried out a subjective evaluation using the WORLD vocoder
and found that the optimum number of dimensions was around 50. The
kind of frequency warping did not significantly affect the sound quality
in the dimensions.
</description>
    </item>
    
    <item>
        <title>Robust Source-Filter Separation of Speech Signal in the Phase Domain</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0210.PDF</link>
        <description>In earlier work we proposed a framework for speech source-filter separation
that employs phase-based signal processing. This paper presents a further
theoretical investigation of the model and optimisations that make
the filter and source representations less sensitive to the effects
of noise and better matched to downstream processing. To this end,
first, in computing the Hilbert transform, the log function is replaced
by the generalised logarithmic function. This introduces a tuning parameter
that adjusts both the dynamic range and distribution of the phase-based
representation. Second, when computing the group delay, a more robust
estimate for the derivative is formed by applying a regression filter
instead of using sample differences. The effectiveness of these modifications
is evaluated in clean and noisy conditions by considering the accuracy
of the fundamental frequency extracted from the estimated source, and
the performance of speech recognition features extracted from the estimated
filter. In particular, the proposed filter-based front-end reduces
Aurora-2 WERs by 6.3% (average 0&amp;#8211;20 dB) compared with previously
reported results. Furthermore, when tested in a LVCSR task (Aurora-4)
the new features resulted in 5.8% absolute WER reduction compared to
MFCCs without performance loss in the clean/matched condition.
</description>
    </item>
    
    <item>
        <title>A Time-Warping Pitch Tracking Algorithm Considering Fast f<SUB>0</SUB> Changes</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>A Modulation Property of Time-Frequency Derivatives of Filtered Phase and its Application to Aperiodicity and f&lt;SUB&gt;o&lt;/SUB&gt; Estimation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0436.PDF</link>
        <description>We introduce a simple and linear SNR (strictly speaking, periodic to
random power ratio) estimator (0 dB to 80 dB without additional calibration/linearization)
for providing reliable descriptions of aperiodicity in speech corpus.
The main idea of this method is to estimate the background random noise
level without directly extracting the background noise. The proposed
method is applicable to a wide variety of time windowing functions
with very low sidelobe levels. The estimate combines the frequency
derivative and the time-frequency derivative of the mapping from filter
center frequency to the output instantaneous frequency. This procedure
can replace the periodicity detection and aperiodicity estimation subsystems
of recently introduced open source vocoder, YANG vocoder. Source code
of MATLAB implementation of this method will also be open sourced.
</description>
    </item>
    
    <item>
        <title>Non-Local Estimation of Speech Signal for Vowel Onset Point Detection in Varied Environments</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0624.PDF</link>
        <description>Vowel onset point (VOP) is an important information extensively employed
in speech analysis and synthesis. Detecting the VOPs in a given speech
sequence, independent of the text contexts and recording environments,
is a challenging area of research. Performance of existing VOP detection
methods have not yet been extensively studied in varied environmental
conditions. In this paper, we have exploited the non-local means estimation
to detect those regions in the speech sequence which are of high signal-to-noise
ratio and exhibit periodicity. Mostly, those regions happen to be the
vowel regions. This helps in overcoming the ill-effects of environmental
degradations. Next, for each short-time frame of estimated speech sequence,
we cumulatively sum the magnitude of the corresponding Fourier transform
spectrum. The cumulative sum is then used as the feature to detect
the VOPs. The experiments conducted on TIMIT database show that the
proposed approach provides better results in terms of detection and
spurious rate when compared to a few existing methods under clean and
noisy test conditions.
</description>
    </item>
    
    <item>
        <title>Time-Domain Envelope Modulating the Noise Component of Excitation in a Continuous Residual-Based Vocoder for Statistical Parametric Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0678.PDF</link>
        <description>In this paper, we present an extension of a novel continuous residual-based
vocoder for statistical parametric speech synthesis. Previous work
has shown the advantages of adding envelope modulated noise to the
voiced excitation, but this has not been investigated yet in the context
of continuous vocoders, i.e. of which all parameters are continuous.
The noise component is often not accurately modeled in modern vocoders
(e.g. STRAIGHT). For more natural sounding speech synthesis, four time-domain
envelopes (Amplitude, Hilbert, Triangular and True) are investigated
and enhanced, and then applied to the noise component of the excitation
in our continuous vocoder. The performance evaluation is based on the
study of time envelopes. In an objective experiment, we investigated
the Phase Distortion Deviation of vocoded samples. A MUSHRA type subjective
listening test was also conducted comparing natural and vocoded speech
samples. Both experiments have shown that the proposed framework using
Hilbert and True envelopes provides high-quality vocoding while outperforming
the two other envelopes.
</description>
    </item>
    
    <item>
        <title>Wavelet Speech Enhancement Based on Robust Principal Component Analysis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0781.PDF</link>
        <description>Most state-of-the-art speech enhancement (SE) techniques prefer to
enhance utterances in the frequency domain rather than in the time
domain. However, the overlap-add (OLA) operation in the short-time
Fourier transform (STFT) for speech signal processing possibly distorts
the signal and limits the performance of the SE techniques. In this
study, a novel SE method that integrates the discrete wavelet packet
transform (DWPT) and a novel subspace-based method, robust principal
component analysis (RPCA), is proposed to enhance noise-corrupted signals
directly in the time domain. We evaluate the proposed SE method on
the Mandarin hearing in noise test (MHINT) sentences. The experimental
results show that the new method reduces the signal distortions dramatically,
thereby improving speech quality and intelligibility significantly.
In addition, the newly proposed method outperforms the STFT-RPCA-based
speech enhancement system.
</description>
    </item>
    
    <item>
        <title>Vowel Onset Point Detection Using Sonority Information</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0790.PDF</link>
        <description>Vowel onset point (VOP) refers to the starting event of a vowel, that
may be reflected in different aspects of the speech signal. The major
issue in VOP detection using existing methods is the confusion among
the vowels and other categories of sounds preceding them. This work
explores the usefulness of sonority information to reduce this confusion
and improve VOP detection. Vowels are the most sonorant sounds followed
by semivowels, nasals, voiced fricatives, voiced stops. The sonority
feature is derived from the vocal-tract system, excitation source and
suprasegmental aspects. As this feature has the capability to discriminate
among different sonorant sound units, it reduces the confusion among
onset of vowels with that of other sonorant sounds. This results in
improved detection and resolution of VOP detection for continuous speech.
The performance of proposed sonority information based VOP detection
is found to be 92.4%, compared to 85.2% by the existing method. Also
the resolution of localizing VOP within 10 ms is significantly enhanced
and a performance of 73.0% is achieved as opposed to 60.2% by the existing
method.
</description>
    </item>
    
    <item>
        <title>Analytic Filter Bank for Speech Analysis, Feature Extraction and Perceptual Studies</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Learning the Mapping Function from Voltage Amplitudes to Sensor Positions in 3D-EMA Using Deep Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1681.PDF</link>
        <description>The first generation of three-dimensional Electromagnetic Articulography
devices (Carstens AG500) suffered from occasional critical tracking
failures. Although now superseded by new devices, the AG500 is still
in use in many speech labs and many valuable data sets exist. In this
study we investigate whether deep neural networks (DNNs) can learn
the mapping function from raw voltage amplitudes to sensor positions
based on a comprehensive movement data set. This is compared to arriving
sample by sample at individual position values via direct optimisation
as used in previous methods. We found that with appropriate hyperparameter
settings a DNN was able to approximate the mapping function with good
accuracy, leading to a smaller error than the previous methods, but
that the DNN-based approach was not able to solve the tracking problem
completely.
</description>
    </item>
    
    <item>
        <title>Multilingual i-Vector Based Statistical Modeling for Music Genre Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0074.PDF</link>
        <description>For music signal processing, compared with the strategy which models
each short-time frame independently, when the long-time features are
considered, the time-series characteristics of the music signal can
be better presented. As a typical kind of long-time modeling strategy,
the identification vector (i-vector) uses statistical modeling to model
the audio signal in the segment level. It can better capture the important
elements of the music signal, and these important elements may benefit
to the classification of music signal. In this paper, the i-vector
based statistical feature for music genre classification is explored.
In addition to learn enough important elements for music signal, a
new multilingual i-vector feature is proposed based on the multilingual
model. The experimental results show that the multilingual i-vector
based models can achieve better classification performances than conventional
short-time modeling based methods.
</description>
    </item>
    
    <item>
        <title>Indoor/Outdoor Audio Classification Using Foreground Speech Segmentation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0309.PDF</link>
        <description>The task of indoor/ outdoor audio classification using foreground speech
segmentation is attempted in this work. Foreground speech segmentation
is the use of features to segment between foreground speech and background
interfering sources like noise. Initially, the foreground and background
segments are obtained from foreground speech segmentation by using
the normalized autocorrelation peak strength (NAPS) of the zero frequency
filtered signal (ZFFS) as a feature. The background segments are then
considered for determining whether a particular segment is an indoor
or outdoor audio sample. The mel frequency cepstral coefficients are
obtained from the background segments of both the indoor and outdoor
audio samples and are used to train the Support Vector Machine (SVM)
classifier. The use of foreground speech segmentation gives a promising
performance for the indoor/ outdoor audio classification task.
</description>
    </item>
    
    <item>
        <title>Attention Based CLDNNs for Short-Duration Acoustic Scene Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0440.PDF</link>
        <description>Recently, neural networks with deep architecture have been widely applied
to acoustic scene classification. Both Convolutional Neural Networks
(CNNs) and Long Short-Term Memory Networks (LSTMs) have shown improvements
over fully connected Deep Neural Networks (DNNs). Motivated by the
fact that CNNs, LSTMs and DNNs are complimentary in their modeling
capability, we apply the CLDNNs (Convolutional, Long Short-Term Memory,
Deep Neural Networks) framework to short-duration acoustic scene classification
in a unified architecture. The CLDNNs take advantage of frequency modeling
with CNNs, temporal modeling with LSTM, and discriminative training
with DNNs. Based on the CLDNN architecture, several novel attention-based
mechanisms are proposed and applied on the LSTM layer to predict the
importance of each time step. We evaluate the proposed method on the
truncated version of the 2016 TUT acoustic scenes dataset which consists
of recordings from 15 different scenes. By using CLDNNs with bidirectional
LSTM, we achieve higher performance compared to the conventional neural
network architectures. Moreover, by combining the attention-weighted
output with LSTM final time step output, significant improvement can
be further achieved.
</description>
    </item>
    
    <item>
        <title>Frame-Wise Dynamic Threshold Based Polyphonic Acoustic Event Detection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0746.PDF</link>
        <description>Acoustic event detection, the determination of the acoustic event type
and the localisation of the event, has been widely applied in many
real-world applications. Many works adopt multi-label classification
techniques to perform the polyphonic acoustic event detection with
a global threshold to detect the active acoustic events. However, the
global threshold has to be set manually and is highly dependent on
the database being tested. To deal with this, we replaced the fixed
threshold method with a frame-wise dynamic threshold approach in this
paper. Two novel approaches, namely contour and regressor based dynamic
threshold approaches are proposed in this work. Experimental results
on the popular TUT Acoustic Scenes 2016 database of polyphonic events
demonstrated the superior performance of the proposed approaches.
</description>
    </item>
    
    <item>
        <title>Enhanced Feature Extraction for Speech Detection in Media Audio</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0792.PDF</link>
        <description>Speech detection is an important first step for audio analysis on media
contents, whose goal is to discriminate the presence of speech from
non-speech. It remains a challenge owing to various sound sources included
in media audio. In this work, we present a novel audio feature extraction
method to reflect the acoustic characteristic of the media audio in
the time-frequency domain. Since the degree of combination of harmonic
and percussive components varies depending on the type of sound source,
the audio features which further distinguish between speech and non-speech
can be obtained by decomposing the signal into both components. For
the evaluation, we use over 20 hours of drama which manually annotated
for speech detection as well as 4 full-length movies with annotations
released for a research community, whose total length is over 8 hours.
Experimental results with deep neural network show superior performance
of the proposed in media audio condition.
</description>
    </item>
    
    <item>
        <title>Audio Classification Using Class-Specific Learned Descriptors</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0982.PDF</link>
        <description>This paper presents a classification scheme for audio signals using
high-level feature descriptors. The descriptor is designed to capture
the relevance of each acoustic feature group (or feature set like mel-frequency
cepstral coefficients, perceptual features etc.) in recognizing an
audio class. For this, a bank of RVM classifiers are modeled for each
&amp;#8216;audio class&amp;#8217;&amp;#8211;&amp;#8216;feature group&amp;#8217; pair. The
response of an input signal to this bank of RVM classifiers forms the
entries of the descriptor. Each entry of the descriptor thus measures
the proximity of the input signal to an audio class based on a single
feature group. This form of signal representation offers two-fold advantages.
First, it helps to determine the effectiveness of each feature group
in classifying a specific audio class. Second, the descriptor offers
higher discriminability than the low-level feature groups and a simple
SVM classifier trained on the descriptor produces better performance
than several state-of-the-art methods. 
</description>
    </item>
    
    <item>
        <title>Hidden Markov Model Variational Autoencoder for Acoustic Unit Discovery</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1160.PDF</link>
        <description>Variational Autoencoders (VAEs) have been shown to provide efficient
neural-network-based approximate Bayesian inference for observation
models for which exact inference is intractable. Its extension, the
so-called Structured VAE (SVAE) allows inference in the presence of
both discrete and continuous latent variables. Inspired by this extension,
we developed a VAE with Hidden Markov Models (HMMs) as latent models.
We applied the resulting HMM-VAE to the task of acoustic unit discovery
in a zero resource scenario. Starting from an initial model based on
variational inference in an HMM with Gaussian Mixture Model (GMM) emission
probabilities, the accuracy of the acoustic unit discovery could be
significantly improved by the HMM-VAE. In doing so we were able to
demonstrate for an unsupervised learning task what is well-known in
the supervised learning case: Neural networks provide superior modeling
power compared to GMMs.
</description>
    </item>
    
    <item>
        <title>Virtual Adversarial Training and Data Augmentation for Acoustic Event Detection with Gated Recurrent Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1238.PDF</link>
        <description>In this paper, we use gated recurrent neural networks (GRNNs) for efficiently
detecting environmental events of the IEEE Detection and Classification
of Acoustic Scenes and Events challenge (DCASE2016). For this acoustic
event detection task data is limited. Therefore, we propose data augmentation
such as  on-the-fly shuffling and virtual adversarial training for
regularization of the GRNNs. Both improve the performance using GRNNs.
We obtain a segment-based error rate of 0.59 and an F-score of 58.6%.
</description>
    </item>
    
    <item>
        <title>Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1386.PDF</link>
        <description>We present the Montreal Forced Aligner (MFA), a new open-source system
for speech-text alignment. MFA is an update to the Prosodylab-Aligner,
and maintains its key functionality of trainability on new data, as
well as incorporating improved architecture (triphone acoustic models
and speaker adaptation), and other features. MFA uses Kaldi instead
of HTK, allowing MFA to be distributed as a stand-alone package, and
to exploit parallel processing for computationally-intensive training
and scaling to larger datasets. We evaluate MFA&amp;#8217;s performance
on aligning word and phone boundaries in English conversational and
laboratory speech, relative to human-annotated boundaries, focusing
on the effects of aligner architecture and training on the data to
be aligned. MFA performs well relative to two existing open-source
aligners with simpler architecture (Prosodylab-Aligner and FAVE), and
both its improved architecture and training on data to be aligned generally
result in more accurate boundaries.
</description>
    </item>
    
    <item>
        <title>A Robust Voiced/Unvoiced Phoneme Classification from Whispered Speech Using the &amp;#8216;Color&amp;#8217; of Whispered Phonemes and Deep Neural Network</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1388.PDF</link>
        <description>In this work, we propose a robust method to perform frame-level classification
of voiced (V) and unvoiced (UV) phonemes from whispered speech, a challenging
task due to its voiceless and noise-like nature. We hypothesize that
a whispered speech spectrum can be represented as a linear combination
of a set of colored noise spectra. A five-dimensional (5D) feature
is computed by employing non-negative matrix factorization with a fixed
basis dictionary, constructed using spectra of five colored noises.
Deep Neural Network (DNN) is used as the classifier. We consider two
baseline features-1) Mel Frequency Cepstral Coefficients (MFCC), 2)
features computed from a data driven dictionary. Experiments reveal
that the features from the colored noise dictionary perform better
(on average) than that using the data driven dictionary, with a relative
improvement in the average V/UV accuracy of 10.30%, within, and 10.41%,
across, data from seven subjects. We also find that the MFCCs and 5D
features carry complementary information regarding the nature of voicing
decisions in whispered speech. Hence, across all subjects, we obtain
a balanced frame-level V/UV classification performance, when MFCC and
5D features are combined, compared to a skewed performance when they
are considered separately.
</description>
    </item>
    
    <item>
        <title>Rescoring-Aware Beam Search for Reduced Search Errors in Contextual Automatic Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Comparison of Decoding Strategies for CTC Acoustic Models</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Phone Duration Modeling for LVCSR Using Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1680.PDF</link>
        <description>We describe our work on incorporating probabilities of phone durations,
learned by a neural net, into an ASR system. Phone durations are incorporated
via lattice rescoring. The input features are derived from the phone
identities of a context window of phones, plus the durations of preceding
phones within that window. Unlike some previous work, our network outputs
the probability of different durations (in frames) directly, up to
a fixed limit. We evaluate this method on several large vocabulary
tasks, and while we consistently see improvements inWord Error Rates,
the improvements are smaller when the lattices are generated with neural
net based acoustic models.
</description>
    </item>
    
    <item>
        <title>Towards Better Decoding and Language Model Integration in Sequence to Sequence Models</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0343.PDF</link>
        <description>The recently proposed Sequence-to-Sequence (seq2seq) framework advocates
replacing complex data processing pipelines, such as an entire automatic
speech recognition system, with a single neural network trained in
an end-to-end fashion. In this contribution, we analyse an attention-based
seq2seq speech recognition system that directly transcribes recordings
into characters. We observe two shortcomings: overconfidence in its
predictions and a tendency to produce incomplete transcriptions when
language models are used. We propose practical solutions to both problems
achieving competitive speaker independent word error rates on the Wall
Street Journal dataset: without separate language models we reach 10.6%
WER, while together with a trigram language model, we reach 6.7% WER,
a state-of-the-art result for HMM-free methods.
</description>
    </item>
    
    <item>
        <title>Empirical Evaluation of Parallel Training Algorithms on Acoustic Modeling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0547.PDF</link>
        <description>Deep learning models (DLMs) are state-of-the-art techniques in speech
recognition. However, training good DLMs can be time consuming especially
for production-size models and corpora. Although several parallel training
algorithms have been proposed to improve training efficiency, there
is no clear guidance on which one to choose for the task in hand due
to lack of systematic and fair comparison among them. In this paper
we aim at filling this gap by comparing four popular parallel training
algorithms in speech recognition, namely asynchronous stochastic gradient
descent (ASGD), blockwise model-update filtering (BMUF), bulk synchronous
parallel (BSP) and elastic averaging stochastic gradient descent (EASGD),
on 1000-hour LibriSpeech corpora using feed-forward deep neural networks
(DNNs) and convolutional, long short-term memory, DNNs (CLDNNs). Based
on our experiments, we recommend using BMUF as the top choice to train
acoustic models since it is most stable, scales well with number of
GPUs, can achieve reproducible results, and in many cases even outperforms
single-GPU SGD. ASGD can be used as a substitute in some cases.
</description>
    </item>
    
    <item>
        <title>Binary Deep Neural Networks for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1343.PDF</link>
        <description>Deep neural networks (DNNs) are widely used in most current automatic
speech recognition (ASR) systems. To guarantee good recognition performance,
DNNs usually require significant computational resources, which limits
their application to low-power devices. Thus, it is appealing to reduce
the computational cost while keeping the accuracy. In this work, in
light of the success in image recognition, binary DNNs are utilized
in speech recognition, which can achieve competitive performance and
substantial speed up. To our knowledge, this is the first time that
binary DNNs have been used in speech recognition. For binary DNNs,
network weights and activations are constrained to be binary values,
which enables faster matrix multiplication based on bit operations.
By exploiting the hardware population count instructions, the proposed
binary matrix multiplication can achieve 5&amp;#126;7 times speed up compared
with highly optimized floating-point matrix multiplication. This results
in much faster DNN inference since matrix multiplication is the most
computationally expensive operation. Experiments on both TIMIT phone
recognition and a 50-hour Switchboard speech recognition show that,
binary DNNs can run about 4 times faster than standard DNNs during
inference, with roughly 10.0% relative accuracy reduction.
</description>
    </item>
    
    <item>
        <title>Hierarchical Constrained Bayesian Optimization for Feature, Acoustic Model and Decoder Parameter Optimization</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1583.PDF</link>
        <description>We describe the implementation of a hierarchical constrained Bayesian
Optimization algorithm and it&amp;#8217;s application to joint optimization
of features, acoustic model structure and decoding parameters for deep
neural network (DNN)-based large vocabulary continuous speech recognition
(LVCSR) systems. Within our hierarchical optimization method we perform
constrained Bayesian optimization jointly of feature hyper-parameters
and acoustic model structure in the first-level, and then perform an
iteration of constrained Bayesian optimization for the decoder hyper-parameters
in the second. We show the the proposed hierarchical optimization method
can generate a model with higher performance than a manually optimized
system on a server platform. Furthermore, we demonstrate that the proposed
framework can be used to automatically build real-time speech recognition
systems for graphics processing unit (GPU)-enabled embedded platforms
that retain similar accuracy to a server platform, while running with
constrained computing resources.
</description>
    </item>
    
    <item>
        <title>Use of Global and Acoustic Features Associated with Contextual Factors to Adapt Language Models for Spontaneous Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0717.PDF</link>
        <description>In this study, we propose a new method of adapting language models
for speech recognition using para-linguistic and extra-linguistic features
in speech. When we talk with others, we often change the way of lexical
choice and speaking style according to various contextual factors.
This fact indicates that the performance of automatic speech recognition
can be improved by taking the contextual factors into account, which
can be estimated from speech acoustics. In this study, we attempt to
find global and acoustic features that are associated with those contextual
factors, then integrate those features into Recurrent Neural Network
(RNN) language models for speech recognition. In experiments, using
Japanese spontaneous speech corpora, we examine how i-vector and openSMILE
are associated with contextual factors. Then, we use those features
in the reranking process of RNN-based language models. Results show
that perplexity is reduced by 16% relative and word error rate is reduced
by 2.1% relative for highly emotional speech.
</description>
    </item>
    
    <item>
        <title>Joint Learning of Correlated Sequence Labeling Tasks Using Bidirectional Recurrent Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1247.PDF</link>
        <description>The stream of words produced by Automatic Speech Recognition (ASR)
systems is typically devoid of punctuations and formatting. Most natural
language processing applications expect segmented and well-formatted
texts as input, which is not available in ASR output. This paper proposes
a novel technique of jointly modeling multiple correlated tasks such
as punctuation and capitalization using bidirectional recurrent neural
networks, which leads to improved performance for each of these tasks.
This method could be extended for joint modeling of any other correlated
sequence labeling tasks.
</description>
    </item>
    
    <item>
        <title>Estimation of Gap Between Current Language Models and Human Performance</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0729.PDF</link>
        <description>Language models (LMs) have gained dramatic improvement in the past
years due to the wide application of neural networks. This raises the
question of how far we are away from the perfect language model and
how much more research is needed in language modelling. As for perplexity
giving a value for human perplexity (as an upper bound of what is reasonably
expected from an LM) is difficult. Word error rate (WER) has the disadvantage
that it also measures the quality of other components of a speech recognizer
like the acoustic model and the feature extraction. We therefore suggest
evaluating LMs in a generative setting (which has been done before
on selected hand-picked examples) and running a human evaluation on
the generated sentences. The results imply that LMs need about 10 to
20 more years of research before human performance is reached. Moreover,
we show that the human judgement scores on the generated sentences
and perplexity are closely correlated. This leads to an estimated perplexity
of 12 for an LM that would be able to pass the human judgement test
in the setting we suggested.
</description>
    </item>
    
    <item>
        <title>A Phonological Phrase Sequence Modelling Approach for Resource Efficient and Robust Real-Time Punctuation Recovery</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0204.PDF</link>
        <description>For the automatic punctuation of Automatic Speech Recognition (ASR)
output, both prosodic and text based features are used, often in combination.
Pure prosody based approaches usually have low computation needs, introduce
little latency (delay) and they are also more robust to ASR errors.
Text based approaches usually yield better performance, they are however
resource demanding (both regarding their training and computational
needs), often introduce high time latency and are more sensitive to
ASR errors. The present paper proposes a lightweight prosody based
punctuation approach following a new paradigm: we argue in favour of
an all-inclusive modelling of speech prosody instead of just relying
on distinct acoustic markers: first, the entire phonological phrase
structure is reconstructed, then its close correlation with punctuations
is exploited in a sequence modelling approach with recurrent neural
networks. With this tiny and easy to implement model we reach performance
in Hungarian punctuation comparable to large, text based models for
other languages by keeping resource requirements minimal and suitable
for real-time operation with low latency.
</description>
    </item>
    
    <item>
        <title>Factors Affecting the Intelligibility of Low-Pass Filtered Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0002.PDF</link>
        <description>Frequency compression is an effective alternative to conventional hearing
aids amplification for patients with severe-to-profound middle- and
high-frequency hearing loss and with some low-frequency residual hearing.
In order to develop novel frequency compression strategy, it is important
to first understand the mechanism for recognizing low-pass filtered
speech, which simulates high-frequency hearing loss. The present work
investigated three factors affecting the intelligibility of low-pass
filtered speech, i.e., vowels, temporal fine-structure, and fundamental
frequency (F0) contour. Mandarin sentences were processed to generate
three types (i.e., vowel-only, fine-structure-only, and F0-contour-flattened)
of low-pass filtered stimuli. Listening experiments with normal-hearing
listeners showed that among the three factors assessed, the vowel-only
low-pass filtered speech was the most intelligible, which was followed
by the fine-structure-based low-pass filtered speech. Flattening F0-contour
significantly deteriorated the intelligibility of low-pass filtered
speech.
</description>
    </item>
    
    <item>
        <title>Phonetic Restoration of Temporally Reversed Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0004.PDF</link>
        <description>Early study showed that temporally reversed speech may still be very
intelligible. The present work further assessed the role of acoustic
cues accounting for the intelligibility of temporally reversed speech.
Mandarin sentences were edited to be temporally reversed. Experiment
1 preserved the original consonant segments, and experiment 2 only
preserved the temporally reversed fine-structure waveform. Experimental
results with normal-hearing listeners showed that for Mandarin speech,
listeners could still perfectly understand the temporally reversed
speech with a reversion duration up to 50 ms. Preserving original consonant
segments did not significantly improve the intelligibility of the temporally
reversed speech, suggesting that the reversion processing applied to
vowels largely affected the intelligibility of temporally reversed
speech. When the local short-time envelope waveform was removed, listeners
could still understand stimuli with primarily temporally reversed fine-structure
waveform, suggesting the perceptual role of temporally reversed fine-structure
to the intelligibility of temporally reversed speech.
</description>
    </item>
    
    <item>
        <title>Simultaneous Articulatory and Acoustic Distortion in L1 and L2 Listening: Locally Time-Reversed &amp;#8220;Fast&amp;#8221; Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0083.PDF</link>
        <description>The current study explores how native and non-native speakers cope
with simultaneous articulatory and acoustic distortion in speech perception.
The articulatory distortion was generated by asking a speaker to articulate
target speech as fast as possible (fast speech). The acoustic distortion
was created by dividing speech signals into small segments with equal
time duration (e.g., 50 ms) from the onset of speech, and flipping
every segment on a temporal axis, and putting them back together (locally
time-reversed speech). This study explored how &amp;#8220;locally time-reversed
fast speech&amp;#8221; was intelligible as compared to &amp;#8220;locally time-reversed
normal speech&amp;#8221; measured in Ishida, Samuel, and Arai (2016). Participants
were native English speakers and native Japanese speakers who spoke
English as a second language. They listened to English words and pseudowords
that contained a lot of stop consonants. These items were spoken fast
and locally time-reversed at every 10, 20, 30, 40, 50, or 60 ms. In
general, &amp;#8220;locally time-reversed fast speech&amp;#8221; became gradually
unintelligible as the length of reversed segments increased. Native
speakers generally understood locally time-reversed fast spoken words
well but not pseudowords, while non-native speakers hardly understood
both words and pseudowords. Language proficiency strongly supported
the perceptual restoration of locally time-reversed fast speech.
</description>
    </item>
    
    <item>
        <title>Lexically Guided Perceptual Learning in Mandarin Chinese</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0618.PDF</link>
        <description>Lexically guided perceptual learning refers to the use of lexical knowledge
to retune speech categories and thereby adapt to a novel talker&amp;#8217;s
pronunciation. This adaptation has been extensively documented, but
primarily for segmental-based learning in English and Dutch. In languages
with lexical tone, such as Mandarin Chinese, tonal categories can also
be retuned in this way, but segmental category retuning had not been
studied. We report two experiments in which Mandarin Chinese listeners
were exposed to an ambiguous mixture of [f] and [s] in lexical contexts
favoring an interpretation as either [f] or [s]. Listeners were subsequently
more likely to identify sounds along a continuum between [f] and [s],
and to interpret minimal word pairs, in a manner consistent with this
exposure. Thus lexically guided perceptual learning of segmental categories
had indeed taken place, consistent with suggestions that such learning
may be a universally available adaptation process.
</description>
    </item>
    
    <item>
        <title>The Effect of Spectral Profile on the Intelligibility of Emotional Speech in Noise</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0948.PDF</link>
        <description>The current study investigated why the intelligibility of expressive
speech in noise varies as a function of the emotion expressed (e.g.,
happiness being more intelligible than sadness), even though the signal-to-noise
ratio is the same. We tested the straightforward proposal that the
expression of some emotions affect speech intelligibility by shifting
spectral energy above the energy profile of the noise masker. This
was done by determining how the spectral profile of speech is affected
by different emotional expressions using three different expressive
speech databases. We then examined if these changes were correlated
with scores produced by an objective intelligibility metric. We found
a relatively consistent shift in spectral energy for different emotions
across the databases and a high correlation between the extent of these
changes and the objective intelligibility scores. Moreover, the pattern
of intelligibility scores is consistent with human perception studies
(although there was considerable individual variation). We suggest
that the intelligibility of emotion speech in noise is simply related
to its audibility as conditioned by the effect that the expression
of emotion has on its spectral profile.
</description>
    </item>
    
    <item>
        <title>Whether Long-Term Tracking of Speech Rate Affects Perception Depends on Who is Talking</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1517.PDF</link>
        <description>Speech rate is known to modulate perception of temporally ambiguous
speech sounds. For instance, a vowel may be perceived as short when
the immediate speech context is slow, but as long when the context
is fast. Yet, effects of long-term tracking of speech rate are largely
unexplored. Two experiments tested whether long-term tracking of rate
influences perception of the temporal Dutch vowel contrast /&amp;#593;/-/a:/.
In Experiment 1, one low-rate group listened to &amp;#8216;neutral&amp;#8217;
rate speech from talker A and to slow speech from talker B. Another
high-rate group was exposed to the same neutral speech from A, but
to fast speech from B. Between-group comparison of the &amp;#8216;neutral&amp;#8217;
trials revealed that the low-rate group reported a higher proportion
of /a:/ in A&amp;#8217;s &amp;#8216;neutral&amp;#8217; speech, indicating that
A sounded faster when B was slow. Experiment 2 tested whether one&amp;#8217;s
own speech rate also contributes to effects of long-term tracking of
rate. Here, talker B&amp;#8217;s speech was replaced by playback of participants&amp;#8217;
own fast or slow speech. No evidence was found that one&amp;#8217;s own
voice affected perception of talker A in larger speech contexts. These
results carry implications for our understanding of the mechanisms
involved in rate-dependent speech perception and of dialogue.
</description>
    </item>
    
    <item>
        <title>Emotional Thin-Slicing: A Proposal for a Short- and Long-Term Division of Emotional Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1719.PDF</link>
        <description>Human listeners are adept at successfully recovering linguistically-
and socially-relevant information from very brief utterances. Studies
using the &amp;#8216;thin-slicing&amp;#8217; approach show that accurate judgments
of the speaker&amp;#8217;s emotional state can be made from minimal quantities
of speech. The present experiment tested the performance of listeners
exposed to thin-sliced samples of spoken Brazilian Portuguese selected
to exemplify four emotions ( anger, fear, sadness, happiness). Rather
than attaching verbal labels to the audio samples, participants were
asked to pair the excerpts with averaged facial images illustrating
the four emotion categories. Half of the listeners were native speakers
of Brazilian Portuguese, while the others were native English speakers
who knew no Portuguese. Both groups of participants were found to be
accurate and consistent in assigning the audio samples to the expected
emotion category, but some emotions were more reliably identified than
others.  Fear was misidentified most frequently. We conclude that the
phonetic cues to speakers&amp;#8217; emotional states are sufficiently
salient and differentiated that listeners need only a few syllables
upon which to base judgments, and that as a species we owe our perceptual
sensitivity in this area to the survival value of being able to make
rapid decisions concerning the psychological states of others.
</description>
    </item>
    
    <item>
        <title>Predicting Epenthetic Vowel Quality from Acoustics</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1735.PDF</link>
        <description>Past research has shown that sound sequences not permitted in our native
language may be distorted by our perceptual system. A well-documented
example is vowel epenthesis, a phenomenon by which listeners hallucinate
non-existent vowels within illegal consonantal sequences. As reported
in previous work, this occurs for instance in Japanese (JP) and Brazilian
Portuguese (BP), languages for which the &amp;#8216;default&amp;#8217; epenthetic
vowels are /u/ and /i/, respectively. In a perceptual experiment, we
corroborate the finding that the quality of this illusory vowel is
language-dependent, but also that this default choice can be overridden
by coarticulatory information present on the consonant cluster. In
a second step, we analyse recordings of JP and BP speakers producing
&amp;#8216;epenthesized&amp;#8217; versions of stimuli from the perceptual
task. Results reveal that the default vowel corresponds to the vowel
with the most reduced acoustic characteristics and whose formants are
acoustically closest to formant transitions present in consonantal
clusters. Lastly, we model behavioural responses from the perceptual
experiment with an exemplar model using dynamic time warping (DTW)-based
similarity measures on MFCCs.
</description>
    </item>
    
    <item>
        <title>The Effect of Spectral Tilt on Size Discrimination of Voiced Speech Sounds</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0282.PDF</link>
        <description>A number of studies, with either voiced or unvoiced speech, have demonstrated
that a speaker&amp;#8217;s geometric mean formant frequency (MFF) has a
large effect on the perception of the speaker&amp;#8217;s size, as would
be expected. One study with unvoiced speech showed that lifting the
slope of the speech spectrum by 6 dB/octave also led to a reduction
in the perceived size of the speaker. This paper reports an analogous
experiment to determine whether lifting the slope of the speech spectrum
by 6 dB/octave affects the perception of speaker size with voiced speech
(words). The results showed that voiced speech with high-frequency
enhancement was perceived to arise from smaller speakers. On average,
the point of subjective equality in MFF discrimination was reduced
by about 5%. However, there were large individual differences; some
listeners were effectively insensitive to spectral enhancement of 6
dB/octave; others showed a consistent effect of the same enhancement.
The results suggest that models of speaker size perception will need
to include a listener specific parameter for the effect of spectral
slope.
</description>
    </item>
    
    <item>
        <title>Misperceptions of the Emotional Content of Natural and Vocoded Speech in a Car</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0532.PDF</link>
        <description>This paper analyzes a) how often listeners interpret the emotional
content of an utterance incorrectly when listening to vocoded or natural
speech in adverse conditions; b) which noise conditions cause the most
misperceptions; and c) which group of listeners misinterpret emotions
the most. The long-term goal is to construct new emotional speech synthesizers
that adapt to the environment and to the listener. We performed a large-scale
listening test where over 400 listeners between the ages of 21 and
72 assessed natural and vocoded acted emotional speech stimuli. The
stimuli had been artificially degraded using a room impulse response
recorded in a car and various in-car noise types recorded in a real
car. Experimental results show that the recognition rates for emotions
and perceived emotional strength degrade as signal-to-noise ratio decreases.
Interestingly, misperceptions seem to be more pronounced for negative
and low-arousal emotions such as calmness or anger, while positive
emotions such as happiness appear to be more robust to noise. An ANOVA
analysis of listener meta-data further revealed that gender and age
also influenced results, with elderly male listeners most likely to
incorrectly identify emotions.
</description>
    </item>
    
    <item>
        <title>The Relative Cueing Power of F0 and Duration in German Prominence Perception</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0375.PDF</link>
        <description>Previous studies showed for German and other (West) Germanic language,
including English, that perceived syllable prominence is primarily
controlled by changes in duration and F0, with the latter cue being
more powerful than the former. Our study is an initial approach to
develop this prominence hierarchy further by putting numbers on the
interplay of duration and F0. German listeners indirectly judged through
lexical identification the relative prominence levels of two neighboring
syllables. Results show that an increase in F0 of between 0.49 and
0.76 st is required to outweigh the prominence effect of a 30% increase
in duration of a neighboring syllable. These numbers are fairly stable
across a large range of absolute F0 and duration levels and hence useful
in speech technology.
</description>
    </item>
    
    <item>
        <title>Perception and Acoustics of Vowel Nasality in Brazilian Portuguese</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0570.PDF</link>
        <description>This study explores the relationship between identification, degree
of nasality and vowel quality in oral, nasal and nasalized vowels in
Brazilian Portuguese. Despite common belief that the language possesses
contrastive nasal vowels, literature examination shows that nasal vowels
may be followed by a nasal resonance, while nasalized vowels must be
followed by a nasal consonant. It is argued that the nasal resonance
may be the remains of a consonant that nasalizes the vowel, making
nasal vowels simply coarticulatorily nasalized (e.g. [1]). If so, vowel
nasality should not be more informative for the perception of a word
containing a nasal vowel than for a word containing a nasalized vowel,
as nasality is attributed to coarticulation. To test this hypothesis,
randomized stimuli containing the first syllable of words with oral,
nasal and nasalized vowels were presented to BP listeners who had to
identify the stimuli original word. Preliminary results demonstrate
that accuracy decreased for nasal and nasalized stimuli. A comparison
between patterns of response to measured degrees of vowel acoustic
nasality and formant values demonstrate that vowel quality differences
may play a more relevant role in word identification than type of nasality
in a vowel.
</description>
    </item>
    
    <item>
        <title>Sociophonetic Realizations Guide Subsequent Lexical Access</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1742.PDF</link>
        <description>Previous studies on spoken word recognition suggest that lexical access
is facilitated when social information attributed to the voice is congruent
with the social characteristics associated with the word. This paper
builds on this work, presenting results from a lexical decision task
in which target words associated with different age groups were preceded
by sociophonetic primes. No age-related phonetic cues were provided
within the target words; instead, the non-related prime words contained
a sociophonetic variable involved in ongoing change. We found that
age-associated words are recognized faster when preceded by an age-congruent
phonetic variant in the prime word. The results demonstrate that lexical
access is influenced by sociophonetic variation, a result which we
argue arises from experience-based probabilities of covariation between
sounds and words.
</description>
    </item>
    
    <item>
        <title>Critical Articulators Identification from RT-MRI of the Vocal Tract</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Semantic Edge Detection for Tracking Vocal Tract Air-Tissue Boundaries in Real-Time Magnetic Resonance Images</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1580.PDF</link>
        <description>Recent developments in real-time magnetic resonance imaging (rtMRI)
have enabled the study of vocal tract dynamics during production of
running speech at high frame rates (e.g., 83 frames per second). Such
large amounts of acquired data require scalable automated methods to
identify different articulators (e.g., tongue, velum) for further analysis.
In this paper, we propose a convolutional neural network with an encoder-decoder
architecture to jointly detect the relevant air-tissue boundaries as
well as to label them, which we refer to as &amp;#8216;semantic edge detection&amp;#8217;.
We pose this as a pixel labeling problem, with the outline contour
of each articulator of interest as positive class and the remaining
tissue and airway as negative classes. We introduce a loss function
modified with additional penalty for misclassification at air-tissue
boundaries to account for class imbalance and improve edge localization.
We then use a greedy search algorithm to draw contours from the probability
maps of the positive classes predicted by the network. The articulator
contours obtained by our method are comparable to the true labels generated
by iteratively fitting a manually created subject-specific template.
Our results generalize well across subjects and different vocal tract
postures, demonstrating a significant improvement over the structured
regression baseline.
</description>
    </item>
    
    <item>
        <title>Vocal Tract Airway Tissue Boundary Tracking for rtMRI Using Shape and Appearance Priors</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1016.PDF</link>
        <description>Knowledge about the dynamic shape of the vocal tract is the basis of
many speech production applications such as, articulatory analysis,
modeling and synthesis. Vocal tract airway tissue boundary segmentation
in the mid-sagittal plane is necessary as an initial step for extraction
of the cross-sectional area function. This segmentation problem is
however challenging due to poor resolution of real-time speech MRI,
grainy noise and the rapidly varying vocal tract shape. We present
a novel approach to vocal tract airway tissue boundary tracking by
training a statistical shape and appearance model for human vocal tract.
We manually segment a set of vocal tract profiles and utilize a statistical
approach to train a shape and appearance model for the tract. An active
contour approach is employed to segment the airway tissue boundaries
of the vocal tract while restricting the curve movement to the trained
shape and appearance model. Then the contours in subsequent frames
are tracked using dense motion estimation methods. Experimental evaluations
over the mean square error metric indicate significant improvements
compared to the state-of-the-art.
</description>
    </item>
    
    <item>
        <title>An Objective Critical Distance Measure Based on the Relative Level of Spectral Valley</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0201.PDF</link>
        <description>Spectral integration is a subjective phenomenon in which a vowel with
two formants, spaced below a critical distance, is perceived to be
of the same phonetic quality as that of a vowel with a single formant.
It is tedious to conduct perceptual tests to determine the critical
distance for various experimental conditions. To alleviate this difficulty,
we propose an objective critical distance (OCD) that can be determined
from the spectral envelope of a speech signal. OCD is defined as that
spacing between the adjacent formants when the level of the spectral
valley between them reaches the mean spectral value. The measured OCD
lies in the same range of 3 to 3.5 Bark as the subjective critical
distance for similar experimental conditions giving credibility to
the definition. However, it is noted that OCD for front vowels is significantly
different from that for the back vowels.
</description>
    </item>
    
    <item>
        <title>Database of Volumetric and Real-Time Vocal Tract MRI for Speech Science</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0608.PDF</link>
        <description>We present the USC Speech and Vocal Tract Morphology MRI Database,
a 17-speaker magnetic resonance imaging database for speech research.
The database consists of real-time magnetic resonance images (rtMRI)
of dynamic vocal tract shaping, denoised audio recorded simultaneously
with rtMRI, and 3D volumetric MRI of vocal tract shapes during sustained
speech sounds. We acquired 2D real-time MRI of vocal tract shaping
during consonant-vowel-consonant sequences, vowel-consonant-vowel sequences,
read passages, and spontaneous speech. We acquired 3D volumetric MRI
of the full set of vowels and continuant consonants of American English.
Each 3D volumetric MRI was acquired in one 7-second scan in which the
participant sustained the sound. This is the first database to combine
rtMRI of dynamic vocal tract shaping and 3D volumetric MRI of the entire
vocal tract. The database provides a unique resource with which to
examine the relationship between vocal tract morphology and vocal tract
function. The USC Speech and Vocal Tract Morphology MRI Database is
provided free for research use at  http://sail.usc.edu/span/morphdb.
</description>
    </item>
    
    <item>
        <title>The Influence on Realization and Perception of Lexical Tones from Affricate&amp;#8217;s Aspiration</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1267.PDF</link>
        <description>Consonants in /CV/ syllables usually have potential influence on onset
fundamental frequency (i.e., onset f0) of succeeding vowels. Previous
studies showed such effect with respect to the aspiration of stops
with evidence from Mandarin, a tonal language. While few studies investigated
the effect on onset f0 from the aspiration of affricates. The differences
between stops and affricates in aspiration leave space for further
investigations. We examined the effect of affricate&amp;#8217;s aspiration
on the realization of onset f0 of following vowels in the form of isolated
syllables and continuous speech by reference to a minimal pair of syllables
which differ only in aspiration. Besides, we conducted tone identification
tests using two sets of tone continua based on the same minimal pair
of syllables. Experimental results showed that the aspirated syllables
increased the onset f0 of following vowels compared with unaspirated
counterparts in both kinds of contexts. While the magnitude of differences
varied with tones. And the perception results showed that aspirated
syllables tended to be perceived as tones that have relative lower
onset f0, which in turn supported the production result. The present
study may have applications for speech identification and speech synthesis.
</description>
    </item>
    
    <item>
        <title>Audiovisual Recalibration of Vowel Categories</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>The Effect of Gesture on Persuasive Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0194.PDF</link>
        <description>Speech perception is multimodal, with not only speech, but also gesture
presumably playing a role in how a message is perceived. However, there
have not been many studies on the effect that hand gestures may have
on speech perception in general, and on persuasive speech in particular.
Moreover, we do not yet know whether an effect of gestures may be larger
when addressees are not involved in the topic of the discourse, and
are therefore more focused on peripheral cues, rather than the content
of the message. In the current study participants were shown a speech
with or without gestures. Some participants were involved in the topic
of the speech, others were not. We studied five measures of persuasiveness.
Results showed that for all but one measure, viewing the video with
accompanying gestures made the speech more persuasive. In addition,
there were several interactions, showing that the performance of the
speaker and the factual accuracy of the speech scored high especially
for those participants who not only saw gestures but were also not
involved in the topic of the speech.
</description>
    </item>
    
    <item>
        <title>Auditory-Visual Integration of Talker Gender in Cantonese Tone Perception</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1069.PDF</link>
        <description>This study investigated the auditory-visual integration of talker gender
in the perception of tone variances. Two experiments were conducted
to evaluate how listeners use the information of talker gender to adjust
their expectation towards speakers&amp;#8217; pitch range and uncover intended
tonal targets in Cantonese tone perception. Results from an audio-only
tone identification task showed that tone categorization along the
same pitch continuum shifted under different conditions of voice gender.
Listeners generally heard a tone of lower pitch when the word was produced
by a female voice, while they heard a tone of higher pitch when the
word was produced at the same pitch level by a male voice. Results
from an audio-visual tone identification task showed that tone categorization
along the same pitch continuum shifted under different conditions of
face gender, despite the fact that the photos of different genders
were disguised for the same set of stimuli in identical voices with
identical pitch heights. These findings show that gender normalization
plays a role in uncovering linguistic pitch targets, and lend support
to a hypothesis according to which listeners make use of socially constructed
stereotypes to facilitate their basic phonological categorization in
speech perception and processing.
</description>
    </item>
    
    <item>
        <title>Event-Related Potentials Associated with Somatosensory Effect in Audio-Visual Speech Perception</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0139.PDF</link>
        <description>Speech perception often involves multisensory processing. Although
previous studies have demonstrated visual [1, 2] and somatosensory
interactions [3, 4] with auditory processing, it is not clear whether
somatosensory information can contribute to the processing of audio-visual
speech perception. This study explored the neural consequence of somatosensory
interactions in audio-visual speech processing. We assessed whether
somatosensory orofacial stimulation influenced event-related potentials
(ERPs) in response to an audio-visual speech illusion (the McGurk Effect
[1]). 64 scalp sites of ERPs were recorded in response to audio-visual
speech stimulation and somatosensory stimulation. In the audio-visual
condition, an auditory stimulus /ba/ was synchronized with the video
of congruent facial motion (the production of /ba/) or incongruent
facial motion (the production of the /da/: McGurk condition). These
two audio-visual stimulations were randomly presented with and without
somatosensory stimulation associated with facial skin deformation.
We found ERPs differences associated with the McGurk effect in the
presence of the somatosensory conditions. ERPs for the McGurk effect
reliably diverge around 280 ms after auditory onset. The results demonstrate
a change of cortical potential of audio-visual processing due to somatosensory
inputs and suggest that somatosensory information encoding facial motion
also influences speech processing.
</description>
    </item>
    
    <item>
        <title>When a Dog is a Cat and How it Changes Your Pupil Size: Pupil Dilation in Response to Information Mismatch</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0353.PDF</link>
        <description>In the present study, we investigate pupil dilation as a measure of
lexical retrieval. We captured pupil size changes in reaction to a
match or a mismatch between a picture and an auditorily presented word
in 120 trials presented to ten native speakers of Swedish. In each
trial a picture was displayed for six seconds, and 2.5 seconds into
the trial the word was played through loudspeakers. The picture and
the word were matching in half of the trials, and all stimuli were
common high-frequency monosyllabic Swedish words. The difference in
pupil diameter trajectories across the two conditions was analyzed
with Functional Data Analysis. In line with the expectations, the results
indicate greater dilation in the mismatch condition starting from around
800 ms after the stimulus onset. Given that similar processes were
observed in brain imaging studies, pupil dilation measurements seem
to provide an appropriate tool to reveal lexical retrieval. The results
suggest that pupillometry could be a viable alternative to existing
methods in the field of speech and language processing, for instance
across different ages and clinical groups.
</description>
    </item>
    
    <item>
        <title>Cross-Modal Analysis Between Phonation Differences and Texture Images Based on Sentiment Correlations</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1236.PDF</link>
        <description>Motivated by the success of speech characteristics representation by
color attributes, we analyzed the cross-modal sentiment correlations
between voice source characteristics and textural image characteristics.
For the analysis, we employed vowel sounds with representative three
phonation differences (modal, creaky and breathy) and 36 texture images
with 36 semantic attributes (e.g., banded, cracked and scaly) annotated
one semantic attribute for each texture. By asking 40 subjects to select
the most fitted textures from 36 figures with different textures after
listening 30 speech samples with different phonations, we measured
the correlations between acoustic parameters showing voice source variations
and the parameters of selected textural image differences showing coarseness,
contrast, directionality, busyness, complexity and strength. From the
texture classifications, voice characteristics can be roughly characterized
by textural differences: modal &amp;#8212; gauzy, banded and smeared, creaky
&amp;#8212; porous, crystalline, cracked and scaly, breathy &amp;#8212; smeared,
freckled and stained. We have also found significant correlations between
voice source acoustic parameters and textural parameters. These correlations
suggest the possibility of cross-modal mapping between voice source
characteristics and textural parameters, which enables visualization
of speech information with source variations reflecting human sentiment
perception.
</description>
    </item>
    
    <item>
        <title>Wireless Neck-Surface Accelerometer and Microphone on Flex Circuit with Application to Noise-Robust Monitoring of Lombard Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0048.PDF</link>
        <description>Ambulatory monitoring of real-world voice characteristics and behavior
has the potential to provide important assessment of voice and speech
disorders and psychological and emotional state. In this paper, we
report on the novel development of a lightweight, wireless voice monitor
that synchronously records dual-channel data from an acoustic microphone
and a neck-surface accelerometer embedded on a flex circuit. In this
paper, Lombard speech effects were investigated in pilot data from
four adult speakers with normal vocal function who read a phonetically
balanced paragraph in the presence of different ambient acoustic noise
levels. Whereas the signal-to-noise ratio (SNR) of the microphone signal
decreased in the presence of increasing ambient noise level, the SNR
of the accelerometer sensor remained high. Lombard speech properties
were thus robustly computed from the accelerometer signal and observed
in all four speakers who exhibited increases in average estimates of
sound pressure level (+2.3 dB), fundamental frequency (+21.4 Hz), and
cepstral peak prominence (+1.3 dB) from quiet to loud ambient conditions.
Future work calls for ambulatory data collection in naturalistic environments,
where the microphone acts as a sound level meter and the accelerometer
functions as a noise-robust voicing sensor to assess voice disorders,
neurological conditions, and cognitive load.
</description>
    </item>
    
    <item>
        <title>Video-Based Tracking of Jaw Movements During Speech: Preliminary Results and Future Directions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1371.PDF</link>
        <description>Facial (e.g., lips and jaw) movements can provide important information
for the assessment, diagnosis and treatment of motor speech disorders.
However, due to the high costs of the instrumentation used to record
speech movements, such information is typically limited to research
studies. With the recent development of depth sensors and efficient
algorithms for facial tracking, clinical applications of this technology
may be possible. Although lip tracking methods have been validated
in the past, jaw tracking remains a challenge. In this study, we assessed
the accuracy of tracking jaw movements with a video-based system composed
of a face tracker and a depth sensor, specifically developed for short
range applications (Intel RealSense SR300). The assessment was performed
on healthy subjects during speech and non-speech tasks. Preliminary
results showed that jaw movements can be tracked with reasonable accuracy
(RMSE&amp;#8776;2mm), with better performance for slow movements. Further
tests are needed in order to improve the performance of these systems
and develop accurate methodologies that can reveal subtle changes in
jaw movements for the assessment and treatment of motor speech disorders.
</description>
    </item>
    
    <item>
        <title>Accurate Synchronization of Speech and EGG Signal Using Phase Information</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1374.PDF</link>
        <description>Synchronization of speech and corresponding Electroglottographic (EGG)
signal is very helpful for speech processing research and development.
During simultaneous recording of speech and EGG signals, the speech
signal will be delayed by the duration corresponding to the speech
wave propagation from the glottis to the microphone relative to the
EGG signal. Even in same session of recording, the delay between the
speech and the EGG signals is varying due to the natural movement of
speaker&amp;#8217;s head and movement of microphone in case MIC is held
by hand. To study and model the information within glottal cycles,
precise synchronization of speech and EGG signals is of utmost necessity.
In this work, we propose a method for synchronization of speech and
EGG signals based on the glottal activity information present in the
signals. The performance of the proposed method is demonstrated by
estimation of delay between the two signals (speech signals and corresponding
EGG signals) and synchronizing these signals by compensating the estimated
delay. The CMU-Arctic database consist of simultaneous recording of
the speech and the EGG signals is used for the evaluation of the proposed
method.
</description>
    </item>
    
    <item>
        <title>The Acquisition of Focal Lengthening in Stockholm Swedish</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1065.PDF</link>
        <description>In order to be efficient communicators, children need to adapt their
utterances to the common ground shared between themselves and their
conversational partners. One way of doing this is by prosodically highlighting
focal information. In this paper we look at one specific prosodic manipulation,
namely word duration, asking whether Swedish-speaking children lengthen
words to mark focus, as compared to adult controls. To the best of
our knowledge, this is the first study on the relationship between
focus and word duration in Swedish-speaking children.
</description>
    </item>
    
    <item>
        <title>Multilingual Recurrent Neural Networks with Residual Learning for Low-Resource Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0111.PDF</link>
        <description>The shared-hidden-layer multilingual deep neural network (SHL-MDNN),
in which the hidden layers of feed-forward deep neural network (DNN)
are shared across multiple languages while the softmax layers are language
dependent, has been shown to be effective on acoustic modeling of multilingual
low-resource speech recognition. In this paper, we propose that the
shared-hidden-layer with Long Short-Term Memory (LSTM) recurrent neural
networks can achieve further performance improvement considering LSTM
has outperformed DNN as the acoustic model of automatic speech recognition
(ASR). Moreover, we reveal that shared-hidden-layer multilingual LSTM
(SHL-MLSTM) with residual learning can yield additional moderate but
consistent gain from multilingual tasks given the fact that residual
learning can alleviate the degradation problem of deep LSTMs. Experimental
results demonstrate that SHL-MLSTM can relatively reduce word error
rate (WER) by 2.1&amp;#8211;6.8% over SHL-MDNN trained using six languages
and 2.6&amp;#8211;7.3% over monolingual LSTM trained using the language
specific data on CALLHOME datasets. Additional WER reduction, about
relatively 2% over SHL-MLSTM, can be obtained through residual learning
on CALLHOME datasets, which demonstrates residual learning is useful
for SHL-MLSTM on multilingual low-resource ASR.
</description>
    </item>
    
    <item>
        <title>CTC Training of Multi-Phone Acoustic Models for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0505.PDF</link>
        <description>Phone-sized acoustic units such as triphones cannot properly capture
the long-term co-articulation effects that occur in spontaneous speech.
For that reason, it is interesting to construct acoustic units covering
a longer time-span such as syllables or words. Unfortunately, the frequency
distribution of those units is such that a few high frequency units
account for most of the tokens, while many units rarely occur. As a
result, those units suffer from data sparsity and can be difficult
to train. In this paper we propose a scalable data-driven approach
to construct a set of salient units made of sequences of phones called
M-phones. We illustrate that since the decomposition of a word sequence
into a sequence of M-phones is ambiguous, those units are well suited
to be used with a connectionist temporal classification (CTC) approach
which does not rely on an explicit frame-level segmentation of the
word sequence into a sequence of acoustic units. Experiments are presented
on a Voice Search task using 12,500 hours of training data.
</description>
    </item>
    
    <item>
        <title>An Investigation of Deep Neural Networks for Multilingual Speech Recognition Training and Adaptation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>2016 BUT Babel System: Multilingual BLSTM Acoustic Model with i-Vector Based Adaptation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1775.PDF</link>
        <description>The paper provides an analysis of BUT automatic speech recognition
systems (ASR) built for the 2016 IARPA Babel evaluation. The IARPA
Babel program concentrates on building ASR system for many low resource
languages, where only a limited amount of transcribed speech is available
for each language. In such scenario, we found essential to train the
ASR systems in a multilingual fashion. In this work, we report superior
results obtained with pre-trained multilingual BLSTM acoustic models,
where we used multi-task training with separate classification layer
for each language. The results reported on three Babel Year 4 languages
show over 3% absolute WER reductions obtained from such multilingual
pre-training. Experiments with different input features show that the
multilingual BLSTM performs the best with simple log-Mel-filter-bank
outputs, which makes our previously successful multilingual stack bottleneck
features with CMLLR adaptation obsolete. Finally, we experiment with
different configurations of i-vector based speaker adaptation in the
mono- and multi-lingual BLSTM architectures. This results in additional
WER reductions over 1% absolute.
</description>
    </item>
    
    <item>
        <title>Optimizing DNN Adaptation for Recognition of Enhanced Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0755.PDF</link>
        <description>Speech enhancement directly using deep neural network (DNN) is of major
interest due to the capability of DNN to tangibly reduce the impact
of noisy conditions in speech recognition tasks. Similarly, DNN based
acoustic model adaptation to new environmental conditions is another
challenging topic. In this paper we present an analysis of acoustic
model adaptation in presence of a disjoint speech enhancement component,
identifying an optimal setting for improving the speech recognition
performance. Adaptation is derived from a consolidated technique that
introduces in the training process a regularization term to prevent
overfitting. We propose to optimize the adaptation of the clean acoustic
models towards the enhanced speech by tuning the regularization term
based on the degree of enhancement. Experiments on a popular noisy
dataset (e.g., AURORA-4) demonstrate the validity of the proposed approach.
</description>
    </item>
    
    <item>
        <title>Deep Least Squares Regression for Speaker Adaptation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0783.PDF</link>
        <description>Recently, speaker adaptation methods in deep neural networks (DNNs)
have been widely studied for automatic speech recognition. However,
almost all adaptation methods for DNNs have to consider various heuristic
conditions such as mini-batch sizes, learning rate scheduling, stopping
criteria, and initialization conditions because of the inherent property
of a stochastic gradient descent (SGD)-based training process. Unfortunately,
those heuristic conditions are hard to be properly tuned. To alleviate
those difficulties, in this paper, we propose a least squares regression-based
speaker adaptation method in a DNN framework utilizing posterior mean
of each class. Also, we show how the proposed method can provide a
unique solution which is quite easy and fast to calculate without SGD.
The proposed method was evaluated in the TED-LIUM corpus. Experimental
results showed that the proposed method achieved up to a 4.6% relative
improvement against a speaker independent DNN. In addition, we report
further performance improvement of the proposed method with speaker-adapted
features.
</description>
    </item>
    
    <item>
        <title>Multi-Task Learning Using Mismatched Transcription for Under-Resourced Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0788.PDF</link>
        <description>It is challenging to obtain large amounts of native (matched) labels
for audio in under-resourced languages. This could be due to a lack
of literate speakers of the language or a lack of universally acknowledged
orthography. One solution is to increase the amount of labeled data
by using mismatched transcription, which employs transcribers who do
not speak the language (in place of native speakers), to transcribe
what they hear as nonsense speech in their own language (e.g., Mandarin).
This paper presents a multi-task learning framework where the DNN acoustic
model is simultaneously trained using both a limited amount of native
(matched) transcription and a larger set of mismatched transcription.
We find that by using a multi-task learning framework, we achieve improvements
over monolingual baselines and previously proposed mismatched transcription
adaptation techniques. In addition, we show that using alignments provided
by a GMM adapted by mismatched transcription further improves acoustic
modeling performance. Our experiments on Georgian data from the IARPA
Babel program show the effectiveness of the proposed method.
</description>
    </item>
    
    <item>
        <title>Generalized Distillation Framework for Speaker Normalization</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0874.PDF</link>
        <description>Generalized distillation framework has been shown to be effective in
speech enhancement in the past. We extend this idea to speaker normalization
without any explicit adaptation data in this paper. In the generalized
distillation framework, we assume the presence of some &amp;#8220;privileged&amp;#8221;
information to guide the training process in addition to the training
data. In the proposed approach, the privileged information is obtained
from a &amp;#8220;teacher&amp;#8221; model, trained on speaker-normalized FMLLR
features. The &amp;#8220;student&amp;#8221; model is trained on un-normalized
filterbank features and uses teacher&amp;#8217;s supervision for cross-entropy
training. The proposed distillation method does not need first pass
decode information during testing and imposes no constraints on the
duration of the test data for computing speaker-specific transforms
unlike in FMLLR or  i-vector. Experiments done on Switchboard and AMI
corpus show that the generalized distillation framework shows improvement
over un-normalized features with or without  i-vectors.
</description>
    </item>
    
    <item>
        <title>Learning Factorized Transforms for Unsupervised Adaptation of LSTM-RNN Acoustic Models</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1136.PDF</link>
        <description>Factorized Hidden Layer (FHL) adaptation has been proposed for speaker
adaptation of deep neural network (DNN) based acoustic models. In FHL
adaptation, a speaker-dependent (SD) transformation matrix and an SD
bias are included in addition to the standard affine transformation.
The SD transformation is a linear combination of rank-1 matrices whereas
the SD bias is a linear combination of vectors. Recently, the Long
Short-Term Memory (LSTM) Recurrent Neural Networks (RNNs) have shown
to outperform DNN acoustic models in many Automatic Speech Recognition
(ASR) tasks. In this work, we investigate the effectiveness of SD transformations
for LSTM-RNN acoustic models. Experimental results show that when combined
with scaling of LSTM cell states&amp;#8217; outputs, SD transformations
achieve 2.3% and 2.1% absolute improvements over the baseline LSTM
systems for the AMI IHM and AMI SDM tasks respectively.
</description>
    </item>
    
    <item>
        <title>Factorised Representations for Neural Network Adaptation to Diverse Acoustic Environments</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1365.PDF</link>
        <description>Adapting acoustic models jointly to both speaker and environment has
been shown to be effective. In many realistic scenarios, however, either
the speaker or environment at test time might be unknown, or there
may be insufficient data to learn a joint transform. Generating independent
speaker and environment transforms improves the match of an acoustic
model to unseen combinations. Using i-vectors, we demonstrate that
it is possible to factorise speaker or environment information using
multi-condition training with neural networks. Specifically, we extract
bottleneck features from networks trained to classify either speakers
or environments. We perform experiments on the Wall Street Journal
corpus combined with environment noise from the Diverse Environments
Multichannel Acoustic Noise Database. Using the factorised i-vectors
we show improvements in word error rates on perturbed versions of the
eval92 and dev93 test sets, both when one factor is missing and when
the factors are seen but not in the desired combination.
</description>
    </item>
    
    <item>
        <title>An RNN Model of Text Normalization</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0035.PDF</link>
        <description>We present a recurrent neural net (RNN) model of text normalization
&amp;#8212; defined as the mapping of  written text to its  spoken form,
and a description of the open-source dataset that we used in our experiments.
We show that while the RNN model achieves very high overall accuracies,
there remain errors that would be unacceptable in a speech application
like TTS. We then show that a simple FST-based filter can help mitigate
those errors. Even with that mitigation challenges remain, and we end
the paper outlining some possible solutions. In releasing our data
we are thereby inviting others to help solve this problem.
</description>
    </item>
    
    <item>
        <title>Weakly-Supervised Phrase Assignment from Text in a Speech-Synthesis System Using Noisy Labels</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0487.PDF</link>
        <description>The proper segmentation of an input text string into meaningful intonational
phrase units is a fundamental task in the text-processing component
of a text-to-speech (TTS) system that generates intelligible and natural
synthesis. In this work we look at the creation of a symbolic, phrase-assignment
model within the front end (FE) of a North American English TTS system
when high-quality labels for supervised learning are unavailable and/or
potentially mismatched to the target corpus and domain. We explore
a labeling scheme that merges heuristics derived from (i) automatic
high-quality phonetic alignments, (ii) linguistic rules, and (iii)
a legacy acoustic phrase-labeling system to arrive at a ground truth
that can be used to train a bidirectional recurrent neural network
model. We evaluate the performance of this model in terms of objective
metrics describing categorical phrase assignment within the FE proper,
as well as on the effect that these intermediate labels carry onto
the TTS back end for the task of continuous prosody prediction (i.e.,
intonation and duration contours, and pausing). For this second task,
we rely on subjective listening tests and demonstrate that the proposed
system significantly outperforms a linguistic rules-based baseline
for two different synthetic voices.
</description>
    </item>
    
    <item>
        <title>Prosody Aware Word-Level Encoder Based on BLSTM-RNNs for DNN-Based Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0521.PDF</link>
        <description>Recent studies have shown the effectiveness of the use of word vectors
in DNN-based speech synthesis. However, these word vectors trained
from a large amount of text generally carry not prosodic information,
which is important information for speech synthesis, but semantic information.
Therefore, if word vectors that take prosodic information into account
can be obtained, it would be expected to improve the quality of synthesized
speech. In this paper, to obtain word-level vectors that take prosodic
information into account, we propose a novel prosody aware word-level
encoder. A novel point of the proposed technique is to train a word-level
encoder by using a large speech corpus constructed for automatic speech
recognition. A word-level encoder that estimates the F0 contour for
each word from the input word sequence is trained. The outputs of the
bottleneck layer in the trained encoder are used as the word-level
vector. By training the relationship between words and their prosodic
information by using large speech corpus, the outputs of the bottleneck
layer would be expected to contain prosodic information. The results
of objective and subjective experiments indicate the proposed technique
can synthesize speech with improved naturalness.
</description>
    </item>
    
    <item>
        <title>Global Syllable Vectors for Building TTS Front-End with Deep Learning</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0669.PDF</link>
        <description>Recent vector space representations of words have succeeded in capturing
syntactic and semantic regularities. In the context of text-to-speech
(TTS) synthesis, a front-end is a key component for extracting multi-level
linguistic features from text, where syllable acts as a link between
low- and high-level features. This paper describes the use of global
syllable vectors as features to build a front-end, particularly evaluated
in Chinese. The global syllable vectors directly capture global statistics
of syllable-syllable co-occurrences in a large-scale text corpus. They
are learned by a global log-bilinear regression model in an unsupervised
manner, whilst the front-end is built using deep bidirectional recurrent
neural networks in a supervised fashion. Experiments are conducted
on large-scale Chinese speech and treebank text corpora, evaluating
grapheme to phoneme (G2P) conversion, word segmentation, part of speech
(POS) tagging, phrasal chunking, and pause break prediction. Results
show that the proposed method is efficient for building a compact and
robust front-end with high performance. The global syllable vectors
can be acquired relatively cheaply from plain text resources, therefore,
they are vital to develop multilingual speech synthesis, especially
for under-resourced language modeling.
</description>
    </item>
    
    <item>
        <title>Prosody Control of Utterance Sequence for Information Delivering</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0708.PDF</link>
        <description>We propose a conversational speech synthesis system in which the prosodic
features of each utterance are controlled throughout the entire input
text. We have developed a &amp;#8220;news-telling system,&amp;#8221; which
delivered news articles through spoken language. The speech synthesis
system for the news-telling should be able to highlight utterances
containing noteworthy information in the article with a particular
way of speaking so as to impress them on the users. To achieve this,
we introduced role and position features of the individual utterances
in the article into the control parameters for prosody generation throughout
the text. We defined three categories for the role feature: a nucleus
(which is assigned to the utterance including the noteworthy information),
a front satellite (which precedes the nucleus) and a rear satellite
(which follows the nucleus). We investigated how the prosodic features
differed depending on the role and position features through an analysis
of news-telling speech data uttered by a voice actress. We designed
the speech synthesis system on the basis of a deep neural network having
the role and position features added to its input layer. Objective
and subjective evaluation results showed that introducing those features
was effective in the speech synthesis for the information delivering.
</description>
    </item>
    
    <item>
        <title>Multi-Task Learning for Prosodic Structure Generation Using BLSTM RNN with Structured Output Layer</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0949.PDF</link>
        <description>Prosodic structure generation from text plays an important role in
Chinese text-to-speech (TTS) synthesis, which greatly influences the
naturalness and intelligibility of the synthesized speech. This paper
proposes a multi-task learning method for prosodic structure generation
using bidirectional long short-term memory (BLSTM) recurrent neural
network (RNN) and structured output layer (SOL). Unlike traditional
methods where prerequisites such as lexicon word or even syntactic
tree are usually required as the input, the proposed method predicts
prosodic boundary labels directly from Chinese characters. BLSTM RNN
is used to capture the bidirectional contextual dependencies of prosodic
boundary labels. SOL further models correlations between prosodic structures,
lexicon words as well as part-of-speech (POS), where the prediction
of prosodic boundary labels are conditioned upon word tokenization
and POS tagging results. Experimental results demonstrate the effectiveness
of the proposed method.
</description>
    </item>
    
    <item>
        <title>Investigating Efficient Feature Representation Methods and Training Objective for BLSTM-Based Phone Duration Prediction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1086.PDF</link>
        <description>Accurate modeling and prediction of speech-sound durations are important
in generating natural synthetic speech. This paper focuses on both
feature and training objective aspects to improve the performance of
the phone duration model for speech synthesis system. In feature aspect,
we combine the feature representation from gradient boosting decision
tree (GBDT) and phoneme identity embedding model (which is realized
by the jointly training of phoneme embedded vector (PEV) and word embedded
vector (WEV)) for BLSTM to predict the phone duration. The PEV is used
to replace the one-hot phoneme identity, and GBDT is utilized to transform
the traditional contextual features. In the training objective aspect,
a new training objective function which taking into account of the
correlation and consistency between the predicted utterance and the
natural utterance is proposed. Perceptual tests indicate the proposed
methods could improve the naturalness of the synthetic speech, which
benefits from the proposed feature representation methods could capture
more precise contextual features, and the proposed training objective
function could tackle the over-averaged problem for the generated phone
durations.
</description>
    </item>
    
    <item>
        <title>Discrete Duration Model for Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1144.PDF</link>
        <description>The acoustic model and the duration model are the two major components
in statistical parametric speech synthesis (SPSS) systems. The neural
network based acoustic model makes it possible to model phoneme duration
at phone-level instead of state-level in conventional hidden Markov
model (HMM) based SPSS systems. Since the duration of phonemes is countable
value, the distribution of the phone-level duration is discrete given
the linguistic features, which means the Gaussian hypothesis is no
longer necessary. This paper provides an investigation on the performance
of LSTM-RNN duration model that directly models the probability of
the countable duration values given linguistic features using cross
entropy as criteria. The multi-task learning is also experimented at
the same time, with a comparison to the standard LSTM-RNN duration
model in objective and subjective measures. The result shows that directly
modeling the discrete distribution has its benefit and multi-task model
achieves better performance in phone-level duration modeling.
</description>
    </item>
    
    <item>
        <title>Comparison of Modeling Target in LSTM-RNN Duration Model</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1152.PDF</link>
        <description>Speech duration is an important component in statistical parameter
speech synthesis(SPSS). In LSTM-RNN based SPSS system, the speech duration
affects the quality of synthesized speech in two aspects, the prosody
of speech and the position features in acoustic model. This paper investigated
the effects of duration in LSTM-RNN based SPSS system. The performance
of the acoustic models with position features at different levels are
compared. Also, duration models with different network architectures
are presented. A method to utilize the priori knowledge that the sum
of state duration of a phoneme should be equal to the phone duration
is proposed and proved to have better performance in both state duration
and phone duration modeling. The result shows that acoustic model with
state-level position features has better performance in acoustic modeling
(especially in voice/unvoice classification), which means state-level
duration model still has its advantage and the duration models with
the priori knowledge can result in better speech quality.
</description>
    </item>
    
    <item>
        <title>Learning Word Vector Representations Based on Acoustic Counts</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1340.PDF</link>
        <description>This paper presents a simple count-based approach to learning word
vector representations by leveraging statistics of co-occurrences between
text and speech. This type of representation requires two discrete
sequences of units defined across modalities. Two possible methods
for the discretization of an acoustic signal are presented, which are
then applied to fundamental frequency and energy contours of a transcribed
corpus of speech, yielding a sequence of textual objects (e.g. words,
syllables) aligned with a sequence of discrete acoustic events. Constructing
a matrix recording the co-occurrence of textual objects with acoustic
events and reducing its dimensionality with matrix decomposition results
in a set of context-independent representations of word types. These
are applied to the task of acoustic modelling for speech synthesis;
objective and subjective results indicate that these representations
are useful for the generation of acoustic parameters in a text-to-speech
(TTS) system. In general, we observe that the more discretization approaches,
acoustic signals, and levels of linguistic analysis are incorporated
into a TTS system via these count-based representations, the better
that TTS system performs.
</description>
    </item>
    
    <item>
        <title>Synthesising Uncertainty: The Interplay of Vocal Effort and Hesitation Disfluencies</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1507.PDF</link>
        <description>As synthetic voices become more flexible, and conversational systems
gain more potential to adapt to the environmental and social situation,
the question needs to be examined, how different modifications to the
synthetic speech interact with each other and how their specific combinations
influence perception. This work investigates how the vocal effort of
the synthetic speech together with added disfluencies affect listeners&amp;#8217;
perception of the degree of uncertainty in an utterance. We introduce
a DNN voice built entirely from spontaneous conversational speech data
and capable of producing a continuum of vocal efforts, prolongations
and filled pauses with a corpus-based method. Results of a listener
evaluation indicate that decreased vocal effort, filled pauses and
prolongation of function words increase the degree of perceived uncertainty
of conversational utterances expressing the speaker&amp;#8217;s beliefs.
We demonstrate that the effect of these three cues are not merely additive,
but that interaction effects, in particular between the two types of
disfluencies and between vocal effort and prolongations need to be
considered when aiming to communicate a specific level of uncertainty.
The implications of these findings are relevant for adaptive and incremental
conversational systems using expressive speech synthesis and aspiring
to communicate the attitude of uncertainty.
</description>
    </item>
    
    <item>
        <title>Prosograph: A Tool for Prosody Visualisation of Large Speech Corpora</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2034.PDF</link>
        <description>This paper presents an open-source tool that has been developed to
visualize a speech corpus with its transcript and prosodic features
aligned at word level. In particular, the tool is aimed at providing
a simple and clear way to visualize prosodic patterns along large segments
of speech corpora, and can be applied in any research that involves
prosody analysis.
</description>
    </item>
    
    <item>
        <title>ChunkitApp: Investigating the Relevant Units of Online Speech Processing</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2048.PDF</link>
        <description>This paper presents a web-based application for tablets &amp;#8216;ChunkitApp&amp;#8217;
developed to investigate chunking in online speech processing. The
design of the app is based on recent theoretical developments in linguistics
and cognitive science, and in particular on the suggestions of Linear
Unit Grammar [1]. The data collected using the app provides evidence
for the reality of online chunking in language processing and the validity
of the construct. In addition to experimental uses, the app has potential
applications in language education and speech recognition.
</description>
    </item>
    
    <item>
        <title>Extending the EMU Speech Database Management System: Cloud Hosting, Team Collaboration, Automatic Revision Control</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2049.PDF</link>
        <description>In this paper, we introduce a new component of the EMU Speech Database
Management System [1, 2] to improve the team workflow of handling production
data (both acoustic and physiological) in phonetics and the speech
sciences. It is named  emuDB Manager, and it facilitates the coordination
of team efforts, possibly distributed over several nations, by introducing
automatic revision control (based on Git), cloud hosting (in private
clouds provided by the researchers themselves or a third party), by
keeping track of which parts of the database have already been edited
(and by whom), and by centrally collecting and making searchable the
notes made during the edit process.
</description>
    </item>
    
    <item>
        <title>HomeBank: A Repository for Long-Form Real-World Audio Recordings of Children</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2051.PDF</link>
        <description>HomeBank is a new component of the TalkBank system, focused on long-form
(i.e., multi-hour, typically daylong) real-world recordings of children&amp;#8217;s
language experiences, and it is linked to a GitHub repository in which
tools for analyzing those recordings can be shared. HomeBank constitutes
not only a rich resource for researchers interested in early language
acquisition specifically, but also for those seeking to study spontaneous
speech, media exposure, and audio environments more generally. This
Show and Tell describes the procedures for accessing and contributing
HomeBank data and code. It also overviews the current contents of the
repositories, and provides some examples of audio recordings, available
transcriptions, and currently available analysis tools.
</description>
    </item>
    
    <item>
        <title>A System for Real Time Collaborative Transcription Correction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2052.PDF</link>
        <description>We present a system to enable efficient, collaborative human correction
of ASR transcripts, designed to operate in real-time situations, for
example, when post-editing live captions generated for news broadcasts.
In the system, confusion networks derived from ASR lattices are used
to highlight low-confident words and present alternatives to the user
for quick correction. The system uses a client-server architecture,
whereby information about each manual edit is posted to the server.
Such information can be used to dynamically update the one-best ASR
output for all utterances currently in the editing pipeline. We propose
to make updates in three different ways; by finding a new one-best
path through an existing ASR lattice consistent with the correction
received; by identifying further instances of out-of-vocabulary terms
entered by the user; and by adapting the language model on the fly.
Updates are received asynchronously by the client.
</description>
    </item>
    
    <item>
        <title>MoPAReST &amp;#8212; Mobile Phone Assisted Remote Speech Therapy Platform</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2058.PDF</link>
        <description>Through this paper, we present the Mobile Phone Assisted Remote Speech
Therapy Platform for individuals with speech disabilities to avail
the benefits of therapy remotely with minimal  face-to-face sessions
with the Speech Language Pathologist (SLP). The objective is to address
the skewed ratio of SLP to patients as well increase the efficacy of
the therapy by keeping the patient engaged more frequently albeit asynchronously
and remotely. The platform comprises (1) A web-interface to be used
by the SLP to monitor the progress of their patients at a time convenient
to them and (2) A mobile application along with speech processing algorithms
to provide instant feedback to the patient. We envision this platform
to cut down the therapy time, especially for rural Indian patients.
Evaluation of this platform is being done for five patients with mis-articulation
in Marathi language.
</description>
    </item>
    
    <item>
        <title>An Apparatus to Investigate Western Opera Singing Skill Learning Using Performance and Result Biofeedback, and Measuring its Neural Correlates</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2035.PDF</link>
        <description>We present our preliminary developments on a biofeedback interface
for Western operatic style training, combining performance and result
biofeedback. Electromyographic performance feedbacks, as well as formant-tuning
result feedbacks are displayed visually, using continuously scrolling
displays, or discrete post-trial evaluations. Our final aim is to investigate
electroencephalographic (EEG) measurements in order to identify neural
correlates of feedback-based skill learning. 
</description>
    </item>
    
    <item>
        <title>PercyConfigurator &#8212; Perception Experiments as a Service</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>System for Speech Transcription and Post-Editing in Microsoft Word</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2045.PDF</link>
        <description>In this demonstration paper, we introduce a transcription service that
can be used for transcription of different meetings, sessions etc.
The service performs speaker diarization, automatic speech recognition,
punctuation restoration and produces human-readable transcripts as
special Microsoft Word documents that have audio and word alignments
embedded. Thereby, a widely-used word processor is transformed into
a transcription post-editing tool. Currently, Latvian and Lithuanian
languages are supported, but other languages can be easily added.
</description>
    </item>
    
    <item>
        <title>Emojive! Collecting Emotion Data from Speech and Facial Expression Using Mobile Game App</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2047.PDF</link>
        <description>We developed Emojive!, a mobile game app to make emotion recognition
from audio and image interactive and fun, motivating the users to play
with the app. The game is to act out a specific emotion, among six
emotion labels (happy, sad, anger, anxiety, loneliness, criticism),
given by the system. Double player mode lets two people to compete
their acting skills. The more users play the game, the more emotion-labelled
data will be acquired. We are using deep Convolutional Neural Network
(CNN) models to recognize emotion from audio and facial image in real-time
with a mobile front-end client including intuitive user interface and
simple data visualization.
</description>
    </item>
    
    <item>
        <title>Mylly &#8212; The Mill: A New Platform for Processing Speech and Text Corpora Easily and Efficiently</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Visual Learning 2: Pronunciation App Using Ultrasound, Video, and MRI</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2040.PDF</link>
        <description>We demonstrate  Visual Learning 2, an English pronunciation app for
second-language (L2) learners and phonetics students. This iOS app
links together audio, front and side video, MRI and ultrasound movies
of a native speaker reading a phonetically balanced text. Users can
watch and shadow front and side video overlaid with an ultrasound tongue
movie. They are able to play the video at three speeds and start the
video from any word by tapping on it, with a choice of display in either
English or IPA. Users can record their own audio/video and play it
back in sync with the model for comparison.
</description>
    </item>
    
    <item>
        <title>Dialogue as Collaborative Problem Solving</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/3002.PDF</link>
        <description>I will describe the current status of a long-term effort at developing
dialogue systems that go beyond simple task execution models to systems
that involve collaborative problem solving. Such systems involve open-ended
discussion and the tasks cannot be accomplished without extensive interaction
(e.g., 10 turns or more). The key idea is that dialogue itself arises
from an agent&amp;#8217;s ability for collaborative problem solving (CPS).
In such dialogues, agents may introduce, modify and negotiate goals;
propose and discuss the merits possible paths to solutions; explicitly
discuss progress as the two agents work towards the goals; and evaluate
how well a goal was accomplished. To complicate matters, user utterances
in such settings are much more complex than seen in simple task execution
dialogues and requires full semantic parsing. A key question we have
been exploring in the past few years is how much of dialogue can be
accounted for by domain-independent mechanisms. I will discuss these
issues and draw examples from a dialogue system we have built that,
except for the specialized domain reasoning required in each case,
uses the same architecture to perform three different tasks: collaborative
blocks world planning, when the system and user build structures and
may have differing goals; biocuration, in which a biologist and the
system interact in order to build executable causal models of biological
pathways; and collaborative composition, where the user and system
collaborate to compose simple pieces of music.
</description>
    </item>
    
    <item>
        <title>Elicitation Design for Acoustic Depression Classification: An Investigation of Articulation Effort, Linguistic Complexity, and Word Affect</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1223.PDF</link>
        <description>Assessment of neurological and psychiatric disorders like depression
are unusual from a speech processing perspective, in that speakers
can be prompted or instructed in what they should say (e.g. as part
of a clinical assessment). Despite prior speech-based depression studies
that have used a variety of speech elicitation methods, there has been
little evaluation of the best elicitation mode. One approach to understand
this better is to analyze an existing database from the perspective
of articulation effort, word affect, and linguistic complexity measures
as proxies for depression sub-symptoms (e.g. psychomotor retardation,
negative stimulus suppression, cognitive impairment). Here a novel
measure for quantifying articulation effort is introduced, and when
applied experimentally to the DAIC corpus shows promise for identifying
speech data that are more discriminative of depression. Interestingly,
experiment results demonstrate that by selecting speech with higher
articulation effort, linguistic complexity, or word-based arousal/valence,
improvements in acoustic speech-based feature depression classification
performance can be achieved, serving as a guide for future elicitation
design.
</description>
    </item>
    
    <item>
        <title>Robustness Over Time-Varying Channels in DNN-HMM ASR Based Human-Robot Interaction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1308.PDF</link>
        <description>This paper addresses the problem of time-varying channels in speech-recognition-based
human-robot interaction using Locally-Normalized Filter-Bank features
(LNFB), and training strategies that compensate for microphone response
and room acoustics. Testing utterances were generated by re-recording
the Aurora-4 testing database using a PR2 mobile robot, equipped with
a Kinect audio interface while performing head rotations and movements
toward and away from a fixed source. Three training conditions were
evaluated called Clean, 1-IR and 33-IR. With Clean training, the DNN-HMM
system was trained using the Aurora-4 clean training database. With
1-IR training, the same training data were convolved with an impulse
response estimated at one meter from the source with no rotation of
the robot head. With 33-IR training, the Aurora-4 training data were
convolved with impulse responses estimated at one, two and three meters
from the source and 11 angular positions of the robot head. The 33-IR
training method produced reductions in WER greater than 50% when compared
with Clean training using both LNFB and conventional Mel filterbank
features. Nevertheless, LNFB features provided a WER 23% lower than
MelFB using 33-IR training. The use of 33-IR training and LNFB features
reduced WER by 64% compared to Clean training and MelFB features.
</description>
    </item>
    
    <item>
        <title>Analysis of Engagement and User Experience with a Laughter Responsive Social Robot</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1395.PDF</link>
        <description>We explore the effect of laughter perception and response in terms
of engagement in human-robot interaction. We designed two distinct
experiments in which the robot has two modes: laughter responsive and
laughter non-responsive. In responsive mode, the robot detects laughter
using a multimodal real-time laughter detection module and invokes
laughter as a backchannel to users accordingly. In non-responsive mode,
robot has no utilization of detection, thus provides no feedback. In
the experimental design, we use a straightforward question-answer based
interaction scenario using a back-projected robot head. We evaluate
the interactions with objective and subjective measurements of engagement
and user experience.
</description>
    </item>
    
    <item>
        <title>Automatic Classification of Autistic Child Vocalisations: A Novel Database and Results</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0730.PDF</link>
        <description>Humanoid robots have in recent years shown great promise for supporting
the educational needs of children on the autism spectrum. To further
improve the efficacy of such interactions, user-adaptation strategies
based on the individual needs of a child are required. In this regard,
the proposed study assesses the suitability of a range of speech-based
classification approaches for automatic detection of autism severity
according to the commonly used Social Responsiveness Scale second edition
(SRS-2). Autism is characterised by socialisation limitations including
child language and communication ability. When compared to neurotypical
children of the same age these can be a strong indication of severity.
This study introduces a novel dataset of 803 utterances recorded from
14 autistic children aged between 4&amp;#8211;10 years, during Wizard-of-Oz
interactions with a humanoid robot. Our results demonstrate the suitability
of support vector machines (SVMs) which use acoustic feature sets from
multiple Interspeech COMPARE challenges. We also evaluate deep spectrum
features, extracted via an image classification convolutional neural
network (CNN) from the spectrogram of autistic speech instances. At
best, by using SVMs on the acoustic feature sets, we achieved a UAR
of 73.7% for the proposed 3-class task. 
</description>
    </item>
    
    <item>
        <title>Crowd-Sourced Design of Artificial Attentive Listeners</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Studying the Link Between Inter-Speaker Coordination and Speech Imitation Through Human-Machine Interactions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1431.PDF</link>
        <description>According to accounts of inter-speaker coordination based on internal
predictive models, speakers tend to imitate each other each time they
need to coordinate their behavior. According to accounts based on the
notion of dynamical coupling, imitation should be observed only if
it helps stabilizing the specific coordinative pattern produced by
the interlocutors or if it is a direct consequence of inter-speaker
coordination. To compare these accounts, we implemented an artificial
agent designed to repeat a speech utterance while coordinating its
behavior with that of a human speaker performing the same task. We
asked 10 Italian speakers to repeat the utterance /topkop/ simultaneously
with the agent during short time intervals. In some interactions, the
agent was parameterized to cooperate with the speakers (by producing
its syllables simultaneously with those of the human) while in others
it was parameterized to compete with them (by producing its syllables
in-between those of the human). A positive correlation between the
stability of inter-speaker coordination and the degree of f0 imitation
was observed only in cooperative interactions. However, in line with
accounts based on prediction, speakers imitate the f0 of the agent
regardless of whether this is parameterized to cooperate or to compete
with them.
</description>
    </item>
    
    <item>
        <title>Adjusting the Frame: Biphasic Performative Control of Speech Rhythm</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0396.PDF</link>
        <description>Performative time and pitch scaling is a new research paradigm for
prosodic analysis by synthesis. In this paper, a system for real-time
recorded speech time and pitch scaling by the means of hands or feet
gestures is designed and evaluated. Pitch is controlled with the preferred
hand, using a stylus on a graphic tablet. Time is controlled using
rhythmic frames, or constriction gestures, defined by pairs of control
points. The &amp;#8220;Arsis&amp;#8221; corresponds to the constriction (weak
beat of the syllable) and the &amp;#8220;Thesis&amp;#8221; corresponds to the
vocalic nucleus (strong beat of the syllable). This biphasic control
of rhythmic units is performed by the non-preferred hand using a button.
Pitch and time scales are modified according to these gestural controls
with the help of a real-time pitch synchronous overlap-add technique
(RT-PSOLA). Rhythm and pitch control accuracy are assessed in a prosodic
imitation experiment: the task is to reproduce intonation and rhythm
of various sentences. The results show that inter-vocalic durations
differ on average of only 20 ms. The system appears as a new and effective
tool for performative speech and singing synthesis. Consequences and
applications in speech prosody research are discussed.
</description>
    </item>
    
    <item>
        <title>Attentional Factors in Listeners&amp;#8217; Uptake of Gesture Cues During Speech Processing</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1676.PDF</link>
        <description>In conversation, speakers spontaneously produce manual gestures that
can facilitate listeners&amp;#8217; comprehension of speech. However, various
factors may affect listeners&amp;#8217; ability to use gesture cues. Here
we examine a situation where a speaker is referring to physical objects
in the contextual here-and-now. In this situation, objects for potential
reference will compete with gestures for visual attention. In two experiments,
a speaker provided instructions to pick up objects in the visual environment
(&amp;#8220; Pick up the candy&amp;#8221;). On some trials, the speaker produced
a &amp;#8220;pick up&amp;#8221; gesture that reflected the size/shape of the
target object. Gaze position was recorded to evaluate how listeners
allocated attention to scene elements. Experiment 1 showed that, although
iconic gestures (when present) were rarely fixated directly, peripheral
uptake of these cues speeded listeners&amp;#8217; visual identification
of intended referents as the instruction unfolded. However, the benefit
was mild and occurred primarily for small/hard-to-identify objects.
In Experiment 2, background noise was added to reveal whether challenging
auditory environments lead listeners to allocate additional visual
attention to gesture cues in a compensatory manner. Interestingly,
background noise actually  reduced listeners&amp;#8217; use of gesture
cues. Together the findings highlight how situational factors govern
the use of visual cues during multimodal communication.
</description>
    </item>
    
    <item>
        <title>Motion Analysis in Vocalized Surprise Expressions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0631.PDF</link>
        <description>The background of our research is the generation of natural human-like
motions during speech in android robots that have a highly human-like
appearance. Mismatches in speech and motion are sources of unnaturalness,
especially when emotion expressions are involved. Surprise expressions
often occur in dialogue interactions, and they are often accompanied
by verbal interjectional utterances. In this study, we analyze facial,
head and body motions during several types of vocalized surprise expressions
appearing in human-human dialogue interactions. The analysis results
indicate an inter-dependence between motion types and different types
of surprise expression (such as emotional, social or quoted) as well
as different degrees of surprise expression. The synchronization between
motion and surprise utterances is also analyzed.
</description>
    </item>
    
    <item>
        <title>Enhancing Backchannel Prediction Using Word Embeddings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1606.PDF</link>
        <description>Backchannel responses like &amp;#8220;uh-huh&amp;#8221;, &amp;#8220;yeah&amp;#8221;,
&amp;#8220;right&amp;#8221; are used by the listener in a social dialog as
a way to provide feedback to the speaker. In the context of human-computer
interaction, these responses can be used by an artificial agent to
build rapport in conversations with users. In the past, multiple approaches
have been proposed to detect backchannel cues and to predict the most
natural timing to place those backchannel utterances. Most of these
are based on manually optimized fixed rules, which may fail to generalize.
Many systems rely on the location and duration of pauses and pitch
slopes of specific lengths. In the past, we proposed an approach by
training artificial neural networks on acoustic features such as pitch
and power and also attempted to add word embeddings via word2vec. In
this work, we refined this approach by evaluating different methods
to add timed word embeddings via word2vec. Comparing the performance
using various feature combinations, we could show that adding linguistic
features improves the performance over a prediction system that only
uses acoustic features.
</description>
    </item>
    
    <item>
        <title>A Computational Model for Phonetically Responsive Spoken Dialogue Systems</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1042.PDF</link>
        <description>This paper introduces a model for segment-level phonetic responsiveness.
It is based on behavior observed in human-human interaction, and is
designed to be integrated into spoken dialogue systems to capture potential
phonetic variation and simulate convergence capabilities. Each step
in the process is responsible for an aspect of the interaction, including
monitoring the input speech and appropriately analyzing it. Various
parameters can be tuned to configure the speech handling and adjust
the response style. Evaluation was performed by simulating simple end-to-end
dialogue scenarios, including analyzing the synthesized output of the
model. The results show promising ground for further extensions.
</description>
    </item>
    
    <item>
        <title>Incremental Dialogue Act Recognition: Token- vs Chunk-Based Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0738.PDF</link>
        <description>This paper presents a machine learning based approach to incremental
dialogue act classification with a focus on the recognition of communicative
functions associated with dialogue segments in a multidimensional space,
as defined in the ISO 24617-2 dialogue act annotation standard. The
main goal is to establish the nature of an  increment whose processing
will result in a reliable overall system performance. We explore scenarios
where increments are tokens or syntactically, semantically or prosodically
motivated chunks. Combing local classification with meta-classifiers
at a late fusion decision level we obtained state-of-the-art classification
performance. Experiments were carried out on manually corrected transcriptions
and on potentially erroneous ASR output. Chunk-based classification
yields better results on the manual transcriptions, whereas token-based
classification shows a more robust performance on the ASR output. It
is also demonstrated that layered hierarchical and cascade training
procedures result in better classification performance than the single-layered
approach based on a joint classification predicting complex class labels.
</description>
    </item>
    
    <item>
        <title>Clear Speech &amp;#8212; Mere Speech? How Segmental and Prosodic Speech Reduction Shape the Impression That Speakers Create on Listeners</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0028.PDF</link>
        <description>Research on speech reduction is primarily concerned with analyzing,
modeling, explaining, and, ultimately, predicting phonetic variation.
That is, the focus is on the speech signal itself. The present paper
adds a little side note to this fundamental line of research by addressing
the question whether variation in the degree of reduction also has
a systematic effect on the attributes we ascribe to the speaker who
produces the speech signal. A perception experiment was carried out
for German in which 46 listeners judged whether or not speakers showing
3 different combinations of segmental and prosodic reduction levels
(unreduced, moderately reduced, strongly reduced) are appropriately
described by 13 physical, social, and cognitive attributes. The experiment
shows that clear speech is not mere speech, and less clear speech is
not just reduced either. Rather, results revealed a complex interplay
of reduction levels and perceived speaker attributes in which moderate
reduction can make a better impression on listeners than no reduction.
In addition to its relevance in reduction models and theories, this
interplay is instructive for various fields of speech application from
social robotics to charisma coaching.
</description>
    </item>
    
    <item>
        <title>Relationships Between Speech Timing and Perceived Hostility in a French Corpus of Political Debates</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0293.PDF</link>
        <description>This study investigates the relationship between perceived hostility
and speech timing features within extracts from Montreuil&amp;#8217;s City
Council sessions in 2013, marked by a tense political context at this
time. A dataset of 118 speech extracts from the mayor (Dominique Voynet)
and four of her political opponents during the City Council has been
analyzed through the combination of perception tests and speech timing
phenomena, estimated from classical timing-related measurements and
custom metrics. We also develop a methodological framework for the
phonetic analysis of nonscripted speech: a double perceptive evaluation
of the original dataset (22 participants) allowed us to measure the
difference of hostility perceived (dHost) between the original audio
extracts and their read transcriptions, and the five speakers produced
the same utterances in a controlled reading task to make the direct
comparison with original extracts possible. Correlations between dHost
and speech timing features differences between each original utterance
and its control counterpart show that perceived hostility is mainly
influenced by local deviations to the expected accentuation pattern
in French combined with the insertion of silent pauses. Moreover, a
finer-grained analysis of rhythmic features reveals different strategies
amongst speakers, especially regarding the realization of interpausal
speech rate variation and final syllables lengthening.
</description>
    </item>
    
    <item>
        <title>Towards Speaker Characterization: Identifying and Predicting Dimensions of Person Attribution</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0328.PDF</link>
        <description>A great number of investigations on person characterization rely on
the assessment of the Big-Five personality traits, a prevalent and
widely accepted model with strong psychological foundation. However,
in the context on characterizing unfamiliar individuals from their
voices only, it may be hard for assessors to determine the Big-Five
traits based on their first impression. In this study, a 28-item semantic
differential rating scale has been completed by a total of 33 listeners
who were presented with 15 male voice stimuli. A factor analysis on
their responses enabled us to identify five perceptual factors of person
attribution: (social and physical) attractiveness, confidence, apathy,
serenity, and incompetence. A discussion on the relations of these
dimensions of speaker attribution to the Big-Five factors is provided
and speech features relevant to the automatic prediction of our dimensions
are analyzed, together with SVM regression performance. Although more
data are needed to validate our findings, we believe that our approach
can lead to establish a space of person attributions with dimensions
that can easily be detected from utterances in zero-acquaintance scenarios.
</description>
    </item>
    
    <item>
        <title>Prosodic Analysis of Attention-Drawing Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0623.PDF</link>
        <description>The term &amp;#8220;attention drawing&amp;#8221; refers to the action of sellers
who call out to get the attention of people passing by in front of
their stores or shops to invite them inside to buy or sample products.
Since the speaking styles exhibited in such attention-drawing speech
are clearly different from conversational speech, in this study, we
focused on prosodic analyses of attention-drawing speech and collected
the speech data of multiple people with previous attention-drawing
experience by simulating several situations. We then investigated the
effects of several factors, including background noise, interaction
phases, and shop categories on the prosodic features of attention-drawing
utterances. Analysis results indicate that compared to dialogue interaction
utterances, attention-drawing utterances usually have higher power,
higher mean F0s, smaller F0 ranges, and do not drop at the end of sentences,
regardless of the presence or absence of background noise. Analysis
of sentence-final syllable intonation indicates the presence of lengthened
flat or rising tones in attention-drawing utterances.
</description>
    </item>
    
    <item>
        <title>Perceptual and Acoustic CorreLates of Gender in the Prepubertal Voice</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1055.PDF</link>
        <description>This study investigates the perceptual and acoustic correlates of gender
in the prepubertal voice. 23 German-speaking primary school pupils
(13 female, 10 male) aged 8&amp;#8211;9 years were recorded producing 10
sentences each. Two sentences from each speaker were presented in random
order to a group of listeners who were asked to assign a gender to
each stimulus. Single utterances from each of the three male and three
female speakers whose gender was identified most reliably were played
in a second experiment to two further groups of listeners who judged
each stimulus against seven perceptual attribute pairs. Acoustic analysis
of those parameters corresponding most directly to the perceptual attributes
revealed a number of highly significant correlations, indicating some
aspects of the voice and speech (f0, harmonics-to-noise ratio, tempo)
that children use to construct and adults use to identify gender in
the prepubertal voice.
</description>
    </item>
    
    <item>
        <title>To See or not to See: Interlocutor Visibility and Likeability Influence Convergence in Intonation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1248.PDF</link>
        <description>In this paper we look at convergence and divergence in intonation in
the context of social qualities. Specifically we examine pitch accent
realisations in the GECO corpus of German conversations. Pitch accents
are represented as 6-dimensional vectors where each dimension corresponds
to a characteristic of the accent&amp;#8217;s shape. Convergence/divergence
is then measured by calculating the distance between pitch accent realisations
of conversational partners. A decrease of distance values over time
indicates convergence, an increase divergence. The corpus comprises
dialogue sessions in two modalities: partners either saw each other
during the conversation or not. Linear mixed model analyses show convergence
as well as divergence effects in the realisations of H*L accents. This
convergence/divergence is strongly related to the modality and to how
much speakers like their partners: generally, seeing the partner comes
with divergence, whereas when the dialogue partners cannot see each
other, there is convergence. The effect varies, however, depending
on the extent to which a speaker likes their partner. Less liking entails
a greater change in the realisations over time &amp;#8212; stronger divergence
when partners could see each other, and stronger convergence when they
could not.
</description>
    </item>
    
    <item>
        <title>Acoustic Correlates of Parental Role and Gender Identity in the Speech of Expecting Parents</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>A Semi-Supervised Learning Approach for Acoustic-Prosodic Personality Perception in Under-Resourced Domains</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1732.PDF</link>
        <description>Automatic personality analysis has gained attention in the last years
as a fundamental dimension in human-to-human and human-to-machine interaction.
However, it still suffers from limited number and size of speech corpora
for specific domains, such as the assessment of children&amp;#8217;s personality.
This paper investigates a semi-supervised training approach to tackle
this scenario. We devise an experimental setup with age and language
mismatch and two training sets: a small labeled training set from the
Interspeech 2012 Personality Sub-challenge, containing French adult
speech labeled with personality OCEAN traits, and a large unlabeled
training set of Portuguese children&amp;#8217;s speech. As test set, a
corpus of Portuguese children&amp;#8217;s speech labeled with OCEAN traits
is used. Based on this setting, we investigate a weak supervision approach
that iteratively refines an initial model trained with the labeled
data-set using the unlabeled data-set. We also investigate knowledge-based
features, which leverage expert knowledge in acoustic-prosodic cues
and thus need no extra data. Results show that, despite the large mismatch
imposed by language and age differences, it is possible to attain improvements
with these techniques, pointing both to the benefits of using a weak
supervision and expert-based acoustic-prosodic features across age
and language.
</description>
    </item>
    
    <item>
        <title>Effects of Talker Dialect, Gender &amp;amp; Race on Accuracy of Bing Speech and YouTube Automatic Captions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1746.PDF</link>
        <description>This project compares the accuracy of two automatic speech recognition
(ASR) systems &amp;#8212; Bing Speech and YouTube&amp;#8217;s automatic captions
&amp;#8212; across gender, race and four dialects of American English.
The dialects included were chosen for their acoustic dissimilarity.
Bing Speech had differences in word error rate (WER) between dialects
and ethnicities, but they were not statistically reliable. YouTube&amp;#8217;s
automatic captions, however, did have statistically different WERs
between dialects and races. The lowest average error rates were for
General American and white talkers, respectively. Neither system had
a reliably different WER between genders, which had been previously
reported for YouTube&amp;#8217;s automatic captions [1]. However, the higher
error rate non-white talkers is worrying, as it may reduce the utility
of these systems for talkers of color.
</description>
    </item>
    
    <item>
        <title>A Comparison of Sequence-to-Sequence Models for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>CTC in the Context of Generalized Full-Sum HMM Training</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1073.PDF</link>
        <description>We formulate a generalized hybrid HMM-NN training procedure using the
full-sum over the hidden state-sequence and identify CTC as a special
case of it. We present an analysis of the alignment behavior of such
a training procedure and explain the strong localization of label output
behavior of full-sum training (also referred to as peaky or spiky behavior).
We show how to avoid that behavior by using a state prior. We discuss
the temporal decoupling between output label position/time-frame, and
the corresponding evidence in the input observations when this is trained
with BLSTM models. We also show a way how to overcome this by jointly
training a FFNN. We implemented the Baum-Welch alignment algorithm
in CUDA to be able to do fast soft realignments on GPU. We have published
this code along with some of our experiments as part of RETURNN, RWTH&amp;#8217;s
extensible training framework for universal recurrent neural networks.
We finish with experimental validation of our study on WSJ and Switchboard.
</description>
    </item>
    
    <item>
        <title>Advances in Joint CTC-Attention Based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1296.PDF</link>
        <description>We present a state-of-the-art end-to-end Automatic Speech Recognition
(ASR) model. We learn to listen and write characters with a joint Connectionist
Temporal Classification (CTC) and attention-based encoder-decoder network.
The encoder is a deep Convolutional Neural Network (CNN) based on the
VGG network. The CTC network sits on top of the encoder and is jointly
trained with the attention-based decoder. During the beam search process,
we combine the CTC predictions, the attention-based decoder predictions
and a separately trained LSTM language model. We achieve a 5&amp;#8211;10%
error reduction compared to prior systems on spontaneous Japanese and
Chinese speech, and our end-to-end model beats out traditional hybrid
ASR systems.
</description>
    </item>
    
    <item>
        <title>Multitask Learning with CTC and Segmental CRF for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0071.PDF</link>
        <description>Segmental conditional random fields (SCRFs) and connectionist temporal
classification (CTC) are two sequence labeling methods used for end-to-end
training of speech recognition models. Both models define a transcription
probability by marginalizing decisions about latent segmentation alternatives
to derive a sequence probability: the former uses a globally normalized
joint model of segment labels and durations, and the latter classifies
each frame as either an output symbol or a &amp;#8220;continuation&amp;#8221;
of the previous label. In this paper, we train a recognition model
by optimizing an interpolation between the SCRF and CTC losses, where
the same recurrent neural network (RNN) encoder is used for feature
extraction for both outputs. We find that this multitask objective
improves recognition accuracy when decoding with either the SCRF or
CTC models. Additionally, we show that CTC can also be used to pretrain
the RNN encoder, which improves the convergence rate when learning
the joint model.
</description>
    </item>
    
    <item>
        <title>Direct Acoustics-to-Word Models for English Conversational Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0546.PDF</link>
        <description>Recent work on end-to-end automatic speech recognition (ASR) has shown
that the connectionist temporal classification (CTC) loss can be used
to convert acoustics to phone or character sequences. Such systems
are used with a dictionary and separately-trained Language Model (LM)
to produce word sequences. However, they are not truly end-to-end in
the sense of mapping acoustics directly to words without an intermediate
phone representation. In this paper, we present the first results employing
direct acoustics-to-word CTC models on two well-known public benchmark
tasks: Switchboard and CallHome. These models do not require an LM
or even a decoder at run-time and hence recognize speech with minimal
complexity. However, due to the large number of word output units,
CTC word models require orders of magnitude more data to train reliably
compared to traditional systems. We present some techniques to mitigate
this issue. Our CTC word model achieves a word error rate of 13.0%/18.8%
on the Hub5-2000 Switchboard/CallHome test sets without any LM or decoder
compared with 9.6%/16.0% for phone-based CTC with a 4-gram LM. We also
present rescoring results on CTC word model lattices to quantify the
performance benefits of a LM, and contrast the performance of word
and phone CTC models.
</description>
    </item>
    
    <item>
        <title>Reducing the Computational Complexity of Two-Dimensional LSTMs</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1164.PDF</link>
        <description>Long Short-Term Memory Recurrent Neural Networks (LSTMs) are good at
modeling temporal variations in speech recognition tasks, and have
become an integral component of many state-of-the-art ASR systems.
More recently, LSTMs have been extended to model variations in the
speech signal in two dimensions, namely time and frequency [1, 2].
However, one of the problems with two-dimensional LSTMs, such as Grid-LSTMs,
is that the processing in both time and frequency occurs sequentially,
thus increasing computational complexity. In this work, we look at
minimizing the dependence of the Grid-LSTM with respect to previous
time and frequency points in the sequence, thus reducing computational
complexity. Specifically, we compare reducing computation using a bidirectional
Grid-LSTM (biGrid-LSTM) with non-overlapping frequency sub-band processing,
a PyraMiD-LSTM [3] and a frequency-block Grid-LSTM (fbGrid-LSTM) for
parallel time-frequency processing. We find that the fbGrid-LSTM can
reduce computation costs by a factor of four with no loss in accuracy,
on a 12,500 hour Voice Search task.
</description>
    </item>
    
    <item>
        <title>Functional Principal Component Analysis of Vocal Tract Area Functions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0181.PDF</link>
        <description>This paper shows the application of a functional version of principal
component analysis to build a parametrization of vocal tract area functions
for vowel production. Sets of measured area values for ten vowels are
expressed as smooth functional data and next decomposed into a mean
area function and a basis of orthogonal eigenfunctions. Interpretations
of the first four eigenfunctions are provided in terms of tongue movements
and vocal tract length variations. Also, an alternative set of eigenfunctions
with closer association to specific regions of the vocal tract is obtained
via a varimax rotation. The general intention of the paper is to show
the benefits of a functional approach to analyze vocal tract shapes
and motivate further applications.
</description>
    </item>
    
    <item>
        <title>Analysis of Acoustic-to-Articulatory Speech Inversion Across Different Accents and Languages</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0260.PDF</link>
        <description>The focus of this paper is estimating articulatory movements of the
tongue and lips from acoustic speech data. While there are several
potential applications of such a method in speech therapy and pronunciation
training, performance of such acoustic-to-articulatory inversion systems
is not very high due to limited availability of simultaneous acoustic
and articulatory data, substantial speaker variability, and variable
methods of data collection. This paper therefore evaluates the impact
of speaker, language and accent variability on the performance of an
acoustic-to-articulatory speech inversion system. The articulatory
dataset used in this study consists of 21 Dutch speakers reading Dutch
and English words and sentences, and 22 UK English speakers reading
English words and sentences. We trained several acoustic-to-articulatory
speech inversion systems both based on deep and shallow neural network
architectures in order to estimate electromagnetic articulography (EMA)
sensor positions, as well as vocal tract variables (TVs). Our results
show that with appropriate feature and target normalization, a speaker-independent
speech inversion system trained on data from one language is able to
estimate sensor positions (or TVs) for the same language correlating
at about r = 0.53 with the actual sensor positions (or TVs). Cross-language
results show a reduced performance of r = 0.47.
</description>
    </item>
    
    <item>
        <title>Integrated Mechanical Model of [r]-[l] and [b]-[m]-[w] Producing Consonant Cluster [br]</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0617.PDF</link>
        <description>We have developed two types of mechanical models of the human vocal
tract. The first model was designed for the retroflex approximant [r]
and the alveolar lateral approximant [l]. It consisted of the main
vocal tract and a flapping tongue, where the front half of the tongue
can be rotated against the palate. When the tongue is short and rotated
approximately 90 degrees, the retroflex approximant [r] is produced.
The second model was designed for [b], [m], and [w]. Besides the main
vocal tract, this model contains a movable lower lip for lip closure
and a nasal cavity with a controllable velopharyngeal port. In the
present study, we joined these two mechanical models to form a new
model containing the main vocal tract, the flapping tongue, the movable
lower lip, and the nasal cavity with the controllable velopharyngeal
port. This integrated model now makes it possible to produce consonant
sequences. Therefore, we examined the sequence [br], in particular,
adjusting the timing of the lip and lingual gestures to produce the
best sound. Because the gestures are visually observable from the outside
of this model, the timing of the gestures were examined with the use
of a high-speed video camera.
</description>
    </item>
    
    <item>
        <title>A Speaker Adaptive DNN Training Approach for Speaker-Independent Acoustic Inversion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0804.PDF</link>
        <description>We address the speaker-independent acoustic inversion (AI) problem,
also referred to as acoustic-to-articulatory mapping. The scarce availability
of multi-speaker articulatory data makes it difficult to learn a mapping
which generalizes from a limited number of training speakers and reliably
reconstructs the articulatory movements of unseen speakers. In this
paper, we propose a Multi-task Learning (MTL)-based approach that explicitly
separates the modeling of each training speaker AI peculiarities from
the modeling of AI characteristics that are shared by all speakers.
Our approach stems from the well known Regularized MTL approach and
extends it to feed-forward deep neural networks (DNNs). Given multiple
training speakers, we learn for each an acoustic-to-articulatory mapping
represented by a DNN. Then, through an iterative procedure, we search
for a canonical speaker-independent DNN that is &amp;#8220;similar&amp;#8221;
to all speaker-dependent DNNs. The degree of similarity is controlled
by a regularization parameter. We report experiments on the University
of Wisconsin X-ray Microbeam Database under different training/testing
experimental settings. The results obtained indicate that our MTL-trained
canonical DNN largely outperforms a standardly trained (i.e., single
task learning-based) speaker independent DNN.
</description>
    </item>
    
    <item>
        <title>Acoustic-to-Articulatory Mapping Based on Mixture of Probabilistic Canonical Correlation Analysis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1010.PDF</link>
        <description>In this paper, we propose a novel acoustic-to-articulatory mapping
model based on mixture of probabilistic canonical correlation analysis
(mPCCA). In PCCA, it is assumed that two different kinds of data are
observed as results from different linear transforms of a common latent
variable. It is expected that this variable represents a common factor
which is inherent in the different domains, such as acoustic and articulatory
feature spaces. mPCCA is an expansion of PCCA and it can model a much
more complex structure. In mPCCA, covariance matrices of a joint probabilistic
distribution of acoustic-articulatory data are structuralized reasonably
by using transformation coefficients of the linear transforms. Even
if the number of components in mPCCA increases, the structuralized
covariance matrices can be expected to avoid over-fitting. Training
and mapping processes of the mPCCA-based mapping model are reasonably
derived by using the EM algorithm. Experiments using MOCHA-TIMIT show
that the proposed mapping method has achieved better mapping performance
than the conventional GMM-based mapping.
</description>
    </item>
    
    <item>
        <title>Test-Retest Repeatability of Articulatory Strategies Using Real-Time Magnetic Resonance Imaging</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1488.PDF</link>
        <description>Real-time magnetic resonance imaging (rtMRI) provides information about
the dynamic shaping of the vocal tract during speech production. This
paper introduces and evaluates a method for quantifying articulatory
strategies using rtMRI. The method decomposes the formation and release
of a constriction in the vocal tract into the contributions of individual
articulators such as the jaw, tongue, lips, and velum. The method uses
an anatomically guided factor analysis and dynamical principles from
the framework of Task Dynamics. We evaluated the method within a test-retest
repeatability framework. We imaged healthy volunteers (n = 8, 4 females,
4 males) in two scans on the same day and quantified inter-study agreement
with the intraclass correlation coefficient and mean within-subject
standard deviation. The evaluation established a limit on effect size
and intra-group differences in articulatory strategy which can be studied
using the method.
</description>
    </item>
    
    <item>
        <title>Deep Neural Network Embeddings for Text-Independent Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0620.PDF</link>
        <description>This paper investigates replacing i-vectors for text-independent speaker
verification with embeddings extracted from a feed-forward deep neural
network. Long-term speaker characteristics are captured in the network
by a temporal pooling layer that aggregates over the input speech.
This enables the network to be trained to discriminate between speakers
from variable-length speech segments. After training, utterances are
mapped directly to fixed-dimensional speaker embeddings and pairs of
embeddings are scored using a PLDA-based backend. We compare performance
with a traditional i-vector baseline on NIST SRE 2010 and 2016. We
find that the embeddings outperform i-vectors for short speech segments
and are competitive on long duration test conditions. Moreover, the
two representations are complementary, and their fusion improves on
the baseline at all operating points. Similar systems have recently
shown promising results when trained on very large proprietary datasets,
but to the best of our knowledge, these are the best results reported
for speaker-discriminative neural networks when trained and tested
on publicly available corpora.
</description>
    </item>
    
    <item>
        <title>Tied Variational Autoencoder Backends for i-Vector Speaker Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1018.PDF</link>
        <description>Probabilistic linear discriminant analysis (PLDA) is the de facto standard
for backends in i-vector speaker recognition. If we try to extend the
PLDA paradigm using non-linear models, e.g., deep neural networks,
the posterior distributions of the latent variables and the marginal
likelihood become intractable. In this paper, we propose to approach
this problem using stochastic gradient variational Bayes. We generalize
the PLDA model to let i-vectors depend non-linearly on the latent factors.
We approximate the evidence lower bound (ELBO) by Monte Carlo sampling
using the reparametrization trick. This enables us to optimize of the
ELBO using backpropagation to jointly estimate the parameters that
define the model and the approximate posteriors of the latent factors.
We also present a reformulation of the likelihood ratio, which we call
Q-scoring. Q-scoring makes possible to efficiently score the speaker
verification trials for this model. Experimental results on NIST SRE10
suggest that more data might be required to exploit the potential of
this method.
</description>
    </item>
    
    <item>
        <title>Improved Gender Independent Speaker Recognition Using Convolutional Neural Network Based Bottleneck Features</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1182.PDF</link>
        <description>This paper proposes a novel framework to improve performance of gender
independent i-Vector PLDA based speaker recognition using convolutional
neural network (CNN). Convolutional layers of a CNN offer robustness
to variations in input features including those due to gender. A CNN
is trained for ASR with a linear bottleneck layer. Bottleneck features
extracted using the CNN are then used to train a gender-independent
UBM to obtain frame posteriors for training an i-Vector extractor matrix.
To preserve speaker specific information, a hybrid approach to training
the i-Vector extractor matrix using MFCC features with corresponding
frame posteriors derived from bottleneck features is proposed. On the
NIST SRE10 C5 condition pooled trials, our approach reduces the EER
and minDCF 2010 by +14.62% and +14.42% respectively compared to a standard
mfcc based gender-independent speaker recognition system.
</description>
    </item>
    
    <item>
        <title>Autoencoder Based Domain Adaptation for Speaker Recognition Under Insufficient Channel Information</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0049.PDF</link>
        <description>In real-life conditions, mismatch between development and test domain
degrades speaker recognition performance. To solve the issue, many
researchers explored domain adaptation approaches using matched in-domain
dataset. However, adaptation would be not effective if the dataset
is insufficient to estimate channel variability of the domain. In this
paper, we explore the problem of performance degradation under such
a situation of insufficient channel information. In order to exploit
limited in-domain dataset effectively, we propose an unsupervised domain
adaptation approach using Autoencoder based Domain Adaptation (AEDA).
The proposed approach combines an autoencoder with a denoising autoencoder
to adapt resource-rich development dataset to test domain. The proposed
technique is evaluated on the Domain Adaptation Challenge 13 experimental
protocols that is widely used in speaker recognition for domain mismatched
condition. The results show significant improvements over baselines
and results from other prior studies.
</description>
    </item>
    
    <item>
        <title>Nonparametrically Trained Probabilistic Linear Discriminant Analysis for i-Vector Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0829.PDF</link>
        <description>In this paper we propose to estimate the parameters of the probabilistic
linear discriminant analysis (PLDA) in text-independent i-vector speaker
verification framework using a nonparametric form rather than maximum
likelihood estimation (MLE) obtained by an EM algorithm. In this approach
the between-speaker covariance matrix that represents global information
about the speaker variability is replaced with a local estimation computed
on a nearest neighbor basis for each target speaker. The nonparametric
between- and within-speaker scatter matrices can better exploit the
discriminant information in training data and is more adapted to sample
distribution especially when it does not satisfy Gaussian assumption
as in i-vectors without length-normalization. We evaluated this approach
on the recent NIST 2016 speaker recognition evaluation (SRE) as well
as NIST 2010 core condition and found significant performance improvement
compared with a generatively trained PLDA model.
</description>
    </item>
    
    <item>
        <title>DNN Bottleneck Features for Speaker Clustering</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0144.PDF</link>
        <description>In this work, we explore deep neural network bottleneck features (BNF)
in the context of speaker clustering. A straightforward manner to deal
with speaker clustering is to reuse the bottleneck features extracted
from speaker recognition. However, the selection of a bottleneck architecture
or nonlinearity impacts the performance of both systems. In this work,
we analyze the bottleneck features obtained for speaker recognition
and test them in a speaker clustering scenario. We observe that there
are deep neural network topologies that work better for both cases,
even when their classification criteria (senone classification) is
loosely met. We present results that outperform a traditional MFCC
system by 21% for speaker recognition and between 20% and 37% in clustering
using the same topology.
</description>
    </item>
    
    <item>
        <title>Creak as a Feature of Lexical Stress in Estonian</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Cross-Speaker Variation in Voice Source Correlates of Focus and Deaccentuation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1535.PDF</link>
        <description>This paper describes cross-speaker variation in the voice source correlates
of focal accentuation and deaccentuation. A set of utterances with
varied narrow focus placement as well as broad focus and deaccented
renditions were produced by six speakers of English. These were manually
inverse filtered and parameterized on a pulse-by-pulse basis using
the LF source model. Z-normalized F0, EE, OQ and RD parameters (selected
through correlation and factor analysis) were used to generate speaker
specific baseline voice profiles and to explore cross-speaker variation
in focal and non-focal (post- and prefocal) syllables. As expected,
source parameter values were found to differ in the focal and postfocal
portions of the utterance. For four of the six speakers the measures
revealed a trend of tenser phonation on the focal syllable (an increase
in EE and F0 and typically, a decrease in OQ and RD) as well as increased
laxness in the postfocal part of the utterance. For two of the speakers,
however, the measurements showed a different trend. These speakers
had very high F0 and often high EE on the focal accent. In these cases,
RD and OQ values tended to be raised rather than lowered. The possible
reasons for these differences are discussed.
</description>
    </item>
    
    <item>
        <title>Acoustic Characterization of Word-Final Glottal Stops in Mizo and Assam Sora</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0604.PDF</link>
        <description>The present work proposed an approach to characterize the word-final
glottal stops in Mizo and Assam Sora language. Generally, glottal stops
have more strong glottal and ventricular constriction at the coda position
than at the onset. However, the primary source characteristics of glottal
stops are irregular glottal cycles, abrupt glottal closing, and reduced
open cycle. These changes will not only affect the vocal quality parameters
but may also significantly affect the vocal tract characteristics due
to changes in the subglottal coupling behavior. This motivates to analyze
the dynamic vocal tract characteristics in terms of source behavior,
apart from the excitation source features computed from the Linear
Prediction (LP) residual for the acoustic characterization of the word-final
glottal stops. The dominant resonance frequency (DRF) of the vocal
tract using Hilbert Envelope of Numerator Group Delay (HNGD) are extracted
at every sample instants as a cue to study this deviation. The gradual
increase in the DRF and significantly lower duration for which subglottal
coupling is occurring is observed for the glottal stop region for both
the languages.
</description>
    </item>
    
    <item>
        <title>Iterative Optimal Preemphasis for Improved Glottal-Flow Estimation by Iterative Adaptive Inverse Filtering</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Automatic Measurement of Pre-Aspiration</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0870.PDF</link>
        <description>Pre-aspiration is defined as the period of glottal friction occurring
in sequences of vocalic/consonantal sonorants and phonetically voiceless
obstruents. We propose two machine learning methods for automatic measurement
of pre-aspiration duration: a feedforward neural network, which works
at the frame level; and a structured prediction model, which relies
on manually designed feature functions, and works at the segment level.
The input for both algorithms is a speech signal of an arbitrary length
containing a single obstruent, and the output is a pair of times which
constitutes the pre-aspiration boundaries. We train both models on
a set of manually annotated examples. Results suggest that the structured
model is superior to the frame-based model as it yields higher accuracy
in predicting the boundaries and generalizes to new speakers and new
languages. Finally, we demonstrate the applicability of our structured
prediction algorithm by replicating linguistic analysis of pre-aspiration
in Aberystwyth English with high correlation.
</description>
    </item>
    
    <item>
        <title>Acoustic and Electroglottographic Study of Breathy and Modal Vowels as Produced by Heritage and Native Gujarati Speakers</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>An RNN-Based Quantized F0 Model with Multi-Tier Feedback Links for Text-to-Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0246.PDF</link>
        <description>A recurrent-neural-network-based F0 model for text-to-speech (TTS)
synthesis that generates F0 contours given textual features is proposed.
In contrast to related F0 models, the proposed one is designed to learn
the temporal correlation of F0 contours at multiple levels. The frame-level
correlation is covered by feeding back the F0 output of the previous
frame as the additional input of the current frame; meanwhile, the
correlation over long-time spans is similarly modeled but by using
F0 features aggregated over the phoneme and syllable. Another difference
is that the output of the proposed model is not the interpolated continuous-valued
F0 contour but rather a sequence of discrete symbols, including quantized
F0 levels and a symbol for the unvoiced condition. By using the discrete
F0 symbols, the proposed model avoids the influence of artificially
interpolated F0 curves. Experiments demonstrated that the proposed
F0 model, which was trained using a dropout strategy, generated smooth
F0 contours with relatively better perceived quality than those from
baseline RNN models.
</description>
    </item>
    
    <item>
        <title>Phrase Break Prediction for Long-Form Reading TTS: Exploiting Text Structure Information</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Physically Constrained Statistical F<SUB>0</SUB> Prediction for Electrolaryngeal Speech Enhancement</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>DNN-SPACE: DNN-HMM-Based Generative Model of Voice F&lt;SUB&gt;0&lt;/SUB&gt; Contours for Statistical Phrase/Accent Command Estimation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0719.PDF</link>
        <description>This paper proposes a method to extract prosodic features from a speech
signal by leveraging auxiliary linguistic information. A prosodic feature
extractor called the statistical phrase/accent command estimation (SPACE)
has recently been proposed. This extractor is based on a statistical
model formulated as a stochastic counterpart of the Fujisaki model,
a well-founded mathematical model representing the control mechanism
of vocal fold vibration. The key idea of this approach is that a phrase/accent
command pair sequence is modeled as an output sequence of a path-restricted
hidden Markov model (HMM) so that estimating the state transition amounts
to estimating the phrase/accent commands. Since the phrase and accent
commands are related to linguistic information, we may expect to improve
the command estimation accuracy by using them as auxiliary information
for the inference. To model the relationship between the phrase/accent
commands and linguistic information, we construct a deep neural network
(DNN) that maps the linguistic feature vectors to the state posterior
probabilities of the HMM. Thus, given a pitch contour and linguistic
information, we can estimate phrase/accent commands via state decoding.
We call this method &amp;#8220;DNN-SPACE.&amp;#8221; Experimental results revealed
that using linguistic information was effective in improving the command
estimation accuracy.
</description>
    </item>
    
    <item>
        <title>Controlling Prominence Realisation in Parametric DNN-Based Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1355.PDF</link>
        <description>This work aims to improve text-to-speech synthesis for Wikipedia by
advancing and implementing models of prosodic prominence. We propose
a new system architecture with explicit prominence modeling and test
the first component of the architecture. We automatically extract a
phonetic feature related to prominence from the speech signal in the
ARCTIC corpus. We then modify the label files and train an experimental
TTS system based on the feature using Merlin, a statistical-parametric
DNN-based engine. Test sentences with contrastive prominence on the
word-level are synthesised and separate listening tests a) evaluating
the level of prominence control in generated speech, and b) naturalness,
are conducted. Our results show that the prominence feature-enhanced
system successfully places prominence on the appropriate words and
increases perceived naturalness relative to the baseline.
</description>
    </item>
    
    <item>
        <title>Increasing Recall of Lengthening Detection via Semi-Automatic Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1528.PDF</link>
        <description>Lengthening is the ideal hesitation strategy for synthetic speech and
dialogue systems: it is unobtrusive and hard to notice, because it
occurs frequently in everyday speech before phrase boundaries, in accentuation,
and in hesitation. Despite its elusiveness, it allows valuable extra
time for computing or information highlighting in incremental spoken
dialogue systems. The elusiveness of the matter, however, poses a challenge
for extracting lengthening instances from corpus data: we suspect a
recall problem, as human annotators might not be able to consistently
label lengthening instances. We address this issue by filtering corpus
data for instances of lengthening, using a simple classification method,
based on a threshold for normalized phone duration. The output is then
manually labeled for disfluency. This is compared to an existing, fully
manual disfluency annotation, showing that recall is significantly
higher with semi-automatic pre-classification. This shows that it is
inevitable to use semi-automatic pre-selection to gather enough candidate
data points for manual annotation and subsequent lengthening analyses.
Also, it is desirable to further increase the performance of the automatic
classification. We evaluate in detail human versus semi-automatic annotation
and train another classifier on the resulting dataset to check the
integrity of the disfluent &amp;#8211; non-disfluent distinction.
</description>
    </item>
    
    <item>
        <title>Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Interaction and Transition Model for Speech Emotion Recognition in Dialogue</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0713.PDF</link>
        <description>In this paper we propose a novel emotion recognition method modeling
interaction and transition in dialogue. Conventional emotion recognition
utilizes intra-features such as MFCCs or F0s within individual utterance.
However, human perceive emotions not only through individual utterances
but also by contextual information. The proposed method takes in account
the contextual effect of utterance in dialogue, which the conventional
method fails to. Proposed method introduces Emotion Interaction and
Transition (EIT) models which is constructed by end-to-end LSTMs. The
inputs of EIT model are the previous emotions of both target and opponent
speaker, estimated by state-of-the-art utterance emotion recognition
model. The experimental results show that the proposed method improves
overall accuracy and average precision by a relative error reduction
of 18.8% and 22.6% respectively.
</description>
    </item>
    
    <item>
        <title>Progressive Neural Networks for Transfer Learning in Emotion Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1637.PDF</link>
        <description>Many paralinguistic tasks are closely related and thus representations
learned in one domain can be leveraged for another. In this paper,
we investigate how knowledge can be transferred between three paralinguistic
tasks: speaker, emotion, and gender recognition. Further, we extend
this problem to cross-dataset tasks, asking how knowledge captured
in one emotion dataset can be transferred to another. We focus on progressive
neural networks and compare these networks to the conventional deep
learning method of pre-training and fine-tuning. Progressive neural
networks provide a way to transfer knowledge and avoid the forgetting
effect present when pre-training neural networks on different tasks.
Our experiments demonstrate that: (1) emotion recognition can benefit
from using representations originally learned for different paralinguistic
tasks and (2) transfer learning can effectively leverage additional
datasets to improve the performance of emotion recognition systems.
</description>
    </item>
    
    <item>
        <title>Jointly Predicting Arousal, Valence and Dominance with Multi-Task Learning</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1494.PDF</link>
        <description>An appealing representation of emotions is the use of emotional attributes
such as arousal (passive versus active), valence (negative versus positive)
and dominance (weak versus strong). While previous studies have considered
these dimensions as orthogonal descriptors to represent emotions, there
are strong theoretical and practical evidences showing the interrelation
between these emotional attributes. This observation suggests that
predicting emotional attributes with a unified framework should outperform
machine learning algorithms that separately predict each attribute.
This study presents methods to jointly learn emotional attributes by
exploiting their interdependencies. The framework relies on  multi-task
learning (MTL) implemented with  deep neural networks (DNN) with shared
hidden layers. The framework provides a principled approach to learn
shared feature representations that maximize the performance of regression
models. The results of within-corpus and cross-corpora evaluation show
the benefits of MTL over  single task learning (STL). MTL achieves
gains on  concordance correlation coefficient (CCC) as high as 4.7%
for within-corpus evaluations, and 14.0% for cross-corpora evaluations.
The visualization of the activations of the last hidden layers illustrates
that MTL creates better feature representation. The best structure
has shared layers followed by attribute-dependent layers, capturing
better the relation between attributes.
</description>
    </item>
    
    <item>
        <title>Discretized Continuous Speech Emotion Recognition with Multi-Task Deep Recurrent Neural Network</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0094.PDF</link>
        <description>Estimating continuous emotional states from speech as a function of
time has traditionally been framed as a regression problem. In this
paper, we present a novel approach that moves the problem into the
classification domain by discretizing the training labels at different
resolutions. We employ a multi-task deep bidirectional long-short term
memory (BLSTM) recurrent neural network (RNN) trained with cost-sensitive
Cross Entropy loss to model these labels jointly. We introduce an emotion
decoding algorithm that incorporates long- and short-term temporal
properties of the signal to produce more robust time series estimates.
We show that our proposed approach achieves competitive audio-only
performance on the RECOLA dataset, relative to previously published
works as well as other strong regression baselines. This work provides
a link between regression and classification, and contributes an alternative
approach for continuous emotion recognition.
</description>
    </item>
    
    <item>
        <title>Towards Speech Emotion Recognition &amp;#8220;in the Wild&amp;#8221; Using Aggregated Corpora and Deep Multi-Task Learning</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0736.PDF</link>
        <description>One of the challenges in Speech Emotion Recognition (SER) &amp;#8220;in
the wild&amp;#8221; is the large mismatch between training and test data
(e.g. speakers and tasks). In order to improve the generalisation capabilities
of the emotion models, we propose to use Multi-Task Learning (MTL)
and use gender and naturalness as auxiliary tasks in deep neural networks.
This method was evaluated in within-corpus and various cross-corpus
classification experiments that simulate conditions &amp;#8220;in the wild&amp;#8221;.
In comparison to Single-Task Learning (STL) based state of the art
methods, we found that our MTL method proposed improved performance
significantly. Particularly, models using both gender and naturalness
achieved more gains than those using either gender or naturalness separately.
This benefit was also found in the high-level representations of the
feature space, obtained from our method proposed, where discriminative
emotional clusters could be observed.
</description>
    </item>
    
    <item>
        <title>Speaker-Dependent WaveNet Vocoder</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0314.PDF</link>
        <description>In this study, we propose a speaker-dependent WaveNet vocoder, a method
of synthesizing speech waveforms with WaveNet, by utilizing acoustic
features from existing vocoder as auxiliary features of WaveNet. It
is expected that WaveNet can learn a sample-by-sample correspondence
between speech waveform and acoustic features. The advantage of the
proposed method is that it does not require (1) explicit modeling of
excitation signals and (2) various assumptions, which are based on
prior knowledge specific to speech. We conducted both subjective and
objective evaluation experiments on CMU-ARCTIC database. From the results
of the objective evaluation, it was demonstrated that the proposed
method could generate high-quality speech with phase information recovered,
which was lost by a mel-cepstrum vocoder. From the results of the subjective
evaluation, it was demonstrated that the sound quality of the proposed
method was significantly improved from mel-cepstrum vocoder, and the
proposed method could capture source excitation information more accurately.
</description>
    </item>
    
    <item>
        <title>Waveform Modeling Using Stacked Dilated Convolutional Neural Networks for Speech Bandwidth Extension</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0336.PDF</link>
        <description>This paper presents a waveform modeling and generation method for speech
bandwidth extension (BWE) using stacked dilated convolutional neural
networks (CNNs) with causal or non-causal convolutional layers. Such
dilated CNNs describe the predictive distribution for each wideband
or high-frequency speech sample conditioned on the input narrowband
speech samples. Distinguished from conventional frame-based BWE approaches,
the proposed methods can model the speech waveforms directly and therefore
avert the spectral conversion and phase estimation problems. Experimental
results prove that the BWE methods proposed in this paper can achieve
better performance than the state-of-the-art frame-based approach utilizing
recurrent neural networks (RNNs) incorporating long short-term memory
(LSTM) cells in subjective preference tests.
</description>
    </item>
    
    <item>
        <title>Direct Modeling of Frequency Spectra and Waveform Generation Based on Phase Recovery for DNN-Based Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0488.PDF</link>
        <description>In statistical parametric speech synthesis (SPSS) systems using the
high-quality vocoder, acoustic features such as mel-cepstrum coefficients
and F0 are predicted from linguistic features in order to utilize the
vocoder to generate speech waveforms. However, the generated speech
waveform generally suffers from quality deterioration such as buzziness
caused by utilizing the vocoder. Although several attempts such as
improving an excitation model have been investigated to alleviate the
problem, it is difficult to completely avoid it if the SPSS system
is based on the vocoder. To overcome this problem, there have recently
been attempts to directly model waveform samples. Superior performance
has been demonstrated, but computation time and latency are still issues.
With the aim to construct another type of DNN-based speech synthesizer
with neither the vocoder nor computational explosion, we investigated
direct modeling of frequency spectra and waveform generation based
on phase recovery. In this framework, STFT spectral amplitudes that
include harmonic information derived from F0 are directly predicted
through a DNN-based acoustic model and we use Griffin and Lim&amp;#8217;s
approach to recover phase and generate waveforms. The experimental
results showed that the proposed system synthesized speech without
buzziness and outperformed one generated from a conventional system
using the vocoder.
</description>
    </item>
    
    <item>
        <title>A Hierarchical Encoder-Decoder Model for Statistical Parametric Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Statistical Voice Conversion with WaveNet-Based Waveform Generation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Google&#8217;s Next-Generation Real-Time Unit-Selection Synthesizer Using Sequence-to-Sequence LSTM-Based Autoencoders</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>A Comparison of Sentence-Level Speech Intelligibility Metrics</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0567.PDF</link>
        <description>We examine existing and novel automatically-derived acoustic metrics
that are predictive of speech intelligibility. We hypothesize that
the degree of variability in feature space is correlated with the extent
of a speaker&amp;#8217;s phonemic inventory, their degree of articulatory
displacements, and thus with their degree of perceived speech intelligibility.
We begin by using fully-automatic F1/F2 formant frequency trajectories
for both vowel space area calculation and as input to a proposed class-separability
metric. We then switch to representing vowels by means of short-term
spectral features, and measure vowel separability in that space. Finally,
we consider the case where phonetic labeling is unavailable; here we
calculate short-term spectral features for the entire speech utterance
and then estimate their entropy based on the length of a minimum spanning
tree. In an alternative approach, we propose to first segment the speech
signal using a hidden Markov model, and then calculate spectral feature
separability based on the automatically-derived classes. We apply all
approaches to a database with healthy controls as well as speakers
with mild dysarthria, and report the resulting coefficients of determination.
</description>
    </item>
    
    <item>
        <title>An Auditory Model of Speaker Size Perception for Voiced Speech Sounds</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0196.PDF</link>
        <description>An auditory model was developed to explain the results of behavioral
experiments on perception of speaker size with voiced speech sounds.
It is based on the dynamic, compressive gammachirp (dcGC) filterbank
and a weighting function (SSI weight) derived from a theory of size-shape
segregation in the auditory system. Voiced words with and without high-frequency
emphasis (+6 dB/octave) were produced using a speech vocoder (STRAIGHT).
The SSI weighting function reduces the effect of glottal pulse excitation
in voiced speech, which, in turn, makes it possible for the model to
explain the individual subject variability in the data.
</description>
    </item>
    
    <item>
        <title>The Recognition of Compounds: A Computational Account</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Humans do not Maximize the Probability of Correct Decision When Recognizing DANTALE Words in Noise</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1158.PDF</link>
        <description>Inspired by the DANTALE II listening test paradigm, which is used for
determining the intelligibility of noisy speech, we assess the hypothesis
that humans maximize the probability of correct decision when recognizing
words contaminated by additive Gaussian, speech-shaped noise. We first
propose a statistical Gaussian communication and classification scenario,
where word models are built from short term spectra of human speech,
and optimal classifiers in the sense of maximum a posteriori estimation
are derived. Then, we perform a listening test, where the participants
are instructed to make their best guess of words contaminated with
speech-shaped Gaussian noise. Comparing the human&amp;#8217;s performance
to that of the optimal classifier reveals that at high SNR, humans
perform comparable to the optimal classifier. However, at low SNR,
the human performance is inferior to that of the optimal classifier.
This shows that, at least in this specialized task, humans are generally
not able to maximize the probability of correct decision, when recognizing
words.
</description>
    </item>
    
    <item>
        <title>Single-Ended Prediction of Listening Effort Based on Automatic Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1360.PDF</link>
        <description>A new, single-ended, i.e. reference-free measure for the prediction
of perceived listening effort of noisy speech is presented. It is based
on phoneme posterior probabilities (or posteriorgrams) obtained from
a deep neural network of an automatic speech recognition system. Additive
noisy or other distortions of speech tend to smear the posteriorgrams.
The smearing is quantified by a performance measure, which is used
as a predictor for the perceived listening effort required to understand
the noisy speech. The proposed measure was evaluated using a database
obtained from the subjective evaluation of noise reduction algorithms
of commercial hearing aids. Listening effort ratings of processed noisy
speech samples were gathered from 20 hearing-impaired subjects. Averaged
subjective ratings were compared with corresponding predictions computed
by the proposed new method, the ITU-T standard P.563 for single-ended
speech quality assessment, the American National Standard ANIQUE+ for
single-ended speech quality assessment, and a single-ended SNR estimator.
The proposed method achieved a good correlation with mean subjective
ratings and clearly outperformed the standard speech quality measures
and the SNR estimator.
</description>
    </item>
    
    <item>
        <title>Modeling Categorical Perception with the Receptive Fields of Auditory Neurons</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1611.PDF</link>
        <description>This paper demonstrates that a low-level, linear description of the
response properties of auditory neurons can exhibit some of the high-level
properties of the categorical perception of human speech. In particular,
it is shown that the non-linearities observed in the human perception
of speech sounds which span a categorical boundaries can be understood
as arising rather naturally from a low-level statistical description
of phonemic contrasts in the time-frequency plane, understood here
as the receptive field of auditory neurons. The TIMIT database was
used to train a model auditory neuron which discriminates between /s/
and /sh/, and a computer simulation was conducted which demonstrates
that the neuron responds categorically to a linear continuum of synthetic
fricative sounds which span the /s/-/sh/ boundary. The response of
the model provides a good fit to human labeling behavior, and in addition,
is able to account for asymmetries in reaction time across the two
categories.
</description>
    </item>
    
    <item>
        <title>A Maximum Likelihood Approach to Deep Neural Network Based Nonlinear Spectral Mapping for Single-Channel Speech Separation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0830.PDF</link>
        <description>In contrast to the conventional minimum mean squared error (MMSE) training
criterion for nonlinear spectral mapping based on deep neural networks
(DNNs), we propose a probabilistic learning framework to estimate the
DNN parameters for single-channel speech separation. A statistical
analysis of the prediction error vector at the DNN output reveals that
it follows a unimodal density for each log power spectral component.
By characterizing the prediction error vector as a multivariate Gaussian
density with zero mean vector and an unknown covariance matrix, we
present a maximum likelihood (ML) approach to DNN parameter learning.
Our experiments on the Speech Separation Challenge (SSC) corpus show
that the proposed learning approach can achieve a better generalization
capability and a faster convergence than MMSE-based DNN learning. Furthermore,
we demonstrate that the ML-trained DNN consistently outperforms MMSE-trained
DNN in all the objective measures of speech quality and intelligibility
in single-channel speech separation.
</description>
    </item>
    
    <item>
        <title>Deep Clustering-Based Beamforming for Separation with Unknown Number of Sources</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0721.PDF</link>
        <description>This paper extends a deep clustering algorithm for use with time-frequency
masking-based beamforming and perform separation with an unknown number
of sources. Deep clustering is a recently proposed single-channel source
separation algorithm, which projects inputs into the embedding space
and performs clustering in the embedding domain. In deep clustering,
bi-directional long short-term memory (BLSTM) recurrent neural networks
are trained to make embedding vectors orthogonal for different speakers
and concurrent for the same speaker. Then, by clustering the embedding
vectors at test time, we can estimate time-frequency masks for separation.
In this paper, we extend the deep clustering algorithm to a multiple
microphone setup and incorporate deep clustering-based time-frequency
mask estimation into masking-based beamforming, which has been shown
to be more effective than masking for automatic speech recognition.
Moreover, we perform source counting by computing the rank of the covariance
matrix of the embedding vectors. With our proposed approach, we can
perform masking-based beamforming in a multiple-speaker case without
knowing the number of speakers. Experimental results show that our
proposed deep clustering-based beamformer achieves comparable source
separation performance to that obtained with a complex Gaussian mixture
model-based beamformer, which requires the number of sources in advance
for mask estimation.
</description>
    </item>
    
    <item>
        <title>Time-Frequency Masking for Blind Source Separation with Preserved Spatial Cues</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0066.PDF</link>
        <description>In this paper, we address the problem of speech source separation by
relying on time-frequency binary masks to segregate binaural mixtures.
We describe an algorithm which can tackle reverberant mixtures and
can extract the original sources while preserving their original spatial
locations. The performance of the proposed algorithm is evaluated objectively
and subjectively, by assessing the estimated interaural time differences
versus their theoretical values and by testing for localization acuity
in normal-hearing listeners for different spatial locations in a reverberant
room. Experimental results indicate that the proposed algorithm is
capable of preserving the spatial information of the recovered source
signals while keeping the signal-to-distortion and signal-to-interference
ratios high.
</description>
    </item>
    
    <item>
        <title>Variational Recurrent Neural Networks for Speech Separation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0832.PDF</link>
        <description>We present a new stochastic learning machine for speech separation
based on the variational recurrent neural network (VRNN). This VRNN
is constructed from the perspectives of generative stochastic network
and variational auto-encoder. The idea is to faithfully characterize
the randomness of hidden state of a recurrent neural network through
variational learning. The neural parameters under this latent variable
model are estimated by maximizing the variational lower bound of log
marginal likelihood. An inference network driven by the variational
distribution is trained from a set of mixed signals and the associated
source targets. A novel supervised VRNN is developed for speech separation.
The proposed VRNN provides a stochastic point of view which accommodates
the uncertainty in hidden states and facilitates the analysis of model
construction. The masking function is further employed in network outputs
for speech separation. The benefit of using VRNN is demonstrated by
the experiments on monaural speech separation.
</description>
    </item>
    
    <item>
        <title>Detecting Overlapped Speech on Short Timeframes Using Deep Learning</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0188.PDF</link>
        <description>The intent of this work is to demonstrate how deep learning techniques
can be successfully used to detect overlapped speech on independent
short timeframes. A secondary objective is to provide an understanding
on how the duration of the signal frame influences the accuracy of
the method. We trained a deep neural network with heterogeneous layers
and obtained close to 80% inference accuracy on frames going as low
as 25 milliseconds. The proposed system provides higher detection quality
than existing work and can predict overlapped speech with up to 3 simultaneous
speakers. The method exposes low response latency and does not require
a high amount of computing power.
</description>
    </item>
    
    <item>
        <title>Ideal Ratio Mask Estimation Using Deep Neural Networks for Monaural Speech Segregation in Noisy Reverberant Conditions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0549.PDF</link>
        <description>Monaural speech segregation is an important problem in robust speech
processing and has been formulated as a supervised learning problem.
In supervised learning methods, the ideal binary mask (IBM) is usually
used as the target because of its simplicity and large speech intelligibility
gains. Recently, the ideal ratio mask (IRM) has been found to improve
the speech quality over the IBM. However, the IRM was originally defined
in anechoic conditions and did not consider the effect of reverberation.
In this paper, the IRM is extended to reverberant conditions where
the direct sound and early reflections of target speech are regarded
as the desired signal. Deep neural networks (DNNs) is employed to estimate
the extended IRM in the noisy reverberant conditions. The estimated
IRM is then applied to the noisy reverberant mixture for speech segregation.
Experimental results show that the estimated IRM provides substantial
improvements in speech intelligibility and speech quality over the
unprocessed mixture signals under various noisy and reverberant conditions.
</description>
    </item>
    
    <item>
        <title>The Vocative Chant and Beyond: German Calling Melodies Under Routine and Urgent Contexts</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Comparing Languages Using Hierarchical Prosodic Analysis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Intonation Facilitates Prediction of Focus Even in the Presence of Lexical Tones</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0264.PDF</link>
        <description>In English and Dutch, listeners entrain to prosodic contours to predict
where focus will fall in an utterance. However, is this strategy universally
available, even in languages with different phonological systems? In
a phoneme detection experiment, we examined whether prosodic entrainment
is also found in Mandarin Chinese, a tone language, where in principle
the use of pitch for lexical identity may take precedence over the
use of pitch cues to salience. Consistent with the results from Germanic
languages, response times were facilitated when preceding intonation
predicted accent on the target-bearing word. Acoustic analyses revealed
greater F0 range in the preceding intonation of the predicted-accent
sentences. These findings have implications for how universal and language-specific
mechanisms interact in the processing of salience.
</description>
    </item>
    
    <item>
        <title>Mind the Peak: When  Museum is Temporarily Understood as Musical in Australian English</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0839.PDF</link>
        <description>Intonation languages signal pragmatic functions (e.g. information structure)
by means of different pitch accent types. Acoustically, pitch accent
types differ in the alignment of pitch peaks (and valleys) in regard
to stressed syllables, which makes the position of pitch peaks an 
unreliable cue to lexical stress (even though pitch peaks and lexical
stress often coincide in intonation languages). We here investigate
the effect of  pitch accent type on lexical activation in English.
Results of a visual-world eye-tracking study show that Australian English
listeners temporarily activate SWW-words ( musical) if presented with
WSW-words ( museum) with early-peak accents (H+!H*), compared to medial-peak
accents (L+H*). Thus, in addition to signalling pragmatic functions,
the alignment of tonal targets immediately affects lexical activation
in English.
</description>
    </item>
    
    <item>
        <title>Pashto Intonation Patterns</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>A New Model of Final Lowering in Spontaneous Monologue</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0175.PDF</link>
        <description>F0 downtrend observed in spontaneous monologues in the Corpus of Spontaneous
Japanese was analyzed with special attention to the modeling of final
lowering. In addition to the previous finding that the domain of final
lowering covers all tones in the final accentual phrase, it turned
out that the last L tone in the penultimate accentual phrase played
important role in the control of final lowering. It is this tone that
first reached the bottom of the speaker&amp;#8217;s pitch range in the
time course of utterance; it also turned out that the phonetic realization
of this tone is the most stable of all tones in terms of the F0 variability.
Regression model of F0 downtrends is generated by generalized linear
mixed-effect modeling and evaluated by cross-validation. The mean prediction
error of z-normalized F0 values in the best model was 0.25 standard
deviation.
</description>
    </item>
    
    <item>
        <title>Speech Emotion Recognition with Emotion-Pair Based Framework Considering Emotion Distribution Information in Dimensional Emotion Space</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0619.PDF</link>
        <description>In this work, an emotion-pair based framework is proposed for speech
emotion recognition, which constructs more discriminative feature subspaces
for every two different emotions (emotion-pair) to generate more precise
emotion bi-classification results. Furthermore, it is found that in
the dimensional emotion space, the distances between some of the archetypal
emotions are closer than the others. Motivated by this, a Naive Bayes
classifier based decision fusion strategy is proposed, which aims at
capturing such useful emotion distribution information in deciding
the final emotion category for emotion recognition. We evaluated the
classification framework on the USC IEMOCAP database. Experimental
results demonstrate that the proposed method outperforms the hierarchical
binary decision tree approach on both weighted accuracy (WA) and unweighted
accuracy (UA). Moreover, our framework possesses the advantages that
it can be fully automatically generated without empirical guidance
and is easier to be parallelized.
</description>
    </item>
    
    <item>
        <title>Adversarial Auto-Encoders for Speech Based Emotion Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1421.PDF</link>
        <description>Recently, generative adversarial networks and adversarial auto-encoders
have gained a lot of attention in machine learning community due to
their exceptional performance in tasks such as digit classification
and face recognition. They map the auto-encoder&amp;#8217;s bottleneck
layer output (termed as code vectors) to different noise Probability
Distribution Functions (PDFs), that can be further regularized to cluster
based on class information. In addition, they also allow a generation
of synthetic samples by sampling the code vectors from the mapped PDFs.
Inspired by these properties, we investigate the application of adversarial
auto-encoders to the domain of emotion recognition. Specifically, we
conduct experiments on the following two aspects: (i) their ability
to encode high dimensional feature vector representations for emotional
utterances into a compressed space (with a minimal loss of emotion
class discriminability in the compressed space), and (ii) their ability
to regenerate synthetic samples in the original feature space, to be
later used for purposes such as training emotion recognition classifiers.
We demonstrate promise of adversarial auto-encoders with regards to
these aspects on the Interactive Emotional Dyadic Motion Capture (IEMOCAP)
corpus and present our analysis.
</description>
    </item>
    
    <item>
        <title>An Investigation of Emotion Prediction Uncertainty Using Gaussian Mixture Regression</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0512.PDF</link>
        <description>Existing continuous emotion prediction systems implicitly assume that
prediction certainty does not vary with time. However, perception differences
among raters and other possible sources of variability suggest that
prediction certainty varies with time, which warrants deeper consideration.
In this paper, the correlation between the inter-rater variability
and the uncertainty of predicted emotion is firstly studied. A new
paradigm that estimates the uncertainty in prediction is proposed based
on the strong correlation uncovered in the RECOLA database. This is
implemented by including the inter-rater variability as a representation
of the uncertainty information in a probabilistic Gaussian Mixture
Regression (GMR) model. In addition, we investigate the correlation
between the uncertainty and the performance of a typical emotion prediction
system utilizing average rating as the ground truth, by comparing the
prediction performance in the lower and higher uncertainty regions.
As expected, it is observed that the performance in lower uncertainty
regions is better than that in higher uncertainty regions, providing
a path for improving emotion prediction systems.
</description>
    </item>
    
    <item>
        <title>Capturing Long-Term Temporal Dependencies with Convolutional Networks for Continuous Emotion Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0548.PDF</link>
        <description>The goal of continuous emotion recognition is to assign an emotion
value to every frame in a sequence of acoustic features. We show that
incorporating long-term temporal dependencies is critical for continuous
emotion recognition tasks. To this end, we first investigate architectures
that use dilated convolutions. We show that even though such architectures
outperform previously reported systems, the output signals produced
from such architectures undergo erratic changes between consecutive
time steps. This is inconsistent with the slow moving ground-truth
emotion labels that are obtained from human annotators. To deal with
this problem, we model a downsampled version of the input signal and
then generate the output signal through upsampling. Not only does the
resulting downsampling/upsampling network achieve good performance,
it also generates smooth output trajectories. Our method yields the
best known audio-only performance on the RECOLA dataset.
</description>
    </item>
    
    <item>
        <title>Voice-to-Affect Mapping: Inferences on Language Voice Baseline Settings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Attentive Convolutional Neural Network Based Speech Emotion Recognition: A Study on the Impact of Input Features, Signal Length, and Acted Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0917.PDF</link>
        <description>Speech emotion recognition is an important and challenging task in
the realm of human-computer interaction. Prior work proposed a variety
of models and feature sets for training a system. In this work, we
conduct extensive experiments using an attentive convolutional neural
network with multi-view learning objective function. We compare system
performance using different lengths of the input signal, different
types of acoustic features and different types of emotion speech (improvised/scripted).
Our experimental results on the Interactive Emotional Motion Capture
(IEMOCAP) database reveal that the recognition performance strongly
depends on the type of speech data independent of the choice of input
features. Furthermore, we achieved state-of-the-art results on the
improvised speech data of IEMOCAP.
</description>
    </item>
    
    <item>
        <title>Voice Conversion Using Sequence-to-Sequence Learning of Context Posterior Probabilities</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0247.PDF</link>
        <description>Voice conversion (VC) using sequence-to-sequence learning of context
posterior probabilities is proposed. Conventional VC using shared context
posterior probabilities predicts target speech parameters from the
context posterior probabilities estimated from the source speech parameters.
Although conventional VC can be built from non-parallel data, it is
difficult to convert speaker individuality such as phonetic property
and speaking rate contained in the posterior probabilities because
the source posterior probabilities are directly used for predicting
target speech parameters. In this work, we assume that the training
data partly include parallel speech data and propose sequence-to-sequence
learning between the source and target posterior probabilities. The
conversion models perform non-linear and variable-length transformation
from the source probability sequence to the target one. Further, we
propose a joint training algorithm for the modules. In contrast to
conventional VC, which separately trains the speech recognition that
estimates posterior probabilities and the speech synthesis that predicts
target speech parameters, our proposed method jointly trains these
modules along with the proposed probability conversion modules. Experimental
results demonstrate that our approach outperforms the conventional
VC.
</description>
    </item>
    
    <item>
        <title>Learning Latent Representations for Speech Generation and Transformation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0349.PDF</link>
        <description>An ability to model a generative process and learn a latent representation
for speech in an unsupervised fashion will be crucial to process vast
quantities of unlabelled speech data. Recently, deep probabilistic
generative models such as Variational Autoencoders (VAEs) have achieved
tremendous success in modeling natural images. In this paper, we apply
a convolutional VAE to model the generative process of natural speech.
We derive latent space arithmetic operations to disentangle learned
latent representations. We demonstrate the capability of our model
to modify the phonetic content or the speaker identity for speech segments
using the derived operations, without the need for parallel supervisory
data.
</description>
    </item>
    
    <item>
        <title>Parallel-Data-Free Many-to-Many Voice Conversion Based on DNN Integrated with Eigenspace Using a Non-Parallel Speech Corpus</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0961.PDF</link>
        <description>This paper proposes a novel approach to parallel-data-free and many-to-many
voice conversion (VC). As 1-to-1 conversion has less flexibility, researchers
focus on many-to-many conversion, where speaker identity is often represented
using speaker space bases. In this case, utterances of the same sentences
have to be collected from many speakers. This study aims at overcoming
this constraint to realize a parallel-data-free and many-to-many conversion.
This is made possible by integrating deep neural networks (DNNs) with
eigenspace using a non-parallel speech corpus. In our previous study,
many-to-many conversion was implemented using DNN, whose training was
assisted by EVGMM conversion. By realizing the function of EVGMM equivalently
by constructing eigenspace with a non-parallel speech corpus, the desired
conversion is made possible. A key technique here is to estimate covariance
terms without given parallel data between source and target speakers.
Experiments show that objective assessment scores are comparable to
those of the baseline system trained with parallel data.
</description>
    </item>
    
    <item>
        <title>Sequence-to-Sequence Voice Conversion with Similarity Metric Learned Using Generative Adversarial Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0970.PDF</link>
        <description>We propose a training framework for sequence-to-sequence voice conversion
(SVC). A well-known problem regarding a conventional VC framework is
that acoustic-feature sequences generated from a converter tend to
be over-smoothed, resulting in buzzy-sounding speech. This is because
a particular form of similarity metric or distribution for parameter
training of the acoustic model is assumed so that the generated feature
sequence that averagely fits the training target example is considered
optimal. This over-smoothing occurs as long as a manually constructed
similarity metric is used. To overcome this limitation, our proposed
SVC framework uses a similarity metric implicitly derived from a generative
adversarial network, enabling the measurement of the distance in the
high-level abstract space. This would enable the model to mitigate
the over-smoothing problem caused in the low-level data space. Furthermore,
we use convolutional neural networks to model the long-range context-dependencies.
This also enables the similarity metric to have a shift-invariant property;
thus, making the model robust against misalignment errors involved
in the parallel data. We tested our framework on a non-native-to-native
VC task. The experimental results revealed that the use of the proposed
framework had a certain effect in improving naturalness, clarity, and
speaker individuality.
</description>
    </item>
    
    <item>
        <title>A Mouth Opening Effect Based on Pole Modification for Expressive Singing Voice Transformation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Siamese Autoencoders for Speech Style Extraction and Switching Applied to Voice Identification and Conversion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1434.PDF</link>
        <description>We propose an architecture called siamese autoencoders for extracting
and switching pre-determined styles of speech signals while retaining
the content. We apply this architecture to a voice conversion task
in which we define the content to be the linguistic message and the
style to be the speaker&amp;#8217;s voice. We assume two or more data streams
with the same content but unique styles. The architecture is composed
of two or more separate but shared-weight autoencoders that are joined
by loss functions at the hidden layers. A hidden vector is composed
of style and content sub-vectors and the loss functions constrain the
encodings to decompose style and content. We can select an intended
target speaker either by supplying the associated style vector, or
by extracting a new style vector from a new utterance, using a proposed
style extraction algorithm. We focus on in-training speakers but perform
some initial experiments for out-of-training speakers as well. We propose
and study several types of loss functions. The experiment results show
that the proposed many-to-many model is able to convert voices successfully;
however, its performance does not surpass that of the state-of-the-art
one-to-one model&amp;#8217;s.
</description>
    </item>
    
    <item>
        <title>Recurrent Neural Aligner: An Encoder-Decoder Neural Network Model for Sequence to Sequence Mapping</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1705.PDF</link>
        <description>We introduce an encoder-decoder recurrent neural network model called
Recurrent Neural Aligner (RNA) that can be used for sequence to sequence
mapping tasks. Like connectionist temporal classification (CTC) models,
RNA defines a probability distribution over target label sequences
including blank labels corresponding to each time step in input. The
probability of a label sequence is calculated by marginalizing over
all possible blank label positions. Unlike CTC, RNA does not make a
conditional independence assumption for label predictions; it uses
the predicted label at time t-1 as an additional input to the recurrent
model when predicting the label at time t. We apply this model to end-to-end
speech recognition. RNA is capable of streaming recognition since the
decoder does not employ attention mechanism. The model is trained on
transcribed acoustic data to predict graphemes and no external language
and pronunciation models are used for decoding. We employ an approximate
dynamic programming method to optimize negative log likelihood, and
a sampling-based sequence discriminative training technique to fine-tune
the model to minimize expected word error rate. We show that the model
achieves competitive accuracy without using an external language model
nor doing beam search decoding.
</description>
    </item>
    
    <item>
        <title>Highway-LSTM and Recurrent Highway Networks for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0429.PDF</link>
        <description>Recently, very deep networks, with as many as hundreds of layers, have
shown great success in image classification tasks. One key component
that has enabled such deep models is the use of &amp;#8220;skip connections&amp;#8221;,
including either residual or highway connections, to alleviate the
vanishing and exploding gradient problems. While these connections
have been explored for speech, they have mainly been explored for feed-forward
networks. Since recurrent structures, such as LSTMs, have produced
state-of-the-art results on many of our Voice Search tasks, the goal
of this work is to thoroughly investigate different approaches to adding
depth to recurrent structures. Specifically, we experiment with novel
Highway-LSTM models with bottlenecks skip connections and show that
a 10 layer model can outperform a state-of-the-art 5 layer LSTM model
with the same number of parameters by 2% relative WER. In addition,
we experiment with Recurrent Highway layers and find these to be on
par with Highway-LSTM models, when given sufficient depth.
</description>
    </item>
    
    <item>
        <title>Improving Speech Recognition by Revising Gated Recurrent Units</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Stochastic Recurrent Neural Network for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0856.PDF</link>
        <description>This paper presents a new stochastic learning approach to construct
a latent variable model for recurrent neural network (RNN) based speech
recognition. A hybrid generative and discriminative stochastic network
is implemented to build a deep classification model. In the implementation,
we conduct stochastic modeling for hidden states of recurrent neural
network based on the variational auto-encoder. The randomness of hidden
neurons is represented by the Gaussian distribution with mean and variance
parameters driven by neural weights and learned from variational inference.
Importantly, the class labels of input speech frames are incorporated
to regularize this deep model to sample the informative and discriminative
features for reconstruction of classification outputs. We accordingly
propose the stochastic RNN (SRNN) to reflect the probabilistic property
in RNN classification system. A  stochastic error backpropagation algorithm
is implemented. The experiments on speech recognition using TIMIT and
Aurora4 show the merit of the proposed SRNN.
</description>
    </item>
    
    <item>
        <title>Frame and Segment Level Recurrent Neural Networks for Phone Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1064.PDF</link>
        <description>We introduce a simple and efficient frame and segment level RNN model
(FS-RNN) for phone classification. It processes the input at  frame
level and  segment level by bidirectional gated RNNs. This type of
processing is important to exploit the (temporal) information more
effectively compared to  (i) models which solely process the input
at frame level and  (ii) models which process the input on segment
level using features obtained by heuristic aggregation of frame level
features. Furthermore, we incorporated the activations of the last
hidden layer of the FS-RNN as an additional feature type in a neural
higher-order CRF (NHO-CRF). In experiments, we demonstrated excellent
performance on the TIMIT phone classification task, reporting a performance
of 13.8% phone error rate for the FS-RNN model and 11.9% when combined
with the NHO-CRF. In both cases we significantly exceeded the state-of-the-art
performance.
</description>
    </item>
    
    <item>
        <title>Deep Learning-Based Telephony Speech Recognition in the Wild</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1695.PDF</link>
        <description>In this paper, we explore the effectiveness of a variety of Deep Learning-based
acoustic models for conversational telephony speech, specifically TDNN,
bLSTM and CNN-bLSTM models. We evaluated these models on both research
testsets, such as Switchboard and CallHome, as well as recordings from
a real-world call-center application. Our best single system, consisting
of a single CNN-bLSTM acoustic model, obtained a WER of 5.7% on the
Switchboard testset, and in combination with other models a WER of
5.3% was obtained. On the CallHome testset a WER of 10.1% was achieved
with model combination. On the test data collected from real-world
call-centers, even with model adaptation using application specific
data, the WER was significantly higher at 15.0%. We performed an error
analysis on the real-world data and highlight the areas where speech
recognition still has challenges.
</description>
    </item>
    
    <item>
        <title>The I4U Mega Fusion and Collaboration for NIST Speaker Recognition Evaluation 2016</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0203.PDF</link>
        <description>The 2016  speaker recognition evaluation (SRE&amp;#8217;16) is the latest
edition in the series of benchmarking events conducted by the National
Institute of Standards and Technology (NIST). I4U is a joint entry
to SRE&amp;#8217;16 as the result from the collaboration and active exchange
of information among researchers from sixteen  Institutes and  Universities
across  4 continents. The joint submission and several of its 32 sub-systems
were among top-performing systems. A lot of efforts have been devoted
to two major challenges, namely, unlabeled training data and dataset
shift from  Switchboard-Mixer to the new  Call My Net dataset. This
paper summarizes the lessons learned, presents our shared view from
the sixteen research groups on recent advances, major paradigm shift,
and common tool chain used in speaker recognition as we have witnessed
in SRE&amp;#8217;16. More importantly, we look into the intriguing question
of fusing a large ensemble of sub-systems and the potential benefit
of large-scale collaboration.
</description>
    </item>
    
    <item>
        <title>The MIT-LL, JHU and LRDE NIST 2016 Speaker Recognition Evaluation System</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0537.PDF</link>
        <description>In this paper, the NIST 2016 SRE system that resulted from the collaboration
between MIT Lincoln Laboratory and the team at Johns Hopkins University
is presented. The submissions for the 2016 evaluation consisted of
three fixed condition submissions and a single system open condition
submission. The primary submission on the fixed (and core) condition
resulted in an actual DCF of .618. Details of the submissions are discussed
along with some discussion and observations of the 2016 evaluation
campaign.
</description>
    </item>
    
    <item>
        <title>Nuance - Politecnico di Torino&#8217;s 2016 NIST Speaker Recognition Evaluation System</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>UTD-CRSS Systems for 2016 NIST Speaker Recognition Evaluation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0555.PDF</link>
        <description>This study describes systems submitted by the Center for Robust Speech
Systems (CRSS) from the University of Texas at Dallas (UTD) to the
2016 National Institute of Standards and Technology (NIST) Speaker
Recognition Evaluation (SRE).We developed 4 UBM and DNN i-vector based
speaker recognition systems with alternate data sets and feature representations.
Given that the emphasis of the NIST SRE 2016 is on language mismatch
between training and enrollment/test data, so-called domain mismatch,
in our system development we focused on: (i) utilizing unlabeled in-domain
data for centralizing i-vectors to alleviate the domain mismatch; (ii)
selecting the proper data sets and optimizing configurations for training
LDA/PLDA; (iii) introducing a newly proposed dimension reduction technique
which incorporates unlabeled in-domain data before PLDA training; (iv)
unsupervised speaker clustering of unlabeled data and using them alone
or with previous SREs for PLDA training, and finally (v) score calibration
using unlabeled data with &amp;#8220;pseudo&amp;#8221; speaker labels generated
from speaker clustering. NIST evaluations show that our proposed methods
were very successful for the given task.
</description>
    </item>
    
    <item>
        <title>Analysis and Description of ABC Submission to NIST SRE 2016</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1498.PDF</link>
        <description>We present a condensed description and analysis of the joint submission
for NIST SRE 2016, by Agnitio, BUT and CRIM (ABC). We concentrate on
challenges that arose during development and we analyze the results
obtained on the evaluation data and on our development sets. We show
that testing on mismatched, non-English and short duration data introduced
in NIST SRE 2016 is a difficult problem for current state-of-the-art
systems. Testing on this data brought back the issue of score normalization
and it also revealed that the bottleneck features (BN), which are superior
when used for telephone English, are lacking in performance against
the standard acoustic features like Mel Frequency Cepstral Coefficients
(MFCCs). We offer ABC&amp;#8217;s insights, findings and suggestions for
building a robust system suitable for mismatched, non-English and relatively
noisy data such as those in NIST SRE 2016.
</description>
    </item>
    
    <item>
        <title>The 2016 NIST Speaker Recognition Evaluation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0458.PDF</link>
        <description>In 2016, the National Institute of Standards and Technology (NIST)
conducted the most recent in an ongoing series of speaker recognition
evaluations (SRE) to foster research in robust text-independent speaker
recognition, as well as measure performance of current state-of-the-art
systems. Compared to previous NIST SREs, SRE16 introduced several new
aspects including: an entirely online evaluation platform, a  fixed
training data condition, more variability in test segment duration
(uniformly distributed between 10s and 60s), the use of non-English
(Cantonese, Cebuano, Mandarin and Tagalog) conversational telephone
speech (CTS) collected outside North America, and providing labeled
and unlabeled development (a.k.a. validation) sets for system hyperparameter
tuning and adaptation. The introduction of the new non-English CTS
data made SRE16 more challenging due to domain/channel and language
mismatches as compared to previous SREs. A total of 66 research organizations
from industry and academia registered for SRE16, out of which 43 teams
submitted 121 valid system outputs that produced scores. This paper
presents an overview of the evaluation and analysis of system performance
over all primary evaluation conditions. Initial results indicate that
effective use of the development data was essential for the top performing
systems, and that domain/channel, language, and duration mismatch had
an adverse impact on system performance.
</description>
    </item>
    
    <item>
        <title>A New Cosine Series Antialiasing Function and its Application to Aliasing-Free Glottal Source Models for Speech and Singing Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Speaking Style Conversion from Normal to Lombard Speech Using a Glottal Vocoder and Bayesian GMMs</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0400.PDF</link>
        <description>Speaking style conversion is the technology of converting natural speech
signals from one style to another. In this study, we focus on normal-to-Lombard
conversion. This can be used, for example, to enhance the intelligibility
of speech in noisy environments. We propose a parametric approach that
uses a vocoder to extract speech features. These features are mapped
using Bayesian GMMs from utterances spoken in normal style to the corresponding
features of Lombard speech. Finally, the mapped features are converted
to a Lombard speech waveform with the vocoder. Two vocoders were compared
in the proposed normal-to-Lombard conversion: a recently developed
glottal vocoder that decomposes speech into glottal flow excitation
and vocal tract, and the widely used STRAIGHT vocoder. The conversion
quality was evaluated in two subjective listening tests measuring subjective
similarity and naturalness. The similarity test results show that the
system is able to convert normal speech into Lombard speech for the
two vocoders. However, the subjective naturalness of the converted
Lombard speech was clearly better using the glottal vocoder in comparison
to STRAIGHT.
</description>
    </item>
    
    <item>
        <title>Reducing Mismatch in Training of DNN-Based Glottal Excitation Models in a Statistical Parametric Text-to-Speech System</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0848.PDF</link>
        <description>Neural network-based models that generate glottal excitation waveforms
from acoustic features have been found to give improved quality in
statistical parametric speech synthesis. Until now, however, these
models have been trained separately from the acoustic model. This creates
mismatch between training and synthesis, as the synthesized acoustic
features used for the excitation model input differ from the original
inputs, with which the model was trained on. Furthermore, due to the
errors in predicting the vocal tract filter, the original excitation
waveforms do not provide perfect reconstruction of the speech waveform
even if predicted without error. To address these issues and to make
the excitation model more robust against errors in acoustic modeling,
this paper proposes two modifications to the excitation model training
scheme. First, the excitation model is trained in a connected manner,
with inputs generated by the acoustic model. Second, the target glottal
waveforms are re-estimated by performing glottal inverse filtering
with the predicted vocal tract filters. The results show that both
of these modifications improve performance measured in MSE and MFCC
distortion, and slightly improve the subjective quality of the synthetic
speech.
</description>
    </item>
    
    <item>
        <title>Semi Parametric Concatenative TTS with Instant Voice Modification Capabilities</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Modeling Laryngeal Muscle Activation Noise for Low-Order Physiological Based Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1722.PDF</link>
        <description>Physiological-based synthesis using low order lumped-mass models of
phonation have been shown to mimic and predict complex physical phenomena
observed in normal and pathological speech production, and have received
significant attention due to their ability to efficiently perform comprehensive
parametric investigations that are cost prohibitive with more advanced
computational tools. Even though these numerical models have been shown
to be useful research and clinical tools, several physiological aspects
of them remain to be explored. One of the key components that has been
neglected is the natural fluctuation of the laryngeal muscle activity
that affects the configuration of the model parameters. In this study,
a physiologically-based laryngeal muscle activation model that accounts
for random fluctuations is proposed. The method is expected to improve
the ability to model muscle related pathologies, such as muscle tension
dysphonia and Parkinson&amp;#8217;s disease. The mathematical framework
and underlying assumptions are described, and the effects of the added
random muscle activity is tested in a well-known body-cover model of
the vocal folds with acoustic propagation and interaction. Initial
simulations illustrate that the random fluctuations in the muscle activity
impact the resulting kinematics to varying degrees depending on the
laryngeal configuration.
</description>
    </item>
    
    <item>
        <title>Direct Modelling of Magnitude and Phase Spectra for Statistical Parametric Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Similar Prosodic Structure Perceived Differently in German and English</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0544.PDF</link>
        <description>English and German have similar prosody, but their speakers realize
some pitch falls (not rises) in subtly different ways. We here test
for asymmetry in perception. An ABX discrimination task requiring F0
slope or duration judgements on isolated vowels revealed no cross-language
difference in duration or F0 fall discrimination, but discrimination
of rises (realized similarly in each language) was less accurate for
English than for German listeners. This unexpected finding may reflect
greater sensitivity to rising patterns by German listeners, or reduced
sensitivity by English listeners as a result of extensive exposure
to phrase-final rises (&amp;#8220;uptalk&amp;#8221;) in their language.
</description>
    </item>
    
    <item>
        <title>Disambiguate or not? &#8212; The Role of Prosody in Unambiguous and Potentially Ambiguous Anaphora Production in Strictly Mandarin Parallel Structures</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Acoustic Properties of Canonical and Non-Canonical Stress in French, Turkish, Armenian and Brazilian Portuguese</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1514.PDF</link>
        <description>Languages are often categorized as having either predictable (fixed
or quantity-sensitive) or non-predictable stress. Despite their name,
fixed stress languages may have exceptions, so in fact, their stress
does not always appear in the same position. Since predictability has
been shown to affect certain speech phenomena, with additional or redundant
acoustic cues being provided when the linguistic content is less predictable
(e.g., Smooth Signal Redundancy Hypothesis), we investigate whether,
and to what extent, the predictability of stress position affects the
manifestation of stress in different languages. We examine the acoustic
properties of stress in three languages classified as having fixed
stress (Turkish, French, Armenian), with exceptions, and in one language
with non-predictable-stress, Brazilian Portuguese. Specifically, we
compare the manifestation of stress in the canonical stress (typically
&amp;#8220;fixed&amp;#8221;) position with its manifestation in the non-canonical
(exceptional) position, where it would potentially be less predictable.
We also compare these patterns with the manifestation of stress in
Portuguese, in both the &amp;#8220;default&amp;#8221; penultimate and the less
common final position. Our results show that stress is manifested quite
similarly in canonical and non-canonical positions in the &amp;#8220;fixed&amp;#8221;
stress languages and stress is most clearly produced when it is least
predictable.
</description>
    </item>
    
    <item>
        <title>Phonological Complexity, Segment Rate and Speech Tempo Perception</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>On the Duration of Mandarin Tones</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0029.PDF</link>
        <description>The present study compared the duration of Mandarin tones in three
types of speech contexts: isolated monosyllables, formal text-reading
passages, and casual conversations. A total of 156 adult speakers was
recruited. The speech materials included 44 monosyllables recorded
from each of 121 participants, 18 passages read by 2 participants,
and 20 conversations conducted by 33 participants. The duration pattern
of the four lexical tones in the isolated monosyllables was consistent
with the pattern described in previous literature. However, the duration
of the four lexical tones became much shorter and tended to converge
to that of the neutral tone (i.e., tone 0) in the text-reading and
conversational speech. The maximum-likelihood estimator revealed that
the durational cue contributed to tone recognition in the isolated
monosyllables. With a single speaker, the average tone recognition
based on duration alone could reach approximately 65% correct. As the
number of speakers increased (e.g., &amp;#8805; 4), tone recognition performance
dropped to approximately 45% correct. In conversational speech, the
maximum likelihood estimation of tones based on duration cues was only
23% correct. The tone duration provided little useful cue to differentiate
Mandarin tonal identity in everyday situations.
</description>
    </item>
    
    <item>
        <title>The Formant Dynamics of Long Close Vowels in Three Varieties of Swedish</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1134.PDF</link>
        <description>This study compares the acoustic realisation of /i&amp;#720; y&amp;#720; &amp;#649;&amp;#720;
u&amp;#720;/ in three varieties of Swedish: Central Swedish, Estonian Swedish,
and Finland Swedish. Vowel tokens were extracted from isolated words
produced by six elderly female speakers from each variety. Trajectories
of the first three formants were modelled with discrete cosine transform
(DCT) coefficients, enabling the comparison of the formant means as
well as the direction and magnitude of the formant movement. Cross-dialectal
differences were found in all measures and in all vowels. The most
noteworthy feature of the Estonian Swedish long close vowel inventory
is the lack of /y&amp;#720;/. For Finland Swedish it was shown that /i&amp;#720;/
and /y&amp;#720;/ are more close than in Central Swedish. The realisation
of /&amp;#649;&amp;#720;/ varies from front in Central Swedish, to central
in Estonian Swedish, and back in Finland Swedish. On average, the Central
Swedish vowels exhibited a higher degree of formant movement than the
vowels in the other two varieties. In the present study, regional variation
in Swedish vowels was for the first time investigated using DCT coefficients.
The results stress the importance of taking formant dynamics into account
even in the analysis of nominal monophthongs.
</description>
    </item>
    
    <item>
        <title>Bidirectional LSTM-RNN for Improving Automated Assessment of Non-Native Children&amp;#8217;s Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0250.PDF</link>
        <description>Recent advances in ASR and spoken language processing have led to improved
systems for automated assessment for spoken language. However, it is
still challenging for automated scoring systems to achieve high performance
in terms of the agreement with human experts when applied to non-native
children&amp;#8217;s spontaneous speech. The subpar performance is mainly
caused by the relatively low recognition rate on non-native children&amp;#8217;s
speech. In this paper, we investigate different neural network architectures
for improving non-native children&amp;#8217;s speech recognition and the
impact of the features extracted from the corresponding ASR output
on the automated assessment of speaking proficiency. Experimental results
show that bidirectional LSTM-RNN can outperform feed-forward DNN in
ASR, with an overall relative WER reduction of 13.4%. The improved
speech recognition can then boost the language proficiency assessment
performance. Correlations between the rounded automated scores and
expert scores range from 0.66 to 0.70 for the three speaking tasks
studied, similar to the human-human agreement levels for these tasks.
</description>
    </item>
    
    <item>
        <title>Automatic Scoring of Shadowing Speech Based on DNN Posteriors and Their DTW</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0728.PDF</link>
        <description>Shadowing has become a well-known method to improve learners&amp;#8217;
overall proficiency. Our previous studies realized automatic scoring
of shadowing speech using HMM phoneme posteriors, called GOP (Goodness
of Pronunciation) and learners&amp;#8217; TOEIC scores were predicted adequately.
In this study, we enhance our studies from multiple angles: 1) a much
larger amount of shadowing speech is collected, 2) manual scoring of
these utterances is done by two native teachers, 3) DNN posteriors
are introduced instead of HMM ones, 4) language-independent shadowing
assessment based on posteriors-based DTW (Dynamic Time Warping) is
examined. Experiments suggest that, compared to HMM, DNN can improve
teacher-machine correlation largely by 0.37 and DTW based on DNN posteriors
shows as high correlation as 0.74 even when posterior calculation is
done using a different language from the target language of learning.
</description>
    </item>
    
    <item>
        <title>Off-Topic Spoken Response Detection Using Siamese Convolutional Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1174.PDF</link>
        <description>In this study, we developed an off-topic response detection system
to be used in the context of the automated scoring of non-native English
speakers&amp;#8217; spontaneous speech. Based on transcriptions generated
from an ASR system trained on non-native speakers&amp;#8217; speech and
various semantic similarity features, the system classified each test
response as an on-topic or off-topic response. The recent success of
deep neural networks (DNN) in text similarity detection led us to explore
DNN-based document similarity features. Specifically, we used a siamese
adaptation of the convolutional network, due to its efficiency in learning
similarity patterns simultaneously from both responses and questions
used to elicit responses. In addition, a baseline system was developed
using a standard vector space model (VSM) trained on sample responses
for each question. The accuracy of the siamese CNN-based system was
0.97 and there was a 50% relative error reduction compared to the standard
VSM-based system. Furthermore, the accuracy of the siamese CNN-based
system was consistent across different questions.
</description>
    </item>
    
    <item>
        <title>Phonological Feature Based Mispronunciation Detection and Diagnosis Using Multi-Task DNNs and Active Learning</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1350.PDF</link>
        <description>This paper presents a phonological feature based computer aided pronunciation
training system for the learners of a new language (L2). Phonological
features allow analysing the learners&amp;#8217; mispronunciations systematically
and rendering the feedback more effectively. The proposed acoustic
model consists of a multi-task deep neural network, which uses a shared
representation for estimating the phonological features and HMM state
probabilities. Moreover, an active learning based scheme is proposed
to efficiently deal with the cost of annotation, which is done by expert
teachers, by selecting the most informative samples for annotation.
Experimental evaluations are carried out for German and Italian native-speakers
speaking English. For mispronunciation detection, the proposed feature-based
system outperforms conventional GOP measure and classifier based methods,
while providing more detailed diagnosis. Evaluations also demonstrate
the advantage of active learning based sampling over random sampling.
</description>
    </item>
    
    <item>
        <title>Detection of Mispronunciations and Disfluencies in Children Reading Aloud</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1522.PDF</link>
        <description>To automatically evaluate the performance of children reading aloud
or to follow a child&amp;#8217;s reading in reading tutor applications,
different types of reading disfluencies and mispronunciations must
be accounted for. In this work, we aim to detect most of these disfluencies
in sentence and pseudoword reading. Detecting incorrectly pronounced
words, and quantifying the quality of word pronunciations, is arguably
the hardest task. We approach the challenge as a two-step process.
First, a segmentation using task-specific lattices is performed, while
detecting repetitions and false starts and providing candidate segments
for words. Then, candidates are classified as mispronounced or not,
using multiple features derived from likelihood ratios based on phone
decoding and forced alignment, as well as additional meta-information
about the word. Several classifiers were explored (linear fit, neural
networks, support vector machines) and trained after a feature selection
stage to avoid overfitting. Improved results are obtained using feature
combination compared to using only the log likelihood ratio of the
reference word (22% versus 27% miss rate at constant 5% false alarm
rate).
</description>
    </item>
    
    <item>
        <title>Automatic Assessment of Non-Native Prosody by Measuring Distances on Prosodic Label Sequences</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0366.PDF</link>
        <description>The aim of this paper is to investigate how automatic prosodic labeling
systems contribute to the evaluation of non-native pronunciation. In
particular, it examines the efficiency of a group of metrics to evaluate
the prosodic competence of non-native speakers, based on the information
provided by sequences of labels in the analysis of both native and
non-native speech. A group of Sp_ToBI labels were obtained by means
of an automatic labeling system for the speech of native and non-native
speakers who read the same texts. The metrics assessed the differences
in the prosodic labels for both speech samples. The results showed
the efficiency of the metrics to set apart both groups of speakers.
Furthermore, they exhibited how non-native speakers (American and Japanese
speakers) improved their Spanish productions after doing a set of listening
and repeating activities. Finally, this study also shows that the results
provided by the metrics are correlated with the scores given by human
evaluators on the productions of the different speakers.
</description>
    </item>
    
    <item>
        <title>Inferring Stance from Prosody</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0159.PDF</link>
        <description>Speech conveys many things beyond content, including aspects of stance
and attitude that have not been much studied. Considering 14 aspects
of stance as they occur in radio news stories, we investigated the
extent to which they could be inferred from prosody. By using time-spread
prosodic features and by aggregating local estimates, many aspects
of stance were at least somewhat predictable, with results significantly
better than chance for many stance aspects, including, across English,
Mandarin and Turkish, good, typical, local, background, new information,
and relevant to a large group.
</description>
    </item>
    
    <item>
        <title>Exploring Dynamic Measures of Stance in Spoken Interaction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1706.PDF</link>
        <description>Stance-taking, the expression of opinions or attitudes, informs the
process of negotiation, argumentation, and decision-making. While receiving
significant attention in text materials in work on the related areas
of subjectivity and sentiment analysis, the expression of stance in
speech remains less explored. Prior analysis of the acoustics of stance-expression
in conversational speech has identified some significant differences
across dimensions of stance-related behavior. However, that analysis,
as in much prior work, relied on simple functionals of pitch, energy,
and duration, including maxima, minima, means, and ranges. In contrast,
the current work focuses on exploiting measures that capture the dynamics
of the pitch and energy contour. We employ features based on subband
autocorrelation measures of pitch change and variants of the modulation
spectrum. Using a corpus of conversational speech manually annotated
for dimensions of stance-taking, we demonstrate that these measures
of pitch and energy dynamics can help to characterize and distinguish
among stance-related behaviors in speech.
</description>
    </item>
    
    <item>
        <title>Opinion Dynamics Modeling for Movie Review Transcripts Classification with Hidden Conditional Random Fields</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1035.PDF</link>
        <description>In this paper, the main goal is to detect a movie reviewer&amp;#8217;s
opinion using hidden conditional random fields. This model allows us
to capture the dynamics of the reviewer&amp;#8217;s opinion in the transcripts
of long unsegmented audio reviews that are analyzed by our system.
High level linguistic features are computed at the level of inter-pausal
segments. The features include syntactic features, a statistical word
embedding model and subjectivity lexicons. The proposed system is evaluated
on the ICT-MMMO corpus. We obtain a F1-score of 82%, which is better
than logistic regression and recurrent neural network approaches. We
also offer a discussion that sheds some light on the capacity of our
system to adapt the word embedding model learned from general written
texts data to spoken movie reviews and thus model the dynamics of the
opinion.
</description>
    </item>
    
    <item>
        <title>Transfer Learning Between Concepts for Human Behavior Modeling: An Application to Sincerity and Deception Prediction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0121.PDF</link>
        <description>Transfer learning (TL) involves leveraging information from sources
outside the domain at hand for enhancing model performances. Popular
TL methods either directly use the data or adapt the models learned
on out-of-domain resources and incorporate them within in-domain models.
TL methods have shown promise in several applications such as text
classification, cross-domain language classification and emotion recognition.
In this paper, we propose TL methods to computational human behavioral
trait modeling. Many behavioral traits are abstract constructs (e.g.,
sincerity of an individual), and are often conceptually related to
other constructs (e.g., level of deception) making TL methods an attractive
option for their modeling. We consider the problem of automatically
predicting human sincerity and deception from behavioral data while
leveraging transfer of knowledge from each other. We compare our methods
against baseline models trained only on in-domain data. Our best models
achieve an Unweighted Average Recall (UAR) of 72.02% in classifying
deception (baseline: 69.64%). Similarly, applied methods achieve Spearman&amp;#8217;s/Pearson&amp;#8217;s
correlation values of 49.37%/48.52% between true and predicted sincerity
scores (baseline: 46.51%/41.58%), indicating the success and the potential
of TL for such human behavior tasks.
</description>
    </item>
    
    <item>
        <title>The Sound of Deception &amp;#8212; What Makes a Speaker Credible?</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0384.PDF</link>
        <description>The detection of deception in human speech is a difficult task but
can be performed above chance level by human listeners even when only
audio data is provided. Still, it is highly contested, which speech
features could be used to help identify lies. In this study, we examined
a set of phonetic and paralinguistic cues and their influence on the
credibility of speech using an analysis-by-synthesis approach. 33 linguistically
neutral utterances with different manipulated cues (unfilled pauses,
phonation type, higher speech rate, tremolo and raised F0) were synthesized
using articulatory synthesis. These utterances were presented to 50
subjects who were asked to choose the more credible utterance. From
those choices, a credibility score was calculated for each cue. The
results show a significant increase in credibility when a tremolo is
inserted or the breathiness is increased, and a decrease in credibility
when a pause is inserted or the F0 is raised. Other cues also had a
significant, but less pronounced influence on the credibility while
some only showed trends. In summary, the study showed that the credibility
of a factually unverifiable utterance is in parts controlled by the
presented paralinguistic cues.
</description>
    </item>
    
    <item>
        <title>Hybrid Acoustic-Lexical Deep Learning Approach for Deception Detection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1723.PDF</link>
        <description>Automatic deception detection is an important problem with far-reaching
implications for many disciplines. We present a series of experiments
aimed at automatically detecting deception from speech. We use the
Columbia X-Cultural Deception (CXD) Corpus, a large-scale corpus of
within-subject deceptive and non-deceptive speech, for training and
evaluating our models. We compare the use of spectral, acoustic-prosodic,
and lexical feature sets, using different machine learning models.
Finally, we design a single hybrid deep model with both acoustic and
lexical features trained jointly that achieves state-of-the-art results
on the CXD corpus.
</description>
    </item>
    
    <item>
        <title>A Generative Model for Score Normalization in Speaker Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0137.PDF</link>
        <description>We propose a theoretical framework for thinking about score normalization,
which confirms that normalization is not needed under (admittedly fragile)
ideal conditions. If, however, these conditions are not met, e.g. under
data-set shift between training and runtime, our theory reveals dependencies
between scores that could be exploited by strategies such as score
normalization. Indeed, it has been demonstrated over and over experimentally,
that various ad-hoc score normalization recipes do work. We present
a first attempt at using probability theory to design a generative
score-space normalization model which gives similar improvements to
ZT-norm on the text-dependent RSR 2015 database.
</description>
    </item>
    
    <item>
        <title>Content Normalization for Text-Dependent Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1419.PDF</link>
        <description>Subspace based techniques, such as i-vector and Joint Factor Analysis
(JFA) have shown to provide state-of-the-art performance for fixed
phrase based text-dependent speaker verification. However, the error
rates of such systems on the random digit task of RSR dataset are higher
than that of Gaussian Mixture Model-Universal Background Model (GMM-UBM).
In this paper, we aim at improving i-vector system by normalizing the
content of the enrollment data to match the test data. We estimate
i-vectors for each frames of a speech utterance (also called online
i-vectors). The largest similarity scores across frames between enrollment
and test are taken using these online i-vectors to obtain speaker verification
scores. Experiments on Part3 of RSR corpora show that the proposed
approach achieves 12% relative improvement in equal error rate over
a GMM-UBM based baseline system.
</description>
    </item>
    
    <item>
        <title>End-to-End Text-Independent Speaker Verification with Triplet Loss on Short Utterances</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1608.PDF</link>
        <description>Text-independent speaker verification against short utterances is still
challenging despite of recent advances in the field of speaker recognition
with i-vector framework. In general, to get a robust i-vector representation,
a satisfying amount of data is needed in the MAP adaptation step, which
is hard to meet under short duration constraint. To overcome this,
we present an end-to-end system which directly learns a mapping from
speech features to a compact fixed length speaker discriminative embedding
where the Euclidean distance is employed for measuring similarity within
trials. To learn the feature mapping, a modified Inception Net with
residual block is proposed to optimize the triplet loss function. The
input of our end-to-end system is a fixed length spectrogram converted
from an arbitrary length utterance. Experiments show that our system
consistently outperforms a conventional i-vector system on short duration
speaker verification tasks. To test the limit under various duration
conditions, we also demonstrate how our end-to-end system behaves with
different duration from 2s&amp;#8211;4s.
</description>
    </item>
    
    <item>
        <title>Adversarial Network Bottleneck Features for Noise Robust Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0883.PDF</link>
        <description>In this paper, we propose a noise robust bottleneck feature representation
which is generated by an adversarial network (AN). The AN includes
two cascade connected networks, an encoding network (EN) and a discriminative
network (DN). Mel-frequency cepstral coefficients (MFCCs) of clean
and noisy speech are used as input to the EN and the output of the
EN is used as the noise robust feature. The EN and DN are trained in
turn, namely, when training the DN, noise types are selected as the
training labels and when training the EN, all labels are set as the
same, i.e., the clean speech label, which aims to make the AN features
invariant to noise and thus achieve noise robustness. We evaluate the
performance of the proposed feature on a Gaussian Mixture Model-Universal
Background Model based speaker verification system, and make comparison
to MFCC features of speech enhanced by short-time spectral amplitude
minimum mean square error (STSA-MMSE) and deep neural network-based
speech enhancement (DNN-SE) methods. Experimental results on the RSR2015
database show that the proposed AN bottleneck feature (AN-BN) dramatically
outperforms the STSA-MMSE and DNN-SE based MFCCs for different noise
types and signal-to-noise ratios. Furthermore, the AN-BN feature is
able to improve the speaker verification performance under the clean
condition.
</description>
    </item>
    
    <item>
        <title>What Does the Speaker Embedding Encode?</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1125.PDF</link>
        <description>Developing a good speaker embedding has received tremendous interest
in the speech community. Speaker representations such as i-vector,
d-vector have shown their superiority in speaker recognition, speaker
adaptation and other related tasks. However, not much is known about
which properties are exactly encoded in these speaker embeddings. In
this work, we make an in-depth investigation on three kinds of speaker
embeddings, i.e. i-vector, d-vector and RNN/LSTM based sequence-vector
(s-vector). Classification tasks are carefully designed to facilitate
better understanding of these encoded speaker representations. Their
abilities of encoding different properties are revealed and compared,
such as speaker identity, gender, speaking rate, text content and channel
information. Moreover, a new architecture is proposed to integrate
different speaker embeddings, so that the advantages can be combined.
The new advanced speaker embedding (i-s-vector) outperforms the others,
and shows a more than 50% EER reduction compared to the i-vector baseline
on the RSR2015 content mismatch trials.
</description>
    </item>
    
    <item>
        <title>Incorporating Local Acoustic Variability Information into Short Duration Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0266.PDF</link>
        <description>State-of-the-art speaker verification systems are based on the total
variability model to compactly represent the acoustic space. However,
short duration utterances only contain limited phonetic content, potentially
resulting in an incomplete representation being captured by the total
variability model thus leading to poor speaker verification performance.
In this paper, a technique to incorporate component-wise local acoustic
variability information into the speaker verification framework is
proposed. Specifically, Gaussian Probabilistic Linear Discriminant
Analysis (G-PLDA) of the supervector space, with a block diagonal covariance
assumption, is used in conjunction with the traditional total variability
model. Experimental results obtained using the NIST SRE 2010 dataset
show that the incorporation of the proposed method leads to relative
improvements of 20.48% and 18.99% in the 3 second condition for male
and female speech respectively.
</description>
    </item>
    
    <item>
        <title>DNN i-Vector Speaker Verification with Short, Text-Constrained Test Utterances</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1036.PDF</link>
        <description>We investigate how to improve the performance of DNN i-vector based
speaker verification for short, text-constrained test utterances, e.g.
connected digit strings. A text-constrained verification, due to its
smaller, limited vocabulary, can deliver better performance than a
text-independent one for a short utterance. We study the problem with
&amp;#8220;phonetically aware&amp;#8221; Deep Neural Net (DNN) in its capability
on &amp;#8220;stochastic phonetic-alignment&amp;#8221; in constructing supervectors
and estimating the corresponding i-vectors with two speech databases:
a large vocabulary, conversational, speaker independent database (Fisher)
and a small vocabulary, continuous digit database (RSR2015 Part III).
The phonetic alignment efficiency and resultant speaker verification
performance are compared with differently sized senone sets which can
characterize the phonetic pronunciations of utterances in the two databases.
Performance on RSR2015 Part III evaluation shows a relative improvement
of EER, i.e., 7.89% for male speakers and 3.54% for female speakers
with only digit related senones. The DNN bottleneck features were also
studied to investigate their capability of extracting phonetic sensitive
information which is useful for text-independent or text-constrained
speaker verifications. We found that by tandeming MFCC with bottleneck
features, EERs can be further reduced.
</description>
    </item>
    
    <item>
        <title>Time-Varying Autoregressions for Speaker Verification in Reverberant Conditions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0734.PDF</link>
        <description>In poor room acoustics conditions, speech signals received by a microphone
might become corrupted by the signals&amp;#8217; delayed versions that
are reflected from the room surfaces (e.g. wall, floor). This phenomenon,
reverberation, drops the accuracy of automatic speaker verification
systems by causing mismatch between the training and testing. Since
reverberation causes temporal smearing to the signal, one way to tackle
its effects is to study robust feature extraction, particularly based
on long-time temporal feature extraction. This approach has been adopted
previously in the form of 2-dimensional autoregressive (2DAR) feature
extraction scheme by using frequency domain linear prediction (FDLP).
In 2DAR, FDLP processing is followed by time domain linear prediction
(TDLP). In the current study, we propose modifying the latter part
of the 2DAR feature extraction scheme by replacing TDLP with time-varying
linear prediction (TVLP) to add an extra layer of temporal processing.
Our speaker verification experiments using the proposed features with
the text-dependent RedDots corpus show small but consistent improvements
in clean and reverberant conditions (up to 6.5%) over the 2DAR features
and large improvements over the MFCC features in reverberant conditions
(up to 46.5%).
</description>
    </item>
    
    <item>
        <title>Deep Speaker Embeddings for Short-Duration Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1575.PDF</link>
        <description>The performance of a state-of-the-art speaker verification system is
severely degraded when it is presented with trial recordings of short
duration. In this work we propose to use deep neural networks to learn
short-duration speaker embeddings. We focus on the 5s-5s condition,
wherein both sides of a verification trial are 5 seconds long. In our
previous work we established that learning a non-linear mapping from
i-vectors to speaker labels is beneficial for speaker verification
[1]. In this work we take the idea of learning a speaker classifier
one step further &amp;#8212; we apply deep neural networks directly to
time-frequency speech representations. We propose two feed-forward
network architectures for this task. Our best model is based on a deep
convolutional architecture wherein recordings are treated as images.
From our experimental findings we advocate treating utterances as images
or &amp;#8216;speaker snapshots&amp;#8217;, much like in face recognition.
Our convolutional speaker embeddings perform significantly better than
i-vectors when scoring is done using cosine distance, where the relative
improvement is 23.5%. The proposed deep embeddings combined with cosine
distance also outperform a state-of-the-art i-vector verification system
by 1%, providing further empirical evidence in favor of our learned
speaker features.
</description>
    </item>
    
    <item>
        <title>Using Voice Quality Features to Improve Short-Utterance, Text-Independent Speaker Verification Systems</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0157.PDF</link>
        <description>Due to within-speaker variability in phonetic content and/or speaking
style, the performance of automatic speaker verification (ASV) systems
degrades especially when the enrollment and test utterances are short.
This study examines how different types of variability influence performance
of ASV systems. Speech samples (&amp;#60; 2 sec) from the UCLA Speaker
Variability Database containing 5 different read sentences by 200 speakers
were used to study content variability. Other samples (about 5 sec)
that contained speech directed towards pets, characterized by exaggerated
prosody, were used to analyze style variability. Using the i-vector/PLDA
framework, the ASV system error rate with MFCCs had a relative increase
of at least 265% and 730% in content-mismatched and style-mismatched
trials, respectively. A set of features that represents voice quality
(F0, F1, F2, F3, H1-H2, H2-H4, H4-H2k, A1, A2, A3, and CPP) was also
used. Using score fusion with MFCCs, all conditions saw decreases in
error rates. In addition, using the NIST SRE10 database, score fusion
provided relative improvements of 11.78% for 5-second utterances, 12.41%
for 10-second utterances, and a small improvement for long utterances
(about 5 min). These results suggest that voice quality features can
improve short-utterance text-independent ASV system performance.
</description>
    </item>
    
    <item>
        <title>Gain Compensation for Fast i-Vector Extraction Over Short Duration</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0108.PDF</link>
        <description>I-vector is widely described as a compact and effective representation
of speech utterances for speaker recognition. Standard i-vector extraction
could be an expensive task for applications where computing resource
is limited, for instance, on handheld devices. Fast approximate inference
of i-vector aims to reduce the computational cost required in i-vector
extraction where run-time requirement is critical. Most fast approaches
hinge on certain assumptions to approximate the i-vector inference
formulae with little loss of accuracy. In this paper, we analyze the
 uniform assumption that we had proposed earlier. We show that the
assumption generally hold for long utterances but inadequate for utterances
of short duration. We then propose to compensate for the negative effects
by applying a simple gain factor on the i-vectors estimated from short
utterances. The assertion is confirmed through analysis and experiments
conducted on NIST SRE&amp;#8217;08 and SRE&amp;#8217;10 datasets.
</description>
    </item>
    
    <item>
        <title>Joint Training of Expanded End-to-End DNN for Text-Dependent Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1050.PDF</link>
        <description>We propose an expanded end-to-end DNN architecture for speaker verification
based on b-vectors as well as d-vectors. We embedded the components
of a speaker verification system such as modeling frame-level features,
extracting utterance-level features, dimensionality reduction of utterance-level
features, and trial-level scoring in an expanded end-to-end DNN architecture.
The main contribution of this paper is that, instead of using DNNs
as parts of the system trained independently, we train the whole system
jointly with a fine-tune cost after pre-training each part. The experimental
results show that the proposed system outperforms the baseline d-vector
system and i-vector PLDA system.
</description>
    </item>
    
    <item>
        <title>Speaker Verification via Estimating Total Variability Space Using Probabilistic Partial Least Squares</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0633.PDF</link>
        <description>The i-vector framework is one of the most popular methods in speaker
verification, and estimating a total variability space (TVS) is a key
part in the i-vector framework. Current estimation methods pay less
attention on the discrimination of TVS, but the discrimination is so
important that it will influence the improvement of performance. So
we focus on the discrimination of TVS to achieve a better performance.
In this paper, a discriminative estimating method of TVS based on probabilistic
partial least squares (PPLS) is proposed. In this method, the discrimination
is improved by using the priori information (labels) of speaker, so
both the correlation of intra-class and the discrimination of interclass
are fully utilized. Meanwhile, it also introduces a probabilistic view
of the partial least squares (PLS) method to overcome the disadvantage
of high computational complexity and the inability of channel compensation.
And also this proposed method can achieve a better performance than
the traditional TVS estimation method as well as the PLS-based method.
</description>
    </item>
    
    <item>
        <title>Deep Speaker Feature Learning for Text-Independent Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0452.PDF</link>
        <description>Recently deep neural networks (DNNs) have been used to learn speaker
features. However, the quality of the learned features is not sufficiently
good, so a complex back-end model, either neural or probabilistic,
has to be used to address the residual uncertainty when applied to
speaker verification. This paper presents a convolutional time-delay
deep neural network structure (CT-DNN) for speaker feature learning.
Our experimental results on the  Fisher database demonstrated that
this CT-DNN can produce high-quality speaker features: even with a
single feature (0.3 seconds including the context), the EER can be
as low as 7.68%. This effectively confirmed that the speaker trait
is largely a deterministic short-time property rather than a long-time
distributional pattern, and therefore can be extracted from just dozens
of frames.
</description>
    </item>
    
    <item>
        <title>Duration Mismatch Compensation Using Four-Covariance Model and Deep Neural Network for Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0093.PDF</link>
        <description>Duration mismatch between enrollment and test utterances still remains
a major concern for reliability of real-life speaker recognition applications.
Two approaches are proposed here to deal with this case when using
the i-vector representation. The first one is an adaptation of Gaussian
Probabilistic Linear Discriminant Analysis (PLDA) modeling, which can
be extended to the case of any shift between i-vectors drawn from two
distinct distributions. The second one attempts to map i-vectors of
truncated segments of an utterance to the i-vector of the full segment,
by the use of deep neural networks (DNN). Our results show that both
new approaches outperform the standard PLDA by about 10% relative,
noting that these back-end methods could complement those quantifying
the i-vector uncertainty during its extraction process, in the case
of duration gap.
</description>
    </item>
    
    <item>
        <title>Extended Variability Modeling and Unsupervised Adaptation for PLDA Speaker Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1586.PDF</link>
        <description>Probabilistic Linear Discriminant Analysis (PLDA) continues to be the
most effective approach for speaker recognition in the i-vector space.
This paper extends the PLDA model to include both enrollment and test
cut duration as well as to distinguish between session and channel
variability. In addition, we address the task of unsupervised adaptation
to unknown new domains in two ways: speaker-dependent PLDA parameters
and cohort score normalization using Bayes rule. Experimental results
on the NIST SRE16 task show that these principled techniques provide
state-of-the-art performance with negligible increase in complexity
over a PLDA baseline.
</description>
    </item>
    
    <item>
        <title>Improving the Effectiveness of Speaker Verification Domain Adaptation with Inadequate In-Domain Data</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0438.PDF</link>
        <description>This paper addresses speaker verification domain adaptation with inadequate
in-domain data. Specifically, we explore the cases where in-domain
data sets do not include speaker labels, contain speakers with few
samples, or contain speakers with low channel diversity. Existing domain
adaptation methods are reviewed, and their shortcomings are discussed.
We derive an unsupervised version of fully Bayesian adaptation which
reduces the reliance on rich in-domain data. When applied to domain
adaptation with inadequate in-domain data, the proposed approach yields
competitive results when the samples per speaker are reduced, and outperforms
existing supervised methods when the channel diversity is low, even
without requiring speaker labels. These results are validated on the
NIST SRE16, which uses a highly inadequate in-domain data set.
</description>
    </item>
    
    <item>
        <title>i-Vector DNN Scoring and Calibration for Noise Robust Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0656.PDF</link>
        <description>This paper proposes applying multi-task learning to train deep neural
networks (DNNs) for calibrating the PLDA scores of speaker verification
systems under noisy environments. To facilitate the DNNs to learn the
main task (calibration), several auxiliary tasks were introduced, including
the prediction of SNR and duration from i-vectors and classifying whether
an i-vector pair belongs to the same speaker or not. The possibility
of replacing the PLDA model by a DNN during the scoring stage is also
explored. Evaluations on noise contaminated speech suggest that the
auxiliary tasks are important for the DNNs to learn the main calibration
task and that the uncalibrated PLDA scores are an essential input to
the DNNs. Without this input, the DNNs can only predict the score shifts
accurately, suggesting that the PLDA model is indispensable.
</description>
    </item>
    
    <item>
        <title>Analysis of Score Normalization in Multilingual Speaker Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0803.PDF</link>
        <description>NIST Speaker Recognition Evaluation 2016 has revealed the importance
of score normalization for mismatched data conditions. This paper analyzes
several score normalization techniques for test conditions with multiple
languages. The best performing one for a PLDA classifier is an adaptive
s-norm with 30% relative improvement over the system without any score
normalization. The analysis shows that the adaptive score normalization
(using top scoring files per trial) selects cohorts that in 68% contain
recordings from the same language and in 92% of the same gender as
the enrollment and test recordings. Our results suggest that the data
to select score normalization cohorts should be a pool of several languages
and channels and if possible, its subset should contain data from the
target domain.
</description>
    </item>
    
    <item>
        <title>Alternative Approaches to Neural Network Based Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1062.PDF</link>
        <description>Just like in other areas of automatic speech processing, feature extraction
based on bottleneck neural networks was recently found very effective
for the speaker verification task. However, better results are usually
reported with more complex neural network architectures (e.g. stacked
bottlenecks), which are difficult to reproduce. In this work, we experiment
with the so called deep features, which are based on a simple feed-forward
neural network architecture. We study various forms of applying deep
features to i-vector/PDA based speaker verification. With proper settings,
better verification performance can be obtained by means of this simple
architecture as compared to the more elaborate bottleneck features.
Also, we further experiment with multi-task training, where the neural
network is trained for both speaker recognition and senone recognition
objectives. Results indicate that, with a careful weighting of the
two objectives, multi-task training can result in significantly better
performing deep features.
</description>
    </item>
    
    <item>
        <title>A Distribution Free Formulation of the Total Variability Model</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0219.PDF</link>
        <description>The Total Variability Model (TVM) [1] has been widely used in audio
signal processing as a framework for capturing differences in feature
space distributions across variable length sequences by mapping them
into a fixed-dimensional representation. Its formulation requires making
an assumption about the source data distribution being a Gaussian Mixture
Model (GMM). In this paper, we show that it is possible to arrive at
the same model formulation without requiring such an assumption about
distribution of the data, by showing asymptotic normality of the statistics
used to estimate the model. We highlight some connections between TVM
and heteroscedastic Principal Component Analysis (PCA), as well as
the matrix completion problem, which lead to a computationally efficient
formulation of the Maximum Likelihood estimation problem for the model.
</description>
    </item>
    
    <item>
        <title>Domain Mismatch Modeling of Out-Domain i-Vectors for PLDA Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0668.PDF</link>
        <description>The state-of-the-art i-vector based probabilistic linear discriminant
analysis (PLDA) trained on non-target (or out-domain) data significantly
affects the speaker verification performance due to the domain mismatch
between training and evaluation data. To improve the speaker verification
performance, sufficient amount of domain mismatch compensated out-domain
data must be used to train the PLDA models successfully. In this paper,
we propose a domain mismatch modeling (DMM) technique using maximum-a-posteriori
(MAP) estimation to model and compensate the domain variability from
the out-domain training i-vectors. From our experimental results, we
found that the DMM technique can achieve at least a 24% improvement
in EER over an out-domain only baseline when speaker labels are available.
Further improvement of 3% is obtained when combining DMM with domain-invariant
covariance normalization (DICN) approach. The DMM/DICN combined technique
is shown to perform better than in-domain PLDA system with only 200
labeled speakers or 2,000 unlabeled i-vectors.
</description>
    </item>
    
    <item>
        <title>An Exploration of Dropout with LSTMs</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0129.PDF</link>
        <description>Long Short-Term Memory networks (LSTMs) are a component of many state-of-the-art
DNN-based speech recognition systems. Dropout is a popular method to
improve generalization in DNN training. In this paper we describe extensive
experiments in which we investigated the best way to combine dropout
with LSTMs &amp;#8212; specifically, projected LSTMs (LSTMP). We investigated
various locations in the LSTM to place the dropout (and various combinations
of locations), and a variety of dropout schedules. Our optimized recipe
gives consistent improvements in WER across a range of datasets, including
Switchboard, TED-LIUM and AMI.
</description>
    </item>
    
    <item>
        <title>Residual LSTM: Design of a Deep Recurrent Architecture for Distant Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0477.PDF</link>
        <description>In this paper, a novel architecture for a deep recurrent neural network,
residual LSTM is introduced. A plain LSTM has an internal memory cell
that can learn long term dependencies of sequential data. It also provides
a temporal shortcut path to avoid vanishing or exploding gradients
in the temporal domain. The residual LSTM provides an additional spatial
shortcut path from lower layers for efficient training of deep networks
with multiple LSTM layers. Compared with the previous work, highway
LSTM, residual LSTM separates a spatial shortcut path with temporal
one by using output layers, which can help to avoid a conflict between
spatial and temporal-domain gradient flows. Furthermore, residual LSTM
reuses the output projection matrix and the output gate of LSTM to
control the spatial information flow instead of additional gate networks,
which effectively reduces more than 10% of network parameters. An experiment
for distant speech recognition on the AMI SDM corpus shows that 10-layer
plain and highway LSTM networks presented 13.7% and 6.2% increase in
WER over 3-layer baselines, respectively. On the contrary, 10-layer
residual LSTM networks provided the lowest WER 41.0%, which corresponds
to 3.3% and 2.8% WER reduction over plain and highway LSTM networks,
respectively.
</description>
    </item>
    
    <item>
        <title>Unfolded Deep Recurrent Convolutional Neural Network with Jump Ahead Connections for Acoustic Modeling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0873.PDF</link>
        <description>Recurrent neural networks (RNNs) with jump ahead connections have been
used in the computer vision tasks. Still, they have not been investigated
well for automatic speech recognition (ASR) tasks. In other words,
unfolded RNN has been shown to be an effective model for acoustic modeling
tasks. This paper investigates how to elaborate a sophisticated unfolded
deep RNN architecture in which recurrent connections use a convolutional
neural network (CNN) to model a short-term dependence between hidden
states. In this study, our unfolded RNN architecture is a CNN that
process a sequence of input features sequentially. Each time step,
the CNN inputs a small block of the input features and the output of
the hidden layer from the preceding block in order to compute the output
of its hidden layer. In addition, by exploiting either one or multiple
jump ahead connections between time steps, our network can learn long-term
dependencies more effectively. We carried experiments on the CHiME
3 task showing the effectiveness of our proposed approach.
</description>
    </item>
    
    <item>
        <title>Forward-Backward Convolutional LSTM for Acoustic Modeling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0554.PDF</link>
        <description>An automatic speech recognition (ASR) performance has greatly improved
with the introduction of convolutional neural network (CNN) or long-short
term memory (LSTM) for acoustic modeling. Recently, a convolutional
LSTM (CLSTM) has been proposed to directly use convolution operation
within the LSTM blocks and combine the advantages of both CNN and LSTM
structures into a single architecture. This paper presents the first
attempt to use CLSTMs for acoustic modeling. In addition, we propose
a new forward-backward architecture to exploit long-term left/right
context efficiently. The proposed scheme combines forward and backward
LSTMs at different time points of an utterance with the aim of modeling
long term frame invariant information such as speaker characteristics,
channel etc. Furthermore, the proposed forward-backward architecture
can be trained with truncated back-propagation-through-time unlike
conventional bidirectional LSTM (BLSTM) architectures. Therefore, we
are able to train deeply stacked CLSTM acoustic models, which is practically
challenging with conventional BLSTMs. Experimental results show that
both CLSTM and forward-backward LSTM improve word error rates significantly
compared to standard CNN and LSTM architectures.
</description>
    </item>
    
    <item>
        <title>Convolutional Recurrent Neural Networks for Small-Footprint Keyword Spotting</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1737.PDF</link>
        <description>Keyword spotting (KWS) constitutes a major component of human-technology
interfaces. Maximizing the detection accuracy at a low false alarm
(FA) rate, while minimizing the footprint size, latency and complexity
are the goals for KWS. Towards achieving them, we study Convolutional
Recurrent Neural Networks (CRNNs). Inspired by large-scale state-of-the-art
speech recognition systems, we combine the strengths of convolutional
layers and recurrent layers to exploit local structure and long-range
context. We analyze the effect of architecture parameters, and propose
training strategies to improve performance. With only &amp;#126;230k parameters,
our CRNN model yields acceptably low latency, and achieves 97.71% accuracy
at 0.5 FA/hour for 5 dB signal-to-noise ratio.
</description>
    </item>
    
    <item>
        <title>Deep Activation Mixture Model for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1233.PDF</link>
        <description>Deep learning approaches achieve state-of-the-art performance in a
range of applications, including speech recognition. However, the parameters
of the deep neural network (DNN) are hard to interpret, which makes
regularisation and adaptation to speaker or acoustic conditions challenging.
This paper proposes the deep activation mixture model (DAMM) to address
these problems. The output of one hidden layer is modelled as the sum
of a mixture and residual models. The mixture model forms an activation
function contour while the residual one models fluctuations around
the contour. The use of the mixture model gives two advantages: First,
it introduces a novel regularisation on the DNN. Second, it allows
novel adaptation schemes. The proposed approach is evaluated on a large-vocabulary
U.S. English broadcast news task. It yields a slightly better performance
than the DNN baselines, and on the utterance-level unsupervised adaptation,
the adapted DAMM acquires further performance gains.
</description>
    </item>
    
    <item>
        <title>Ensembles of Multi-Scale VGG Acoustic Models</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0920.PDF</link>
        <description>We present our work on constructing multi-scale deep convolutional
neural networks for automatic speech recognition. Several VGG nets
have been trained that differ solely in the kernel size of the convolutional
layers. The general idea is that receptive fields of varying sizes
match structures of different scales, thus supporting more robust recognition
when combined appropriately. We construct a large multi-scale system
by means of system combination. We use ROVER and the fusion of posterior
predictions as examples of late combination, and knowledge distillation
using soft labels from a model ensemble as a way of early combination.
In this work, distillation is approached from the perspective of knowledge
transfer pre-training, which is followed by a fine-tuning on the original
hard labels. Our results show that it is possible to bundle the individual
recognition strengths of the VGGs in a much simpler CNN architecture
that yields equal performance with the best late combination.
</description>
    </item>
    
    <item>
        <title>Training Context-Dependent DNN Acoustic Models Using Probabilistic Sampling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0338.PDF</link>
        <description>In current HMM/DNN speech recognition systems, the purpose of the DNN
component is to estimate the posterior probabilities of tied triphone
states. In most cases the distribution of these states is uneven, meaning
that we have a markedly different number of training samples for the
various states. This imbalance of the training data is a source of
suboptimality for most machine learning algorithms, and DNNs are no
exception. A straightforward solution is to re-sample the data, either
by upsampling the rarer classes or by downsampling the more common
classes. Here, we experiment with the so-called probabilistic sampling
method that applies downsampling and upsampling at the same time. For
this, it defines a new class distribution for the training data, which
is a linear combination of the original and the uniform class distributions.
As an extension to previous studies, we propose a new method to re-estimate
the class priors, which is required to remedy the mismatch between
the training and the test data distributions introduced by re-sampling.
Using probabilistic sampling and the proposed modification we report
5% and 6% relative error rate reductions on the TED-LIUM and on the
AMI corpora, respectively.
</description>
    </item>
    
    <item>
        <title>A Comparative Evaluation of GMM-Free State Tying Methods for ASR</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0899.PDF</link>
        <description>Deep neural network (DNN) based speech recognizers have recently replaced
Gaussian mixture (GMM) based systems as the state-of-the-art. While
some of the modeling techniques developed for the GMM based framework
may directly be applied to HMM/DNN systems, others may be inappropriate.
One such example is the creation of context-dependent tied states,
for which an efficient decision tree state tying method exists. The
tied states used to train DNNs are usually obtained using the same
tying algorithm, even though it is based on likelihoods of Gaussians,
hence it is more appropriate for HMM/GMMs. Recently, however, several
refinements have been published which seek to adapt the state tying
algorithm to the HMM/DNN hybrid architecture. Unfortunately, these
studies reported results on different (and sometimes very small) datasets,
which does not allow their direct comparison. Here, we tested four
of these methods on the same LVCSR task, and compared their performance
under the same circumstances. We found that, besides changing the input
of the context-dependent state tying algorithm, it is worth adjusting
the tying criterion as well. The methods which utilized a decision
criterion designed directly for neural networks consistently, and significantly,
outperformed those which employed the standard Gaussian-based algorithm.
</description>
    </item>
    
    <item>
        <title>Backstitch: Counteracting Finite-Sample Bias via Negative Steps</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1323.PDF</link>
        <description>In this paper we describe a modification to Stochastic Gradient Descent
(SGD) that improves generalization to unseen data. It consists of doing
two steps for each minibatch: a backward step with a small negative
learning rate, followed by a forward step with a larger learning rate.
The idea was initially inspired by ideas from adversarial training,
but we show that it can be viewed as a crude way of canceling out certain
systematic biases that come from training on finite data sets. The
method gives &amp;#126; 10% relative improvement over our best acoustic
models based on lattice-free MMI, across multiple datasets with 100&amp;#8211;300
hours of data.
</description>
    </item>
    
    <item>
        <title>Node Pruning Based on Entropy of Weights and Node Activity for Small-Footprint Acoustic Model Based on Deep Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0779.PDF</link>
        <description>This paper describes a node-pruning method for an acoustic model based
on deep neural networks (DNNs). Node pruning is a promising method
to reduce the memory usage and computational cost of DNNs. A score
function is defined to measure the importance of each node, and less
important nodes are pruned. The entropy of the activity of each node
has been used as a score function to find nodes with outputs that do
not change at all. We introduce entropy of weights of each node to
consider the number of weights and their patterns of each node. Because
the number of weights and the patterns differ at each layer, the importance
of the node should also be measured using the related weights of the
target node. We then propose a score function that integrates the entropy
of weights and node activity, which will prune less important nodes
more efficiently. Experimental results showed that the proposed pruning
method successfully reduced the number of parameters by about 6% without
any accuracy loss compared with a score function based only on the
entropy of node activity.
</description>
    </item>
    
    <item>
        <title>End-to-End Training of Acoustic Models for Large Vocabulary Continuous Speech Recognition with TensorFlow</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1284.PDF</link>
        <description>This article discusses strategies for end-to-end training of state-of-the-art
acoustic models for Large Vocabulary Continuous Speech Recognition
(LVCSR), with the goal of leveraging TensorFlow components so as to
make efficient use of large-scale training sets, large model sizes,
and high-speed computation units such as Graphical Processing Units
(GPUs). Benchmarks are presented that evaluate the efficiency of different
approaches to batching of training data, unrolling of recurrent acoustic
models, and device placement of TensorFlow variables and operations.
An overall training architecture developed in light of those findings
is then described. The approach makes it possible to take advantage
of both data parallelism and high speed computation on GPU for state-of-the-art
sequence training of acoustic models. The effectiveness of the design
is evaluated for different training schemes and model sizes, on a 15,000
hour Voice Search task.
</description>
    </item>
    
    <item>
        <title>An Efficient Phone N-Gram Forward-Backward Computation Using Dense Matrix Multiplication</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1557.PDF</link>
        <description>The forward-backward algorithm is commonly used to train neural network
acoustic models when optimizing a sequence objective like MMI and sMBR.
Recent work on lattice-free MMI training of neural network acoustic
models shows that the forward-backward algorithm can be computed efficiently
in the probability domain as a series of sparse matrix multiplications
using GPUs. In this paper, we present a more efficient way of computing
forward-backward using a dense matrix multiplication approach. We do
this by exploiting the block-diagonal structure of the n-gram state
transition matrix; instead of multiplying large sparse matrices, the
proposed method involves a series of smaller dense matrix multiplications,
which can be computed in parallel. Efficient implementation can be
easily achieved by leveraging on the optimized matrix multiplication
routines provided by standard libraries, such as NumPy and TensorFlow.
Runtime benchmarks show that the dense multiplication method is consistently
faster than the sparse multiplication method (on both CPUs and GPUs),
when applied to a 4-gram phone language model. This is still the case
even when the sparse multiplication method uses a more compact finite
state model representation by excluding unseen n-grams.
</description>
    </item>
    
    <item>
        <title>Parallel Neural Network Features for Improved Tandem Acoustic Modeling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1747.PDF</link>
        <description>The combination of acoustic models or features is a standard approach
to exploit various knowledge sources. This paper investigates the concatenation
of different bottleneck (BN) neural network (NN) outputs for tandem
acoustic modeling. Thus, combination of NN features is performed via
Gaussian mixture models (GMM). Complementarity between the NN feature
representations is attained by using various network topologies: LSTM
recurrent, feed-forward, and hierarchical, as well as different non-linearities:
hyperbolic tangent, sigmoid, and rectified linear units. Speech recognition
experiments are carried out on various tasks: telephone conversations,
Skype calls, as well as broadcast news and conversations. Results indicate
that LSTM based tandem approach is still competitive, and such tandem
model can challenge comparable hybrid systems. The traditional steps
of tandem modeling, speaker adaptive and sequence discriminative GMM
training, improve the tandem results further. Furthermore, these &amp;#8220;old-fashioned&amp;#8221;
steps remain applicable after the concatenation of multiple neural
network feature streams. Exploiting the parallel processing of input
feature streams, it is shown that 2&amp;#8211;5% relative improvement could
be achieved over the single best BN feature set. Finally, we also report
results after neural network based language model rescoring and examine
the system combination possibilities using such complex tandem models.
</description>
    </item>
    
    <item>
        <title>Acoustic Feature Learning via Deep Variational Canonical Correlation Analysis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1581.PDF</link>
        <description>We study the problem of acoustic feature learning in the setting where
we have access to another (non-acoustic) modality for feature learning
but not at test time. We use deep variational canonical correlation
analysis (VCCA), a recently proposed deep generative method for multi-view
representation learning. We also extend VCCA with improved latent variable
priors and with adversarial learning. Compared to other techniques
for multi-view feature learning, VCCA&amp;#8217;s advantages include an
intuitive latent variable interpretation and a variational lower bound
objective that can be trained end-to-end efficiently. We compare VCCA
and its extensions with previous feature learning methods on the University
of Wisconsin X-ray Microbeam Database, and show that VCCA-based feature
learning improves over previous methods for speaker-independent phonetic
recognition.
</description>
    </item>
    
    <item>
        <title>Online End-of-Turn Detection from Speech Based on Stacked Time-Asynchronous Sequential Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0651.PDF</link>
        <description>This paper presents a novel modeling called stacked time-asynchronous
sequential networks (STASNs) for online end-of-turn detection. An online
end-of-turn detection that determines turn-taking points in a real-time
manner is an essential component for human-computer interaction systems.
In this study, we use long-range sequential information of multiple
time-asynchronous sequential features, such as prosodic, phonetic,
and lexical sequential features, to enhance online end-of-turn detection
performance. Our key idea is to embed individual sequential features
in a fixed-length continuous representation by using sequential networks.
This enables us to simultaneously handle multiple time-asynchronous
sequential features for end-of-turn detection. STASNs can embed all
of the sequential information between a start-of-conversation and the
current end-of-utterance in a fixed-length continuous representation
that can be directly used for classification by stacking multiple sequential
networks. Experiments show that STASNs outperforms conventional modeling
with limited sequential information. Furthermore, STASNs with senone
bottleneck features extracted using senone-based deep neural networks
have superior performance without requiring lexical features decoded
by an automatic speech recognition process.
</description>
    </item>
    
    <item>
        <title>Improving Prediction of Speech Activity Using Multi-Participant Respiratory State</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1176.PDF</link>
        <description>One consequence of situated face-to-face conversation is the co-observability
of participants&amp;#8217; respiratory movements and sounds. We explore
whether this information can be exploited in predicting incipient speech
activity. Using a methodology called stochastic turn-taking modeling,
we compare the performance of a model trained on speech activity alone
to one additionally trained on static and dynamic lung volume features.
The methodology permits automatic discovery of temporal dependencies
across participants and feature types. Our experiments show that respiratory
information substantially lowers cross-entropy rates, and that this
generalizes to unseen data.
</description>
    </item>
    
    <item>
        <title>Turn-Taking Offsets and Dialogue Context</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1495.PDF</link>
        <description>A number of researchers have studied turn-taking offsets in human-human
dialogues. However, that work collapses over a wide number of different
turn-taking contexts. In this work, we delve into the turn-taking delays
based on different contexts. We show that turn-taking behavior, both
who tends to take the turn next, and the turn-taking delays, are dependent
on the previous speech act type, the upcoming speech act, and the nature
of the dialogue. This strongly suggests that in studying turn-taking,
all turn-taking events should not be grouped together. This also suggests
that delays are due to cognitive processing of what to say, rather
than whether a speaker should take the turn.
</description>
    </item>
    
    <item>
        <title>Towards Deep End-of-Turn Prediction for Situated Spoken Dialogue Systems</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1593.PDF</link>
        <description>We address the challenge of improving live end-of-turn detection for
situated spoken dialogue systems. While traditionally silence thresholds
have been used to detect the user&amp;#8217;s end-of-turn, such an approach
limits the system&amp;#8217;s potential fluidity in interaction, restricting
it to a purely reactive paradigm. By contrast, here we present a system
which takes a predictive approach. The user&amp;#8217;s end-of-turn is
predicted live as acoustic features and words are consumed by the system.
We compare the benefits of live lexical and acoustic information by
feature analysis and testing equivalent models with different feature
sets with a common deep learning architecture, a Long Short-Term Memory
(LSTM) network. We show the usefulness of incremental enriched language
model features in particular. Training and testing onWizard-of-Oz data
collected to train an agent in a simple virtual world, we are successful
in improving over a reactive baseline in terms of reducing latency
whilst minimising the cut-in rate.
</description>
    </item>
    
    <item>
        <title>End-of-Utterance Prediction by Prosodic Features and Phrase-Dependency Structure in Spontaneous Japanese Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0837.PDF</link>
        <description>This study is aimed at uncovering a way that participants in conversation
predict end-of-utterance for spontaneous Japanese speech. In spontaneous
everyday conversation, the participants must predict the ends of utterances
of a speaker to perform smooth turn-taking without too much gap. We
consider that they utilize not only syntactic factors but also prosodic
factors for the end-of-utterance prediction because of the difficulty
of prediction of a syntactic completion point in spontaneous Japanese.
In previous studies, we found that prosodic features changed significantly
in the final accentual phrase. However, it is not clear what prosodic
features support the prediction. In this paper, we focused on dependency
structure among bunsetsu-phrases as the syntactic factor, and investigated
the relation between the phrase-dependency and prosodic features. The
results showed that the average fundamental frequency and the average
intensity for accentual phrases did not decline until the modified
phrase appeared. Next, to predict the end of utterance from the syntactic
and prosodic features, we constructed a generalized linear mixed model.
The model provided higher accuracy than using the prosodic features
only. These suggest the possibility that prosodic changes and phrase-dependency
relations inform the hearer that the utterance is approaching its end.
</description>
    </item>
    
    <item>
        <title>Turn-Taking Estimation Model Based on Joint Embedding of Lexical and Prosodic Contents</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0965.PDF</link>
        <description>A natural conversation involves rapid exchanges of turns while talking.
Taking turns at appropriate timing or intervals is a requisite feature
for a dialog system as a conversation partner. This paper proposes
a model that estimates the timing of turn-taking during verbal interactions.
Unlike previous studies, our proposed model does not rely on a silence
region between sentences since a dialog system must respond without
large gaps or overlaps. We propose a Recurrent Neural Network (RNN)
based model that takes the joint embedding of lexical and prosodic
contents as its input to classify utterances into turn-taking related
classes and estimates the turn-taking timing. To this end, we trained
a neural network to embed the lexical contents, the fundamental frequencies,
and the speech power into a joint embedding space. To learn meaningful
embedding spaces, the prosodic features from each single utterance
are pre-trained using RNN and combined with utterance lexical embedding
as the input of our proposed model. We tested this model on a spontaneous
conversation dataset and confirmed that it outperformed the use of
word embedding-based features.
</description>
    </item>
    
    <item>
        <title>Social Signal Detection in Spontaneous Dialogue Using Bidirectional LSTM-CTC</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0457.PDF</link>
        <description>Non-verbal speech cues such as laughter and fillers, which are collectively
called social signals, play an important role in human communication.
Therefore, detection of them would be useful for dialogue systems to
infer speaker&amp;#8217;s intentions, emotions and engagements. The conventional
approaches are based on frame-wise classifiers, which require precise
time-alignment of these events for training. This work investigates
the Connectionist Temporal Classification (CTC) approach which can
learn an alignment between the input and its target label sequence.
This allows for robust detection of the events and efficient training
without precise time information. Experimental evaluations with various
settings demonstrate that CTC based on bidirectional LSTM outperforms
the conventional DNN and HMM based methods.
</description>
    </item>
    
    <item>
        <title>Entrainment in Multi-Party Spoken Dialogues at Multiple Linguistic Levels</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1568.PDF</link>
        <description>Linguistic entrainment, the phenomena whereby dialogue partners speak
more similarly to each other in a variety of dimensions, is key to
the success and naturalness of interactions. While there is considerable
evidence for both lexical and acoustic-prosodic entrainment, little
work has been conducted to investigate the relationship between these
two different modalities using the same measures in the same dialogues,
specifically in multi-party dialogue. In this paper, we measure lexical
and acoustic-prosodic entrainment for multi-party teams to explore
whether entrainment occurs at multiple levels during conversation and
to understand the relationship between these two modalities.
</description>
    </item>
    
    <item>
        <title>Measuring Synchrony in Task-Based Dialogues</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1604.PDF</link>
        <description>In many contexts from casual everyday conversations to formal discussions,
people tend to repeat their interlocutors, and themselves. This phenomenon
not only yields random repetitions one might expect from a natural
Zipfian distribution of linguistic forms, but also projects underlying
discourse mechanisms and rhythms that researchers have suggested establishes
conversational involvement and may support communicative progress towards
mutual understanding. In this paper, advances in an automated method
for assessing interlocutor synchrony in task-based Human-to-Human interactions
are reported. The method focuses on dialogue structure, rather than
temporal distance, measuring repetition between speakers and their
interlocutors last n-turns (n = 1, however far back in the conversation
that might have been) rather than utterances during a prior window
fixed by duration. The significance of distinct linguistic levels of
repetition are assessed by observing contrasts between actual and randomized
dialogues, in order to provide a quantifying measure of communicative
success. Definite patterns of repetitions where identified, notably
in contrasting the role of participants (as information giver or follower).
The extent to which those interacted sometime surprisingly with gender,
eye-contact and familiarity is the principal contribution of this work.
</description>
    </item>
    
    <item>
        <title>Sequence to Sequence Modeling for User Simulation in Dialog Systems</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0161.PDF</link>
        <description>User simulators are a principal offline method for training and evaluating
human-computer dialog systems. In this paper, we examine simple sequence-to-sequence
neural network architectures for training end-to-end, natural language
to natural language, user simulators, using only raw logs of previous
interactions without any additional human labelling. We compare the
neural network-based simulators with a language model (LM)-based approach
for creating natural language user simulators. Using both an automatic
evaluation using LM perplexity and a human evaluation, we demonstrate
that the sequence-to-sequence approaches outperform the LM-based method.
We show correlation between LM perplexity and the human evaluation
on this task, and discuss the benefits of different neural network
architecture variations.
</description>
    </item>
    
    <item>
        <title>Human and Automated Scoring of Fluency, Pronunciation and Intonation During Human&amp;#8211;Machine Spoken Dialog Interactions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1213.PDF</link>
        <description>We present a spoken dialog-based framework for the computer-assisted
language learning (CALL) of conversational English. In particular,
we leveraged the open-source HALEF dialog framework to develop a job
interview conversational application. We then used crowdsourcing to
collect multiple interactions with the system from non-native English
speakers. We analyzed human-rated scores of the recorded dialog data
on three different scoring dimensions critical to the delivery of conversational
English &amp;#8212; fluency, pronunciation and intonation/stress &amp;#8212;
and further examined the efficacy of automatically-extracted, hand-curated
speech features in predicting each of these sub-scores. Machine learning
experiments showed that trained scoring models generally perform at
par with the human inter-rater agreement baseline in predicting human-rated
scores of conversational proficiency.
</description>
    </item>
    
    <item>
        <title>Hierarchical LSTMs with Joint Learning for Estimating Customer Satisfaction from Contact Center Calls</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0725.PDF</link>
        <description>This paper presents a joint modeling of both turn-level and call-level
customer satisfaction in contact center dialogue. Our key idea is to
directly apply turn-level estimation results to call-level estimation
and optimize them jointly; previous work treated both estimations as
being independent. Proposed joint modeling is achieved by stacking
two types of long short-term memory recurrent neural networks (LSTM-RNNs).
The lower layer employs LSTM-RNN for sequential labeling of turn-level
customer satisfaction in which each label is estimated from context
information extracted from not only the target turn but also the surrounding
turns. The upper layer uses another LSTM-RNN to estimate call-level
customer satisfaction labels from all information of estimated turn-level
customer satisfaction. These two networks can be efficiently optimized
by joint learning of both types of labels. Experiments show that the
proposed method outperforms a conventional support vector machine based
method in terms of both turn-level and call-level customer satisfaction
with relative error reductions of over 20%.
</description>
    </item>
    
    <item>
        <title>Domain-Independent User Satisfaction Reward Estimation for Dialogue Policy Learning</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1032.PDF</link>
        <description>Learning suitable and well-performing dialogue behaviour in statistical
spoken dialogue systems has been in the focus of research for many
years. While most work which is based on reinforcement learning employs
an objective measure like task success for modelling the reward signal,
we propose to use a reward based on user satisfaction. We will show
in simulated experiments that a live user satisfaction estimation model
may be applied resulting in higher estimated satisfaction whilst achieving
similar success rates. Moreover, we will show that one satisfaction
estimation model which has been trained on one domain may be applied
in many other domains which cover a similar task. We will verify our
findings by employing the model to one of the domains for learning
a policy from real users and compare its performance to policies using
the user satisfaction and task success acquired directly from the users
as reward.
</description>
    </item>
    
    <item>
        <title>Analysis of the Relationship Between Prosodic Features of Fillers and its Forms or Occurrence Positions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1006.PDF</link>
        <description>Fillers are involved in the ease of understanding by listeners and
turn-taking. However, the knowledge about its prosodic features is
insufficient, and its modeling has not been done either. For these
reasons, there is insufficient knowledge to generate natural and appropriate
fillers in a dialog system at present. Therefore, for the purpose of
clarifying the prosodic features of fillers, its relationship with
occurrence positions or forms were analyzed in this research. &amp;#8216;Ano&amp;#8217;
and &amp;#8216;Eto&amp;#8217; were used as forms, non-/boundary of Dialog Act
and non-/turn-taking for occurrence positions. Duration, F0, and intensity
were utilized as prosodic features. As a result, the followings were
found out: the prosodic features are different depending on the difference
of the occurrence positions even for fillers of the same form, and
similar prosodic features are found between the same occurrence positions
even in different forms.
</description>
    </item>
    
    <item>
        <title>Cross-Subject Continuous Emotion Recognition Using Speech and Body Motion in Dyadic Interactions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1413.PDF</link>
        <description>Dyadic interactions encapsulate rich emotional exchange between interlocutors
suggesting a multimodal, cross-speaker and cross-dimensional continuous
emotion dependency. This study explores the dynamic inter-attribute
emotional dependency at the cross-subject level with implications to
continuous emotion recognition based on speech and body motion cues.
We propose a novel two-stage Gaussian Mixture Model mapping framework
for the continuous emotion recognition problem. In the first stage,
we perform continuous emotion recognition (CER) of both speakers from
speech and body motion modalities to estimate activation, valence and
dominance (AVD) attributes. In the second stage, we improve the first
stage estimates by performing CER of the selected speaker using her/his
speech and body motion modalities as well as using the estimated affective
attribute(s) of the other speaker. Our experimental evaluations indicate
that the second stage, cross-subject continuous emotion recognition
(CSCER), provides complementary information to recognize the affective
state, and delivers promising improvements for the continuous emotion
recognition problem.
</description>
    </item>
    
    <item>
        <title>An Automatically Aligned Corpus of Child-Directed Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0379.PDF</link>
        <description>Forced alignment would enable phonetic analyses of child directed speech
(CDS) corpora which have existing transcriptions. But existing alignment
systems are inaccurate due to the atypical phonetics of CDS. We adapt
a Kaldi forced alignment system to CDS by extending the dictionary
and providing it with heuristically-derived hints for vowel locations.
Using this system, we present a new time-aligned CDS corpus with a
million aligned segments. We manually correct a subset of the corpus
and demonstrate that our system is 70% accurate. Both our automatic
and manually corrected alignments are publically available at  osf.io/ke44q.
</description>
    </item>
    
    <item>
        <title>A Comparison of Danish Listeners&amp;#8217; Processing Cost in Judging the Truth Value of Norwegian, Swedish, and English Sentences</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0009.PDF</link>
        <description>The present study used a sentence verification task to assess the processing
cost involved in native Danish listeners&amp;#8217; attempts to comprehend
true/false statements spoken in Danish, Norwegian, Swedish, and English.
Three groups of native Danish listeners heard 40 sentences each which
were translation equivalents, and assessed the truth value of these
statements. Group 1 heard sentences in Danish and Norwegian, Group
2 in Danish and Swedish, and Group 3 in Danish and English. Response
time and proportion of correct responses were used as indices of processing
cost. Both measures indicate that the processing cost for native Danish
listeners in comprehending Danish and English statements is equivalent,
whereas Norwegian and Swedish statements incur a much higher cost,
both in terms of response time and correct assessments. The results
are discussed with regard to the costs of inter-Scandinavian and English
lingua franca communication.
</description>
    </item>
    
    <item>
        <title>On the Role of Temporal Variability in the Acquisition of the German Vowel Length Contrast</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1282.PDF</link>
        <description>This study is part of a larger project investigating the acquisition
of stable vowel-plus-consonant timing patterns needed to convey the
phonemic vowel length and the voicing contrast in German. The research
is motivated by findings showing greater temporal variability in children
until the age of 12. The specific aims of the current study were to
test (1) whether temporal variability in the production of the vowel
length contrast decreases with increasing age (in general and more
so when the variability is speech rate induced) and (2) whether duration
cues are perceived more categorically with increasing age. Production
and perception data were obtained from eleven preschool, five school
children and eleven adults. Results revealed that children produce
the quantity contrast with temporal patterns that are similar to adults&amp;#8217;
patterns, although vowel duration was overall longer and variability
slightly higher in faster speech and younger children. Apart from that,
the two groups of children did not differ in production. In perception,
however, school children&amp;#8217;s response patterns to a continuum from
a long vowel to a short vowel word were in between those of adults
and preschool children. Findings are discussed with respect to motor
control and phonemic abstraction.
</description>
    </item>
    
    <item>
        <title>A Data-Driven Approach for Perceptually Validated Acoustic Features for Children&#8217;s Sibilant Fricative Productions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Proficiency Assessment of ESL Learner&amp;#8217;s Sentence Prosody with TTS Synthesized Voice as Reference</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0064.PDF</link>
        <description>We investigate how to assess the prosody quality of an ESL learner&amp;#8217;s
spoken sentence against native speaker&amp;#8217;s natural recording or
TTS synthesized voice. A spoken English utterance read by an ESL leaner
is compared with the recording of a native speaker, or TTS voice. The
corresponding F0 contours (with voicings) and breaks are compared at
the mapped syllable level via a DTW. The correlations between the prosody
patterns of learner and native speaker (or TTS voice) of the same sentence
are computed after the speech rates and F0 distributions between speakers
are equalized. Based upon collected native and non-native speakers&amp;#8217;
databases and correlation coefficients, we use Gaussian mixtures to
model them as continuous distributions for training a two-class (native
vs non-native) neural net classifier. We found that classification
accuracy between using native speaker&amp;#8217;s and TTS reference is
close, i.e., 91.2% vs 88.1%. To assess the prosody proficiency of an
ESL learner with one sentence input, the prosody patterns of our high
quality TTS is almost as effective as those of native speakers&amp;#8217;
recordings, which are more expensive and inconvenient to collect.
</description>
    </item>
    
    <item>
        <title>Mechanisms of Tone Sandhi Rule Application by Non-Native Speakers</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0143.PDF</link>
        <description>This study is the first to examine acquisition of two Mandarin tone
sandhi rules by Cantonese speakers. It designs both real and different
types of wug words to test whether learners may exploit a lexical or
computation mechanism in tone sandhi rule application. We also statistically
compared their speech production with Beijing Mandarin speakers. The
results of functional data analysis showed that non-native speakers
applied tone sandhi rules both to real and wug words in a similar manner,
indicating that they might utilize a computation mechanism and compute
the rules under phonological conditions. No significant differences
in applying these two phonological rules on reading wug words also
suggest no bias in the application of these two rules. However, their
speech production differed from native speakers. The application of
third tone sandhi rule was more categorical than native speakers in
that Cantonese speakers tended to neutralize the sandhi Tone 3 more
with Tone 2 produced in isolation compared to native speakers. Also,
Cantonese speakers might not have applied half-third tone sandhi rule
fully since they tended to raise f0 values more at the end of vowels.
</description>
    </item>
    
    <item>
        <title>Changes in Early L2 Cue-Weighting of Non-Native Speech: Evidence from Learners of Mandarin Chinese</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0289.PDF</link>
        <description>This study examined how cue-weighting of a non-native speech cue changes
during early adult second language (L2) acquisition. Ten native English
speaking learners of Mandarin Chinese performed a speeded AX-discrimination
task during months 1, 2, and 3 of a first-year Chinese course. Results
were compared to ten native Mandarin speakers. Learners&amp;#8217; reaction
time and d-prime results became more native-like after two months of
classroom study but plateaued thereafter. Multidimensional scaling
results showed a similar shift to more native-like cue-weighting as
learners attended more to pitch direction and less to pitch height.
Despite the improvements, learners&amp;#8217; month 3 configuration of
cue-weighting differed from that of native speakers; learners appeared
to weight pitch end points rather than overall pitch directions. These
results suggest that learners&amp;#8217; warping of the weights of dimensions
underlying the perceptual space changes rapidly during early acquisition
and can plateau like other measures of L2 acquisition. Previous perceptual
learning studies may have only captured initial L2 perception gains,
not the learning plateau that often follows. New methods of perceptual
learning, especially for tonal languages, are needed to advance learners
off the plateau.
</description>
    </item>
    
    <item>
        <title>Directing Attention During Perceptual Training: A Preliminary Study of Phonetic Learning in Southern Min by Mandarin Speakers</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Prosody Analysis of L2 English for Naturalness Evaluation Through Speech Modification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0332.PDF</link>
        <description>This study investigates how different prosodic features affect native
speakers&amp;#8217; naturalness judgement of L2 English speech by Chinese
students. Through subjective judgment by native speakers and objectively
measured prosodic features, timing and pitch related prosodic features,
as well as segmental goodness of pronunciation have been found to play
key roles in native speakers&amp;#8217; perception of naturalness. In order
to eliminate segmental factors, we used accent conversion techniques
that modify native reference speech with learners&amp;#8217; erroneous
prosodic cues without altering segmental properties. Experimental results
show that without interference of segmental factors, both timing and
pitch features affect naturalness of L2 speech. Timing plays a more
crucial role in naturalness than pitch. Accent modification that corrects
timing or pitch errors can improve naturalness of the speech.
</description>
    </item>
    
    <item>
        <title>Measuring Encoding Efficiency in Swedish and English Language Learner Speech Production</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0337.PDF</link>
        <description>We use n-gram language models to investigate how far language approximates
an optimal code for human communication in terms of Information Theory
[1], and what differences there are between Learner proficiency levels.
Although the language of lower level learners is simpler, it is less
optimal in terms of information theory, and as a consequence more difficult
to process.
</description>
    </item>
    
    <item>
        <title>Lexical Adaptation to a Novel Accent in German: A Comparison Between German, Swedish, and Finnish Listeners</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0369.PDF</link>
        <description>Listeners usually adjust rapidly to unfamiliar regional and foreign
accents in their native (L1) language. Non-native (L2) listeners, however,
usually struggle when confronted with unfamiliar accents in their non-native
language. The present study asks how native language background of
L2 speakers influences lexical adjustments in a novel accent of German,
in which several vowels were systematically lowered. We measured word
judgments on a lexical decision task before and after exposure to a
15-min story in the novel dialect, and compared German, Swedish and
Finnish listeners&amp;#8217; performance. Swedish is a Germanic language
and shares with German a number of lexical roots and a relatively large
vowel inventory. Finnish is a Finno-Ugric language and differs substantially
from Germanic languages in both lexicon and phonology. The results
were as predicted: descriptively, all groups showed a similar pattern
of adaptation to the accented speech, but only German and Swedish participants
showed a significant effect. Lexical and phonological relatedness between
the native and non-native languages may thus positively influence lexical
adaptation in an unfamiliar accent.
</description>
    </item>
    
    <item>
        <title>Qualitative Differences in L3 Learners&amp;#8217; Neurophysiological Response to L1 versus L2 Transfer</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0743.PDF</link>
        <description>Third language (L3) acquisition differs from first language (L1) and
second language (L2) acquisition. There are different views on whether
L1 or L2 is of primary influence on L3 acquisition in terms of transfer.
This study examines differences in the event-related brain potentials
(ERP) response to agreement incongruencies between L1 Spanish speakers
and L3 Spanish learners, comparing response differences to incongruencies
that are transferrable from the learners&amp;#8217; L1 (Swedish), or their
L2 (English). Whereas verb incongruencies, available in L3 learners&amp;#8217;
L2 but not their L1, engendered a similar response for L1 speakers
and L3 learners, adjective incongruencies, available in L3 learners&amp;#8217;
L1 but not their L2, elicited responses that differed between groups:
Adjective incongruencies engendered a negativity in the 450&amp;#8211;550
ms time window for L1 speakers only. Both congruent and incongruent
adjectives also engendered an enhanced P3 wave in L3 learners compared
to L1 speakers. Since the P300 correlates with task-related, strategic
processing, this indicates that L3 learners process grammatical features
that are transferrable from their L1 in a less automatic mode than
features that are transferrable from their L2. L3 learners therefore
seem to benefit more from their knowledge of their L2 than their knowledge
of their L1.
</description>
    </item>
    
    <item>
        <title>Articulation Rate in Swedish Child-Directed Speech Increases as a Function of the Age of the Child Even When Surprisal is Controlled for</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1052.PDF</link>
        <description>In earlier work, we have shown that articulation rate in Swedish child-directed
speech (CDS) increases as a function of the age of the child, even
when utterance length and differences in articulation rate between
subjects are controlled for. In this paper we show on utterance level
in spontaneous Swedish speech that i) for the youngest children, articulation
rate in CDS is lower than in adult-directed speech (ADS), ii) there
is a significant negative correlation between articulation rate and
surprisal (the negative log probability) in ADS, and iii) the increase
in articulation rate in Swedish CDS as a function of the age of the
child holds, even when surprisal along with utterance length and differences
in articulation rate between speakers are controlled for. These results
indicate that adults adjust their articulation rate to make it fit
the linguistic capacity of the child.
</description>
    </item>
    
    <item>
        <title>The Relationship Between the Perception and Production of Non-Native Tones</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0714.PDF</link>
        <description>To further investigate the relationship between non-native tone perception
and production, the present study trained Mandarin speakers to learn
Cantonese lexical tones with a speech shadowing paradigm. After two
weeks&amp;#8217; training, both Mandarin speakers&amp;#8217; Cantonese tone
perception and their production had improved significantly. The overall
performances in Cantonese tone perception and production are moderately
correlated, but the degree of performance change after training among
the two modalities shows no correlation, suggesting that non-native
tone perception and production might be partially correlated, but that
the improvement of the two modalities is not synchronous. A comparison
between the present study and previous studies on non-native tone learning
indicates that experience in lexical tone processing might be important
in forming the correlation between tone perception and production.
Mandarin speakers showed greater improvement in Cantonese tone perception
than in production after training, indicating that second language
(L2) perception might precede production. Besides, both the first language
(L1) and L2 tonal systems showed an influence on Mandarin speakers&amp;#8217;
learning of Cantonese tones.
</description>
    </item>
    
    <item>
        <title>MMN Responses in Adults After Exposure to Bimodal and Unimodal Frequency Distributions of Rotated Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1110.PDF</link>
        <description>The aim of the present study is to further the understanding of the
relationship between perceptual categorization and exposure to different
frequency distributions of sounds. Previous studies have shown that
speech sound discrimination proficiency is influenced by exposure to
different distributions of speech sound continua varying along one
or several acoustic dimensions, both in adults and in infants. In the
current study, adults were presented with either a bimodal or a unimodal
frequency distribution of spectrally rotated sounds along a continuum
(a vowel continuum before rotation). Categorization of the sounds,
quantified as amplitude of the event-related potential (ERP) component
mismatch negativity (MMN) in response to two of the sounds, was measured
before and after exposure. It was expected that the bimodal group would
have a larger MMN amplitude after exposure whereas the unimodal group
would have a smaller MMN amplitude after exposure. Contrary to expectations,
the MMN amplitude was smaller overall after exposure, and no difference
was found between groups. This suggests that either the previously
reported sensitivity to frequency distributions of speech sounds is
not present for non-speech sounds, or the MMN amplitude is not a sensitive
enough measure of categorization to detect an influence from passive
exposure, or both.
</description>
    </item>
    
    <item>
        <title>Float Like a Butterfly Sting Like a Bee: Changes in Speech Preceded Parkinsonism Diagnosis for Muhammad Ali</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0025.PDF</link>
        <description>Early identification of the onset of neurological disease is critical
for testing drugs or interventions to halt or slow progression. Speech
production has been proposed as an early indicator of neurological
impairment. However, for speech to be useful for early detection, speech
changes should be measurable from uncontrolled conversational speech
collected passively in natural recording environments over extended
periods of time. Such longitudinal speech data sets for testing the
robustness of algorithms are difficult to acquire. In this paper, we
exploit YouTube interviews from Muhammad Ali from 1968 to 1981, before
his 1984 diagnosis of parkinsonism. The interviews are unscripted,
conversational in nature, and of varying fidelity. We measured changes
in speech production from the Ali interviews and analyzed these changes
relative to a coded registry of blows Mr. Ali received in each of his
boxing matches over time. This provided a rich and unique opportunity
to evaluate speech change as both a function of disease progression
and as a function of fight history. Multivariate analyses revealed
changes in prosody and articulation consistent with hypokinetic dysarthria
over time, and a relationship between reduced speech intonation and
the amount of time elapsed since the most recent fight preceding the
interview.
</description>
    </item>
    
    <item>
        <title>Cepstral and Entropy Analyses in Vowels Excerpted from Continuous Speech of Dysphonic and Control Speakers</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0335.PDF</link>
        <description>There is a growing interest in Cepstral and Entropy analyses of voice
samples for defining a vocal health indicator, due to their reliability
in investigating both regular and irregular voice signals. The purpose
of this study is to determine whether the Cepstral Peak Prominence
Smoothed (CPPS) and Sample Entropy (SampEn) could differentiate dysphonic
speakers from normal speakers in vowels excerpted from readings and
to compare their discrimination power. Results are reported for 33
patients and 31 controls, who read a standardized phonetically balanced
passage while wearing a head mounted microphone. Vowels were excerpted
from recordings using Automatic Speech Recognition and, after obtaining
a measure for each vowel, individual distributions and their descriptive
statistics were considered for CPPS and SampEn. The Receiver Operating
Curve analysis revealed that the mean of the distributions was the
parameter with the highest discrimination power for both CPPS and SampEn.
CPPS showed a higher diagnostic precision than SampEn, exhibiting an
Area Under Curve (AUC) of 0.85 compared to 0.72. A negative correlation
between the parameters was found (Spearman; &amp;#961; = -0.61), with higher
SampEn corresponding to lower CPPS. The automatic method used in this
study could provide support to voice monitorings in clinic and during
individual&amp;#8217;s daily activities.
</description>
    </item>
    
    <item>
        <title>Classification of Bulbar ALS from Kinematic Features of the Jaw and Lips: Towards Computer-Mediated Assessment</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0478.PDF</link>
        <description>Recent studies demonstrated that lip and jaw movements during speech
may provide important information for the diagnosis of amyotrophic
lateral sclerosis (ALS) and for understanding its progression. A thorough
investigation of these movements is essential for the development of
intelligent video- or optically-based facial tracking systems that
could assist with early diagnosis and progress monitoring. In this
paper, we investigated the potential for a novel and expanded set of
kinematic features obtained from lips and jaw to classify articulatory
data into three stages of bulbar disease progression (i.e., pre-symptomatic,
early symptomatic, and late symptomatic). Feature selection methods
(Relief-F and mRMR) and classification algorithm (SVM) were used for
this purpose. Results showed that even with a limited number of kinematic
features it was possible to obtain good classification accuracy (nearly
80%). Given the recent development of video-based markerless methods
for tracking speech movements, these results provide strong rationale
for supporting the development of portable and cheap systems for monitoring
the orofacial function in ALS.
</description>
    </item>
    
    <item>
        <title>Zero Frequency Filter Based Analysis of Voice Disorders</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0589.PDF</link>
        <description>Pitch period and amplitude perturbations are widely used parameters
to discriminate normal and voice disorder speech. Instantaneous pitch
period and amplitude of glottal vibrations directly from the speech
waveform may not give an accurate estimation of jitter and shimmer.
In this paper, the significance of epochs (glottal closure instants)
and strength of excitation (SoE) derived from the zero-frequency filter
(ZFF) are exploited to discriminate the voice disorder and normal speech.
Pitch epoch derived from ZFF is used to compute the jitter, and SoE
derived around each epoch is used compute the shimmer. The derived
epoch-based features are analyzed on the some of the voice disorders
like Parkinson&amp;#8217;s disease, vocal fold paralysis, cyst, and gastroesophageal
reflux disease. The significance of proposed epoch-based features for
discriminating normal and pathological voices is analyzed and compared
with the state-of-the-art methods using a support vector machine classifier.
The results show that epoch-based features performed significantly
better than other methods both in clean and noisy conditions.
</description>
    </item>
    
    <item>
        <title>Hypernasality Severity Analysis in Cleft Lip and Palate Speech Using Vowel Space Area</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Automatic Prediction of Speech Evaluation Metrics for Dysarthric Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Apkinson &#8212; A Mobile Monitoring Solution for Parkinson&#8217;s Disease</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Dysprosody Differentiate Between Parkinson&amp;#8217;s Disease, Progressive Supranuclear Palsy, and Multiple System Atrophy</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0762.PDF</link>
        <description>Parkinson&amp;#8217;s disease (PD), progressive supranuclear palsy (PSP),
and multiple system atrophy (MSA) are distinctive neurodegenerative
disorders, which manifest similar motor features. Their differentiation
is crucial but difficult. Dysfunctional speech, especially dysprosody,
is a common symptom accompanying PD, PSP, and MSA from early stages.
We hypothesized that automated analysis of monologue could provide
speech patterns distinguishing PD, PSP, and MSA. We analyzed speech
recordings of 16 patients with PSP, 20 patients with MSA, and 23 patients
with PD. Our findings revealed that deviant pause production differentiated
between PSP, MSA, and PD. In addition, PSP showed greater deficits
in speech respiration when compared to MSA and PD. Automated analysis
of connected speech is easy to administer and could provide valuable
information about underlying pathology for differentiation between
PSP, MSA, and PD.
</description>
    </item>
    
    <item>
        <title>Interpretable Objective Assessment of Dysarthric Speech Based on Deep Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1222.PDF</link>
        <description>Improved performance in speech applications using deep neural networks
(DNNs) has come at the expense of reduced model interpretability. For
consumer applications this is not a problem; however, for health applications,
clinicians must be able to interpret why a predictive model made the
decision that it did. In this paper, we propose an interpretable model
for objective assessment of dysarthric speech for speech therapy applications
based on DNNs. Our model aims to predict a general impression of the
severity of the speech disorder; however, instead of directly generating
a severity prediction from a high-dimensional input acoustic feature
space, we add an intermediate interpretable layer that acts as a bottle-neck
feature extractor and constrains the solution space of the DNNs. During
inference, the model provides an estimate of severity at the output
of the network and a set of explanatory features from the intermediate
layer of the network that explain the final decision. We evaluate the
performance of the model on a dysarthric speech dataset and show that
the proposed model provides an interpretable output that is highly
correlated with the subjective evaluation of Speech-Language Pathologists
(SLPs).
</description>
    </item>
    
    <item>
        <title>Deep Autoencoder Based Speech Features for Improved Dysarthric Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1318.PDF</link>
        <description>Dysarthria is a motor speech disorder, resulting in mumbled, slurred
or slow speech that is generally difficult to understand by both humans
and machines. Traditional Automatic Speech Recognizers (ASR) perform
poorly on dysarthric speech recognition tasks. In this paper, we propose
the use of deep autoencoders to enhance the Mel Frequency Cepstral
Coefficients (MFCC) based features in order to improve dysarthric speech
recognition. Speech from healthy control speakers is used to train
an autoencoder which is in turn used to obtain improved feature representation
for dysarthric speech. Additionally, we analyze the use of severity
based tempo adaptation followed by autoencoder based speech feature
enhancement. All evaluations were carried out on Universal Access dysarthric
speech corpus. An overall absolute improvement of 16% was achieved
using tempo adaptation followed by autoencoder based speech front end
representation for DNN-HMM based dysarthric speech recognition.
</description>
    </item>
    
    <item>
        <title>Prediction of Speech Delay from Acoustic Measurements</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1740.PDF</link>
        <description>Speech delay is characterized by a difficulty with producing or perceiving
the sounds of language in comparison to one&amp;#8217;s peers. It is a
common problem in young children, occurring at a rate of about 5%.
There are high rates of co-occurring problems with language, reading,
learning, and social interactions, so intervention is needed for most.
The Goldman-Fristoe Test of Articulation (GFTA) is a standardized tool
for the assessment of consonant articulation in American English children.
GFTA scores are normalized for age and can be used to help diagnose
and assess speech delay. The GFTA was administered to 65 young children,
a mixture of delayed children and controls. Their productions of the
39 GFTA words spoken in isolation were recorded and aligned to 3-state
hidden Markov models. Seven measurements (state log likelihoods, state
durations, and total duration) were extracted from each target segment
in each word. From a subset of these measures, cross-validated statistical
models were used to predict the children&amp;#8217;s GFTA scores and whether
they were delayed. The measurements most useful for prediction came
primarily from approximants /r, l/. An analysis of the predictors and
discussion of the implications will be provided.
</description>
    </item>
    
    <item>
        <title>The Frequency Range of &amp;#8220;The Ling Six Sounds&amp;#8221; in Standard Chinese</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0329.PDF</link>
        <description>&amp;#8220;The Ling Six Sounds&amp;#8221; are a range of speech sounds encompassing
the speech frequencies that are widely used clinically to verify the
effectiveness of hearing aid fitting in children. This study focused
on the spectral features of the six sounds in Standard Chinese. We
examined the frequency range of /m, u, a, i, &amp;#642;, s/ as well as
three consonants in syllables, i.e., /m(o)/, /&amp;#642;(&amp;#x285;)/, and
/s(&amp;#x27f;)/. We presented the frequency distribution of these sounds.
Based on this, we further proposed guidelines to improve &amp;#8220;the
Ling Six-Sound Test&amp;#8221; regarding tones in Standard Chinese. We
also suggested further studies in other dialects/languages spoken in
China with regard to their phonological specifics.
</description>
    </item>
    
    <item>
        <title>Production of Sustained Vowels and Categorical Perception of Tones in Mandarin Among Cochlear-Implanted Children</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Audio Content Based Geotagging in Multimedia</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0040.PDF</link>
        <description>In this paper we propose methods to extract geographically relevant
information in a multimedia recording using its audio content. Our
method primarily is based on the fact that urban acoustic environment
consists of a variety of sounds. Hence, location information can be
inferred from the composition of sound events/classes present in the
audio. More specifically, we adopt matrix factorization techniques
to obtain semantic content of recording in terms of different sound
classes. We use semi-NMF to for to do audio semantic content analysis
using MFCCs. These semantic information are then combined to identify
the location of recording. We show that these semantic content based
geotagging can perform significantly better than state of art methods.
</description>
    </item>
    
    <item>
        <title>Time Delay Histogram Based Speech Source Separation Using a Planar Array</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0055.PDF</link>
        <description>Bin-wise time delay is a valuable clue to form the time-frequency (TF)
mask for speech source separation on the two-microphone array. On widely
spaces microphones, however, the time delay estimation suffers from
spatial aliasing. Although histogram is a simple and effective method
to tackle the problem of spatial aliasing, it can not be directly applied
on planar arrays. This paper proposes a histogram-based method to separate
multiple speech sources on the arbitrary-size planar array, where the
spatial aliasing is resisted. Time delay histogram is firstly utilized
to estimate the delays of multiple sources on each microphone pair.
The estimated delays on all pairs are then incorporated into an azimuth
histogram by means of the pairwise combination test. From the azimuth
histogram, the direction-of-arrivals (DOAs) and the number of sources
are obtained. Eventually, the TF mask is determined based on the estimated
DOAs. Some experiments were conducted under various conditions, confirming
the superiority of the proposed method.
</description>
    </item>
    
    <item>
        <title>Excitation Source Features for Improving the Detection of Vowel Onset and Offset Points in a Speech Sequence</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0135.PDF</link>
        <description>The task of detecting the vowel regions in a given speech signal is
a challenging problem. Over the years, several works on accurate detection
of vowel regions and the corresponding vowel onset points (VOPs) and
vowel end points (VEPs) have been reported. A novel front-end feature
extraction technique exploiting the temporal and spectral characteristics
of the excitation source information in the speech signal is proposed
in this paper to improve the detection of vowel regions, VOPs and VEPs.
To do the same, a three-class classifiers (vowel, non-vowel and silence)
is developed on the TIMIT database using the proposed features as well
as mel-frequency cepstral coefficients (MFCC). Statistical modeling
based on deep neural network has been employed for learning the parameters.
Using the developed three-class classifier, a given speech sample is
then forced aligned against the trained acoustic models to detect the
vowel regions. The use of proposed feature results in detection of
vowel regions quite different from those obtained through the MFCC.
Exploiting the differences in the evidences obtained by using the two
kinds of features, a technique to combine the evidences is also proposed
in order to get a better estimate of the VOPs and VEPs.
</description>
    </item>
    
    <item>
        <title>A Contrast Function and Algorithm for Blind Separation of Audio Signals</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0189.PDF</link>
        <description>This paper presents a contrast function and associated algorithm for
blind separation of audio signals. The contrast function is based on
second-order statistics to minimize the ratio between the product of
the diagonal entries and the determinant of the covariance matrix.
The contrast function can be minimized by a batch and adaptive gradient
descent method to formulate a blind source separation algorithm. Experimental
results on realistic audio signals show that the proposed algorithm
yielded comparable separation performance with benchmark algorithms
for speech signals, and outperformed benchmark algorithms for music
signals.
</description>
    </item>
    
    <item>
        <title>Weighted Spatial Covariance Matrix Estimation for MUSIC Based TDOA Estimation of Speech Source</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0199.PDF</link>
        <description>We study the estimation of time difference of arrival (TDOA) under
noisy and reverberant conditions. Conventional TDOA estimation methods
such as MUltiple SIgnal Classification (MUSIC) are not robust to noise
and reverberation due to the distortion in the spatial covariance matrix
(SCM). To address this issue, this paper proposes a robust SCM estimation
method, called weighted SCM (WSCM). In the WSCM estimation, each time-frequency
(TF) bin of the input signal is weighted by a TF mask which is 0 for
non-speech TF bins and 1 for speech TF bins in ideal case. In practice,
the TF mask takes values between 0 and 1 that are predicted by a long
short term memory (LSTM) network trained from a large amount of simulated
noisy and reverberant data. The use of mask weights significantly reduces
the contribution of low SNR TF bins to the SCM estimation, hence improves
the robustness of MUSIC. Experimental results on both simulated and
real data show that we have significantly improved the robustness of
MUSIC by using the weighted SCM.
</description>
    </item>
    
    <item>
        <title>Speaker Direction-of-Arrival Estimation Based on Frequency-Independent Beampattern</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0229.PDF</link>
        <description>The differential microphone array (DMA) becomes more and more popular
recently. In this paper, we derive the relationship between the direction-of-arrival
(DoA) and DMA&amp;#8217;s frequency-independent beampatterns. The derivation
demonstrates that the DoA can be yielded by solving a trigonometric
polynomial. Taking the dipoles as a special case of this relationship,
we propose three methods to estimate the DoA based on the dipoles.
However, we find these methods are vulnerable to the axial directions
under the reverberation environment. Fortunately, they can complement
each other owing to their robustness to different angles. Hence, to
increase the robustness to the reverberation, we proposed another new
approach by combining the advantages of these three dipole-based methods
for the speaker DoA estimation. Both simulations and experiments show
that the proposed method not only outperforms the traditional methods
for small aperture array but also is much more computationally efficient
with avoiding the spatial spectrum search.
</description>
    </item>
    
    <item>
        <title>A Mask Estimation Method Integrating Data Field Model for Speech Enhancement</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0271.PDF</link>
        <description>In most approaches based on computational auditory scene analysis (CASA),
the ideal binary mask (IBM) is often used for noise reduction. However,
it is almost impossible to obtain the IBM result. The error in IBM
estimation may greatly violate smooth evolution nature of speech because
of the energy absence in many speech-dominated time-frequency (T-F)
units. To reduce the error, the ideal ratio mask (IRM) via modeling
the spatial dependencies of speech spectrum is used as an optimal target
mask because the predictive ratio mask is less sensitive to the error
than the predictive binary mask. In this paper, we introduce a data
field (DF) to model the spatial dependencies of the cochleagram for
obtaining the ratio mask. Firstly, initial T-F units of noise and speech
are obtained from noisy speech. Then we can calculate the forms of
the potentials of noise and speech. Subsequently, their optimal potentials
which reflect their respective distribution of potential field are
obtained by the optimal influence factors of speech and noise. Finally,
we exploit the potentials of speech and noise to obtain the ratio mask.
Experimental results show that the proposed method can obtain a better
performance than the reference methods in speech quality.
</description>
    </item>
    
    <item>
        <title>Improved End-of-Query Detection for Streaming Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0496.PDF</link>
        <description>In many streaming speech recognition applications such as voice search
it is important to determine quickly and accurately when the user has
finished speaking their query. A conventional approach to this task
is to declare end-of-query whenever a fixed interval of silence is
detected by a voice activity detector (VAD) trained to classify each
frame as speech or silence. However silence detection and end-of-query
detection are fundamentally different tasks, and the criterion used
during VAD training may not be optimal. In particular the conventional
approach ignores potential acoustic cues such as filler sounds and
past speaking rate which may indicate whether a given pause is temporary
or query-final. In this paper we present a simple modification to make
the conventional VAD training criterion more closely related to end-of-query
detection. A unidirectional long short-term memory architecture allows
the system to remember past acoustic events, and the training criterion
incentivizes the system to learn to use any acoustic cues relevant
to predicting future user intent. We show experimentally that this
approach improves latency at a given accuracy by around 100 ms for
end-of-query detection for voice search.
</description>
    </item>
    
    <item>
        <title>Using Approximated Auditory Roughness as a Pre-Filtering Feature for Human Screaming and Affective Speech AED</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0593.PDF</link>
        <description>Detecting human screaming, shouting, and other verbal manifestations
of fear and anger are of great interest to security Audio Event Detection
(AED) systems. The Internet of Things (IoT) approach allows wide-covering,
powerful AED systems to be distributed across the Internet. But a good
 feature to  pre-filter the audio is critical to these systems. This
work evaluates the potential of detecting screaming and affective speech
using Auditory Roughness and proposes a very light-weight approximation
method. Our approximation uses a similar amount of Multiple Add Accumulate
(MAA) compared to short-term energy (STE), and at least 10&amp;#215; less
MAA than MFCC. We evaluated the performance of our approximated roughness
on the Mandarin Affective Speech corpus and a subset of the Youtube
AudioSet for screaming against other low-complexity features. We show
that our approximated roughness returns higher accuracy.
</description>
    </item>
    
    <item>
        <title>Improving Source Separation via Multi-Speaker Representations</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0754.PDF</link>
        <description>Lately there have been novel developments in deep learning towards
solving the cocktail party problem. Initial results are very promising
and allow for more research in the domain. One technique that has not
yet been explored in the neural network approach to this task is speaker
adaptation. Intuitively, information on the speakers that we are trying
to separate seems fundamentally important for the speaker separation
task. However, retrieving this speaker information is challenging since
the speaker identities are not known a priori and multiple speakers
are simultaneously active. There is thus some sort of chicken and egg
problem. To tackle this, source signals and i-vectors are estimated
alternately. We show that blind multi-speaker adaptation improves the
results of the network and that (in our case) the network is not capable
of adequately retrieving this useful speaker information itself.
</description>
    </item>
    
    <item>
        <title>Multiple Sound Source Counting and Localization Based on Spatial Principal Eigenvector</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0940.PDF</link>
        <description>Multiple sound source localization remains a challenging issue due
to the interaction between sources. Although traditional approaches
can locate multiple sources effectively, most of them require the number
of sound sources as a priori knowledge. However, the number of sound
sources is generally unknown in practical applications. To overcome
this problem, a spatial principal eigenvector based approach is proposed
to estimate the number and the direction of arrivals (DOAs) of multiple
speech sources. Firstly, a time-frequency (TF) bin weighting scheme
is utilized to select the TF bins dominated by single source. Then,
for these selected bins, the spatial principal eigenvectors are extracted
to construct a contribution function which is used to simultaneously
estimate the number of sources and corresponding coarse DOAs. Finally,
the coarse DOA estimations are refined by iteratively optimizing the
assignment of selected TF bins to each source. Experimental results
validate that the proposed approach yields favorable performance for
multiple sound source counting and localization in the environment
with different levels of noise and reverberation.
</description>
    </item>
    
    <item>
        <title>Subband Selection for Binaural Speech Source Localization</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0954.PDF</link>
        <description>We consider the task of speech source localization using binaural cues,
namely interaural time and level difference (ITD &amp;amp; ILD). A typical
approach is to process binaural speech using gammatone filters and
calculate frame-level ITD and ILD in each subband. The ITD, ILD and
their combination (ITLD) in each subband are statistically modelled
using Gaussian mixture models for every direction during training.
Given a binaural test-speech, the source is localized using maximum
likelihood criterion assuming that the binaural cues in each subband
are independent. We, in this work, investigate the robustness of each
subband for localization and compare their performance against the
full-band scheme with 32 gammatone filters. We propose a subband selection
procedure using the training data where subbands are rank ordered based
on their localization performance. Experiments on Subject 003 from
the CIPIC database reveal that, for high SNRs, the ITD and ITLD of
just one subband centered at 296Hz is sufficient to yield localization
accuracy identical to that of the full-band scheme with a test-speech
of duration 1sec. At low SNRs, in case of ITD, the selected subbands
are found to perform better than the full-band scheme.
</description>
    </item>
    
    <item>
        <title>Unmixing Convolutive Mixtures by Exploiting Amplitude Co-Modulation: Methods and Evaluation on Mandarin Speech Recordings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1227.PDF</link>
        <description>This paper presents and evaluates two frequency-domain methods for
multi-channel sound source separation. The sources are assumed to couple
to the microphones with unknown room responses. Independent component
analysis (ICA) is applied in the frequency domain to obtain maximally
independent amplitude envelopes (AEs) at every frequency. Due to the
nature of ICA, the AEs across frequencies need to be  de-permuted.
To this end, we seek to assign AEs to the same source solely based
on the correlation in their magnitude variation against time. The resulted
time-varying spectra are inverse Fourier transformed to synthesize
separated signals. Objective evaluation showed that both methods achieve
a signal-to-interference ratio (SIR) that is comparable to Mazur et
al (2013). In addition, we created spoken Mandarin materials and recruited
age-matched subjects to perform word-by-word transcription. Results
showed that, first, speech intelligibility significantly improved after
unmixing. Secondly, while both methods achieved similar SIR, the subjects
preferred to listen to the results that were post-processed to ensure
a speech-like spectral shape; the mean opinion scores were 2.9 vs.
4.3 (out of 5) between the two methods. The present results may provide
suggestions regarding deployment of the correlation-based source separation
algorithms into devices with limited computational resources.
</description>
    </item>
    
    <item>
        <title>Bimodal Recurrent Neural Network for Audiovisual Voice Activity Detection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1573.PDF</link>
        <description> Voice activity detection (VAD) is an important preprocessing step
in speech-based systems, especially for emerging hand-free intelligent
assistants. Conventional VAD systems relying on audio-only features
are normally impaired by noise in the environment. An alternative approach
to address this problem is  audiovisual VAD (AV-VAD) systems. Modeling
timing dependencies between acoustic and visual features is a challenge
in AV-VAD. This study proposes a bimodal  recurrent neural network
(RNN) which combines audiovisual features in a principled, unified
framework, capturing the timing dependency within modalities and across
modalities. Each modality is modeled with separate  bidirectional long
short-term memory (BLSTM) networks. The output layers are used as input
of another BLSTM network. The experimental evaluation considers a large
audiovisual corpus with clean and noisy recordings to assess the robustness
of the approach. The proposed approach outperforms audio-only VAD by
7.9% (absolute) under clean/ideal conditions (i.e.,  high definition
(HD) camera, close-talk microphone). The proposed solution outperforms
the audio-only VAD system by 18.5% (absolute) when the conditions are
more challenging (i.e., camera and microphone from a tablet with noise
in the environment). The proposed approach shows the best performance
and robustness across a varieties of conditions, demonstrating its
potential for real-world applications.
</description>
    </item>
    
    <item>
        <title>Domain-Specific Utterance End-Point Detection for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1673.PDF</link>
        <description>The task of automatically detecting the end of a device-directed user
request is particularly challenging in case of switching short command
and long free-form utterances. While low-latency end-pointing configurations
typically lead to good user experiences in the case of short requests,
such as &amp;#8220;play music&amp;#8221;, it can be too aggressive in domains
with longer free-form queries, where users tend to pause noticeably
between words and hence are easily cut off prematurely. We previously
proposed an approach for accurate end-pointing by continuously estimating
pause duration features over all active recognition hypotheses. In
this paper, we study the behavior of these pause duration features
and infer domain-dependent parametrizations. We furthermore propose
to adapt the end-pointer aggressiveness on-the-fly by comparing the
Viterbi scores of active short command vs. long free-form decoding
hypotheses. The experimental evaluation evidences a 18% relative reduction
in word error rate on free-form requests while maintaining low latency
on short queries.
</description>
    </item>
    
    <item>
        <title>Speech Detection and Enhancement Using Single Microphone for Distant Speech Applications in Reverberant Environments</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1760.PDF</link>
        <description>It is well known that in reverberant environments, the human auditory
system has the ability to pre-process reverberant signals to compensate
for reflections and obtain effective cues for improved recognition.
In this study, we propose such a preprocessing technique for combined
detection and enhancement of speech using a single microphone in reverberant
environments for distant speech applications. The proposed system employs
a framework where the target speech is synthesized using continuous
auditory masks estimated from sub-band signals. Linear gammatone analysis/synthesis
filter banks are used as an auditory model for sub-band processing.
The performance of the proposed system is evaluated on the UT-DistantReverb
corpus which consists of speech recorded in a reverberant racquetball
court (T60&amp;#126;9000 msec). The current system shows an average improvement
of 15% STNR over an existing single-channel dereverberation algorithm
and 17% improvement in detecting speech frames over G729B, SOHN &amp;amp;
Combo-SAD unsupervised speech activity detectors on actual reverberant
and noisy environments.
</description>
    </item>
    
    <item>
        <title>A Post-Filtering Approach Based on Locally Linear Embedding Difference Compensation for Speech Enhancement</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0062.PDF</link>
        <description>This paper presents a novel difference compensation post-filtering
approach based on the locally linear embedding (LLE) algorithm for
speech enhancement (SE). The main goal of the proposed post-filtering
approach is to further suppress residual noises in SE-processed signals
to attain improved speech quality and intelligibility. The proposed
system can be divided into offline and online stages. In the offline
stage, we prepare paired differences: the estimated difference of SE-processed
speech; noisy speech and the ground-truth difference of clean speech;
noisy speech. In the online stage, on the basis of estimated difference
of a test utterance, we first predict the corresponding ground-truth
difference based on the LLE algorithm, and then compensate the noisy
speech with the predicted difference. In this study, we integrate a
deep denoising autoencoder (DDAE) SE method with the proposed LLE-based
difference compensation post-filtering approach. The experiment results
reveal that the proposed post-filtering approach obviously enhanced
the speech quality and intelligibility of the DDAE-based SE-processed
speech in different noise types and signal-to-noise-ratio levels.
</description>
    </item>
    
    <item>
        <title>Multi-Target Ensemble Learning for Monaural Speech Separation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0240.PDF</link>
        <description>Speech separation can be formulated as a supervised learning problem
where a machine is trained to cast the acoustic features of the noisy
speech to a time-frequency mask, or the spectrum of the clean speech.
These two categories of speech separation methods can be generally
referred as the masking-based and the mapping-based methods, but none
of them can perfectly estimate the clean speech, since any target can
only describe a part of the characteristics of the speech. However,
the estimated masks and speech spectrum can, sometimes, be complementary
as the speech is described from different perspectives. In this paper,
by adopting an ensemble framework, a multi-target deep neural network
(DNN) based method is proposed, which combines the masking-based and
the mapping-based strategies, and the DNN is trained to jointly estimate
the time-frequency masks and the clean spectrum. We show that as expected
the mask and speech spectrum based targets yield partly complementary
estimates, and the separation performance can be improved by merging
these estimates. Furthermore, a merging model trained jointly with
the multi-target DNN is developed. Experimental results indicate that
the proposed multi-target DNN based method outperforms the DNN based
algorithm which optimizes a single target.
</description>
    </item>
    
    <item>
        <title>Improved Example-Based Speech Enhancement by Using Deep Neural Network Acoustic Model for Noise Robust Example Search</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0543.PDF</link>
        <description>Example-based speech enhancement is a promising single-channel approach
for coping with highly nonstationary noise. Given a noisy speech input,
it first searches in a noisy speech corpus for the noisy speech examples
that best match the input. Then, it concatenates the clean speech examples
that are paired with the matched noisy examples to obtain an estimate
of the underlying clean speech component in the input. The quality
of the enhanced speech depends on how accurate an example search can
be performed given a noisy speech input. The example search is conventionally
performed using a Gaussian mixture model (GMM) with mel-frequency cepstral
coefficient features (MFCCs). To improve the noise robustness of the
GMM-based example search, instead of using noise sensitive MFCCs, we
have proposed using bottleneck features (BNFs), which are extracted
from a deep neural network-based acoustic model (DNN-AM) built for
automatic speech recognition. In this paper, instead of using a GMM
with noise robust BNFs, we propose the direct use of a DNN-AM in the
example search to further improve its noise robustness. Experimental
results on the Aurora4 corpus show that the DNN-AM-based example search
steadily improves the enhanced speech quality compared with the GMM-based
example search using BNFs.
</description>
    </item>
    
    <item>
        <title>Subjective Intelligibility of Deep Neural Network-Based Speech Enhancement</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1041.PDF</link>
        <description>Recent literature indicates increasing interest in deep neural networks
for use in speech enhancement systems. Currently, these systems are
mostly evaluated through objective measures of speech quality and/or
intelligibility. Subjective intelligibility evaluations of these systems
have so far not been reported. In this paper we report the results
of a speech recognition test with 15 participants, where the participants
were asked to pick out words in background noise before and after enhancement
using a common deep neural network approach. We found that, although
the objective measure STOI predicts that intelligibility should improve
or at the very least stay the same, the speech recognition threshold,
which is a measure of intelligibility, deteriorated by 4 dB. These
results indicate that STOI is not a good predictor for the subjective
intelligibility of deep neural network-based speech enhancement systems.
We also found that the postprocessing technique of global variance
normalisation does not significantly affect subjective intelligibility.
</description>
    </item>
    
    <item>
        <title>Real-Time Modulation Enhancement of Temporal Envelopes for Increasing Speech Intelligibility</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1157.PDF</link>
        <description>In this paper, a novel approach is introduced for performing real-time
speech modulation enhancement to increase speech intelligibility in
noise. The proposed modulation enhancement technique operates independently
in the frequency and time domains. In the frequency domain, a compression
function is used to perform energy reallocation within a frame. This
compression function contains novel scaling operations to ensure speech
quality. In the time domain, a mathematical equation is introduced
to reallocate energy from the louder to the quieter parts of the speech.
This proposed mathematical equation ensures that the long-term energy
of the speech is preserved independently of the amount of compression,
hence gaining full control of the time-energy reallocation in real-time.
Evaluations on intelligibility and quality show that the suggested
approach increases the intelligibility of speech while maintaining
the overall energy and quality of the speech signal.
</description>
    </item>
    
    <item>
        <title>On the Influence of Modifying Magnitude and Phase Spectrum to Enhance Noisy Speech Signals</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1173.PDF</link>
        <description>Neural networks have proven their ability to be usefully applied as
component of a speech enhancement system. This is based on the known
feature of neural nets to map regions inside a feature space to other
regions. It can be taken to map noisy magnitude spectra to clean spectra.
This way the net can be used to substitute an adaptive filtering in
the spectral domain. We set up such a system and compared its performance
against a known adaptive filtering approach in terms of speech quality
and in terms of recognition rate. It is a still not fully answered
question how far the speech quality can be enhanced by modifying not
only the magnitude but also the spectral phase and how this phase modification
could be realized. Before trying to use a neural network for a possible
modification of the phase spectrum we ran a set of oracle experiments
to find out how far the quality can be improved by modifying the magnitude
and/or the phase spectrum in voiced segments. It turns out that the
simultaneous modification of magnitude and phase spectrum has the potential
for a considerable improvement of the speech quality in comparison
to modifying the magnitude or the phase only.
</description>
    </item>
    
    <item>
        <title>MixMax Approximation as a Super-Gaussian Log-Spectral Amplitude Estimator for Speech Enhancement</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1243.PDF</link>
        <description>For single-channel speech enhancement, most commonly, the noisy observation
is described as the sum of the clean speech signal and the noise signal.
For machine learning based enhancement schemes where speech and noise
are modeled in the log-spectral domain, however, the log-spectrum of
the noisy observation can be described as the maximum of the speech
and noise log-spectrum to simplify statistical inference. This approximation
is referred to as MixMax model or log-max approximation. In this paper,
we show how this approximation can be used in combination with non-trained,
blind speech and noise power estimators derived in the spectral domain.
Our findings allow to interpret the MixMax based clean speech estimator
as a super-Gaussian log-spectral amplitude estimator. This MixMax based
estimator is embedded in a pre-trained speech enhancement scheme and
compared to a log-spectral amplitude estimator based on an additive
mixing model. Instrumental measures indicate that the MixMax based
estimator causes less musical tones while it virtually yields the same
quality for the enhanced speech signal.
</description>
    </item>
    
    <item>
        <title>Binary Mask Estimation Strategies for Constrained Imputation-Based Speech Enhancement</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1257.PDF</link>
        <description>In recent years, speech enhancement by analysis-resynthesis has emerged
as an alternative to conventional noise filtering approaches. Analysis-resynthesis
replaces noisy speech with a signal that has been reconstructed from
a clean speech model. It can deliver high-quality signals with no residual
noise, but at the expense of losing information from the original signal
that is not well-represented by the model. A recent compromise solution,
called constrained resynthesis, solves this problem by only resynthesising
spectro-temporal regions that are estimated to be masked by noise (conditioned
on the evidence in the unmasked regions). In this paper we first extend
the approach by: i) introducing multi-condition training and a deep
discriminative model for the analysis stage; ii) introducing an improved
resynthesis model that captures within-state cross-frequency dependencies.
We then extend the previous stationary-noise evaluation by using real
domestic audio noise from the CHiME-2 evaluation. We compare various
mask estimation strategies while varying the degree of constraint by
tuning the threshold for reliable speech detection. PESQ and log-spectral
distance measures show that although mask estimation remains a challenge,
it is only necessary to estimate a few reliable signal regions in order
to achieve performance close to that achieved with an optimal oracle
mask.
</description>
    </item>
    
    <item>
        <title>A Fully Convolutional Neural Network for Speech Enhancement</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1465.PDF</link>
        <description>The presence of babble noise degrades hearing intelligibility of human
speech greatly. However, removing the babble without creating artifacts
in human speech is a challenging task in a low SNR environment. Here,
we sought to solve the problem by finding a &amp;#8216;mapping&amp;#8217; between
noisy speech spectra and clean speech spectra via supervised learning.
Specifically, we propose using fully Convolutional Neural Networks,
which consist of lesser number of parameters than fully connected networks.
The proposed network, Redundant Convolutional Encoder Decoder (R-CED),
demonstrates that a convolutional network can be 12 times smaller than
a recurrent network and yet achieves better performance, which shows
its applicability for an embedded system.
</description>
    </item>
    
    <item>
        <title>Speech Enhancement Using Non-Negative Spectrogram Models with Mel-Generalized Cepstral Regularization</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1492.PDF</link>
        <description>Spectral domain speech enhancement algorithms based on non-negative
spectrogram models such as non-negative matrix factorization (NMF)
and non-negative matrix factor deconvolution are powerful in terms
of signal recovery accuracy, however they do not directly lead to an
enhancement in the feature domain (e.g., cepstral domain) or in terms
of perceived quality. We have previously proposed a method that makes
it possible to enhance speech in the spectral and cepstral domains
simultaneously. Although this method was shown to be effective, the
devised algorithm was computationally demanding. This paper proposes
yet another formulation that allows for a fast implementation by replacing
the regularization term with a divergence measure between the NMF model
and the mel-generalized cepstral (MGC) representation of the target
spectrum. Since the MGC is an auditory-motivated representation of
an audio signal widely used in parametric speech synthesis, we also
expect the proposed method to have an effect in enhancing the perceived
quality. Experimental results revealed the effectiveness of the proposed
method in terms of both the signal-to-distortion ratio and the cepstral
distance.
</description>
    </item>
    
    <item>
        <title>A Comparison of Perceptually Motivated Loss Functions for Binary Mask Estimation in Speech Separation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1504.PDF</link>
        <description>This work proposes and compares perceptually motivated loss functions
for deep learning based binary mask estimation for speech separation.
Previous loss functions have focused on maximising classification accuracy
of mask estimation but we now propose loss functions that aim to maximise
the hit minus false-alarm (HIT-FA) rate which is known to correlate
more closely to speech intelligibility. The baseline loss function
is binary cross-entropy (CE), a standard loss function used in binary
mask estimation, which maximises classification accuracy. We propose
first a loss function that maximises the HIT-FA rate instead of classification
accuracy. We then propose a second loss function that is a hybrid between
CE and HIT-FA, providing a balance between classification accuracy
and HIT-FA rate. Evaluations of the perceptually motivated loss functions
with the GRID database show improvements to HIT-FA rate and ESTOI across
babble and factory noises. Further tests then explore application of
the perceptually motivated loss functions to a larger vocabulary dataset.
</description>
    </item>
    
    <item>
        <title>Conditional Generative Adversarial Networks for Speech Enhancement and Noise-Robust Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1620.PDF</link>
        <description>Improving speech system performance in noisy environments remains a
challenging task, and speech enhancement (SE) is one of the effective
techniques to solve the problem. Motivated by the promising results
of generative adversarial networks (GANs) in a variety of image processing
tasks, we explore the potential of conditional GANs (cGANs) for SE,
and in particular, we make use of the image processing framework proposed
by Isola et al. [1] to learn a mapping from the spectrogram of noisy
speech to an enhanced counterpart. The SE cGAN consists of two networks,
trained in an adversarial manner: a generator that tries to enhance
the input noisy spectrogram, and a discriminator that tries to distinguish
between enhanced spectrograms provided by the generator and clean ones
from the database using the noisy spectrogram as a condition. We evaluate
the performance of the cGAN method in terms of perceptual evaluation
of speech quality (PESQ), short-time objective intelligibility (STOI),
and equal error rate (EER) of speaker verification (an example application).
Experimental results show that the cGAN method overall outperforms
the classical short-time spectral amplitude minimum mean square error
(STSA-MMSE) SE algorithm, and is comparable to a deep neural network-based
SE approach (DNN-SE).
</description>
    </item>
    
    <item>
        <title>Speech Enhancement Using Bayesian Wavenet</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1672.PDF</link>
        <description>In recent years, deep learning has achieved great success in speech
enhancement. However, there are two major limitations regarding existing
works. First, the Bayesian framework is not adopted in many such deep-learning-based
algorithms. In particular, the prior distribution for speech in the
Bayesian framework has been shown useful by regularizing the output
to be in the speech space, and thus improving the performance. Second,
the majority of the existing methods operate on the frequency domain
of the noisy speech, such as spectrogram and its variations. The clean
speech is then reconstructed using the approach of overlap-add, which
is limited by its inherent performance upper bound. This paper presents
a Bayesian speech enhancement framework, called BaWN (Bayesian WaveNet),
which directly operates on raw audio samples. It adopts the recently
announced WaveNet, which is shown to be effective in modeling conditional
distributions of speech samples while generating natural speech. Experiments
show that BaWN is able to recover clean and natural speech.
</description>
    </item>
    
    <item>
        <title>Binaural Reverberant Speech Separation Based on Deep Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0297.PDF</link>
        <description>Supervised learning has exhibited great potential for speech separation
in recent years. In this paper, we focus on separating target speech
in reverberant conditions from binaural inputs using supervised learning.
Specifically, deep neural network (DNN) is constructed to map from
both spectral and spatial features to a training target. For spectral
features extraction, we first convert binaural inputs into a single
signal by applying a fixed beamformer. A new spatial feature is proposed
and extracted to complement spectral features. The training target
is the recently suggested ideal ratio mask (IRM). Systematic evaluations
and comparisons show that the proposed system achieves good separation
performance and substantially outperforms existing algorithms under
challenging multi-source and reverberant environments.
</description>
    </item>
    
    <item>
        <title>On the Quality and Intelligibility of Noisy Speech Processed for Near-End Listening Enhancement</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1225.PDF</link>
        <description>Most current techniques for near-end speech intelligibility enhancement
have focused on processing clean input signals, however, in realistic
environments, the input is often noisy. Processing noisy speech for
intelligibility enhancement using algorithms developed for clean signals
can lower the perceptual quality of the samples when they are listened
in quiet. Here we address the quality loss in these conditions by combining
noise reduction with a multi-band version of a state-of-the-art intelligibility
enhancer for clean speech that is based on spectral shaping and dynamic
range compression (SSDRC). Subjective quality and intelligibility assessments
with noisy input speech showed that: (a) In quiet near-end conditions,
the proposed system outperformed the baseline SSDRC in terms of Mean
Opinion Score (MOS); (b) In speech-shaped near-end noise, the proposed
system improved the intelligibility of unprocessed speech by a factor
larger than three at the lowest tested signal-to-noise ratio (SNR)
however, overall, it yielded lower recognition scores than the standard
SSDRC.
</description>
    </item>
    
    <item>
        <title>Applications of the BBN Sage Speech Processing Platform</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Bob Speaks Kaldi</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2025.PDF</link>
        <description>This paper introduces and demonstrates Kaldi integration into Bob signal-processing
and machine learning toolbox. The motivation for this integration is
two-fold. Firstly, Bob benefits from using advanced speech processing
tools developed in Kaldi. Secondly, Kaldi benefits from using complementary
Bob modules, such as modulation-based VAD with an adaptive thresholding.
In addition, Bob is designed as an open science tool, and this integration
might offer to the Kaldi speech community a framework for better reproducibility
of state-of-the-art research results.
</description>
    </item>
    
    <item>
        <title>Real Time Pitch Shifting with Formant Structure Preservation Using the Phase Vocoder</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2028.PDF</link>
        <description>Pitch shifting in speech is presented based on the use of the phase
vocoder in combination with spectral whitening and envelope reconstruction,
applied respectively before and after the transformation. A band preservation
technique is introduced to contain quality degradation when downscaling
the pitch. The transposition ratio is fixed in advance by selecting
analysis and synthesis window sizes. Real time performance is demonstrated
for window sizes having adequate factorization required by fast Fourier
transformation.
</description>
    </item>
    
    <item>
        <title>A Signal Processing Approach for Speaker Separation Using SFF Analysis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2043.PDF</link>
        <description>Multi-speaker separation is necessary to increase intelligibility of
speech signals or to improve accuracy of speech recognition systems.
Ideal binary mask (IBM) has set a gold standard for speech separation
by suppressing the undesired speakers and also by increasing intelligibility
of the desired speech. In this work, single frequency filtering (SFF)
analysis is used to estimate the mask closer to IBM for speaker separation.
The SFF analysis gives good temporal resolution for extracting features
such as glottal closure instants (GCIs), and high spectral resolution
for resolving harmonics. The temporal resolution in SFF gives impulse
locations, which are used to calculate the time delay. The delay compensation
between two microphone signals reinforces the impulses corresponding
to one of the speakers. The spectral resolution of the SFF is exploited
to estimate the masks using the SFF magnitude spectra on the enhanced
impulse-like sequence corresponding to one of the speakers. The estimated
mask is used to refine the SFF magnitude. The refined SFF magnitude
along with the phase of the mixed microphone signal is used to obtain
speaker separation. Performance of proposed algorithm is demonstrated
using multi-speaker data collected in a real room environment.
</description>
    </item>
    
    <item>
        <title>Speech Recognition and Understanding on Hardware-Accelerated DSP</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2056.PDF</link>
        <description>A smart home controller that responds to natural language input is
demonstrated on an Intel embedded processor. This device contains two
DSP cores and a neural network co-processor which share 4MB SRAM. An
embedded configuration of the Intel RealSpeech speech recognizer and
intent extraction engine runs on the DSP cores with neural network
operations offloaded to the co-processor. The prototype demonstrates
that continuous speech recognition and understanding is possible on
hardware with very low power consumption. As an example application,
control of lights in a home via natural language is shown. An Intel
development kit is demonstrated together with a set of tools. Conference
attendees are encouraged to interact with the demo and development
system.
</description>
    </item>
    
    <item>
        <title>MetaLab: A Repository for Meta-Analyses on Language Development, and More</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2053.PDF</link>
        <description>MetaLab is a growing database of meta-analyses, shared in a github
repository and via an interactive website. This website contains interactive
tools for community-augmented meta-analyses, power analyses, and experimental
planning. It currently contains a dozen meta-analyses spanning a number
of phenomena in early language acquisition research, including infants&amp;#8217;
vowel discrimination, acoustic wordform segmentation, and distributional
learning in the laboratory. During the Show and Tell, we will demonstrate
how to use the online visualization tools, download data, and re-use
our analysis scripts for other research purposes. We expect MetaLab
data to be particularly useful to researchers interested in early speech
perception. Additionally, the infrastructure and tools can be adopted
by speech scientists seeking to perform and utilize (meta-)meta-analyses
in other fields.
</description>
    </item>
    
    <item>
        <title>Evolving Recurrent Neural Networks That Process and Classify Raw Audio in a Streaming Fashion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2030.PDF</link>
        <description>The paper describes a neuroevolution-based novel approach to train
recurrent neural networks that can process and classify audio directly
from the raw waveform signal, without any assumption on the signal
itself, on the features that should be extracted, or on the required
network topology to perform the task. Resulting networks are relatively
small in memory size, and their usage in a streaming fashion makes
them particularly suited to embedded real-time applications.
</description>
    </item>
    
    <item>
        <title>Combining Gaussian Mixture Models and Segmental Feature Models for Speaker Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2032.PDF</link>
        <description>In most speaker recognition systems speech utterances are not constrained
in content or language. In a text-dependent speaker recognition system
lexical content of speech and language are known in advance. The goal
of this paper is to show that this information can be used by a segmental
features (SF) approach to improve a standard Gaussian mixture model
with MFCC features (GMM-MFCC). Speech features such as mean energy,
delta energy, pitch, delta pitch, the formants F1&amp;#8211;F4 and their
bandwidths B1&amp;#8211;B4 and the difference between F2 and F1 are calculated
on segments and are associated to phonemes and phoneme groups for each
speaker. The SF and GMM-MFCC approaches are combined by multiplying
the outputs of two classifiers. All the experiments are performed on
the two versions of TEVOID: TEVOID16 with 16 and the upgraded TEVOID50
with 50 speakers. On TEVOID16, SF achieves 84.23%, GMM-MFCC 91.75%,
and the combined approach gives 95.12% recognition rate. On TEVOID50,
the SF approach gives 68.69%, while both GMM-MFCC and the combined
model achieve 95.84% recognition rate. On both databases, the number
of male/female confusions decreased for the combined model. These results
are promising for using segmental features to improve the recognition
rate of text-dependent systems.
</description>
    </item>
    
    <item>
        <title>&amp;#8220;Did you laugh enough today?&amp;#8221; &amp;#8212; Deep Neural Networks for Mobile and Wearable Laughter Trackers</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2036.PDF</link>
        <description>In this paper we describe a mobile and wearable devices app that recognises
laughter from speech in real-time. The laughter detection is based
on a deep neural network architecture, which runs smoothly and robustly,
even natively on a smartwatch. Further, this paper presents results
demonstrating that our approach achieves state-of-the-art laughter
detection performance on the SSPNet Vocalization Corpus (SVC) from
the 2013 Interspeech Computational Paralinguistics Challenge Social
Signals Sub-Challenge. As this technology is tailored for mobile and
wearable devices, it enables and motivates many new use cases, for
example, deployment in health care settings such as laughter tracking
for psychological coaching, depression monitoring, and therapies.
</description>
    </item>
    
    <item>
        <title>Low-Frequency Ultrasonic Communication for Speech Broadcasting in Public Transportation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2037.PDF</link>
        <description>Speech broadcasting via loudspeakers is widely used in public transportation
to send broadcast notifications. However, listeners often fail to catch
spoken context from speech broadcasts due to excessive environmental
noise. We propose an ultrasonic communication method that can be applied
to loudspeaker-based speech broadcasting to cope with this issue. In
other words, text notifications are modulated and carried over low-frequency
ultrasonic waves through loudspeakers to the microphones of each potential
listener&amp;#8217;s mobile device. Then, the received ultrasonic stream
is demodulated back into the text and the listener hears the notification
context by a text-to-speech engine embedded in each mobile device.
Such a transmission system is realized with a 20 kHz carrier frequency
because it is inaudible to most listeners but capable of being used
in communication between a loudspeaker and microphone. In addition,
the performance of the proposed ultrasonic communication method is
evaluated by measuring the success rate of transmitted words under
various signal-to-noise ratio conditions.
</description>
    </item>
    
    <item>
        <title>Real-Time Speech Enhancement with GCC-NMF: Demonstration on the Raspberry Pi and NVIDIA Jetson</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2039.PDF</link>
        <description>We demonstrate a real-time, open source implementation of the online
GCC-NMF stereo speech enhancement algorithm. While the system runs
on a variety of operating systems and hardware platforms, we highlight
its potential for real-world mobile use by presenting it on two embedded
systems: the Raspberry Pi 3 and the NVIDIA Jetson TX1. The effect of
various algorithm parameters on subjective enhancement quality may
be explored interactively via a graphical user interface, with the
results heard in real-time. The trade-off between interference suppression
and target fidelity is controlled by manipulating the parameters of
the coefficient masking function. Increasing the pre-learned dictionary
size improves overall speech enhancement quality at increased computational
cost. We show that real-time GCC-NMF has potential for real-world application,
remaining purely unsupervised and retaining the simplicity and flexibility
of offline GCC-NMF.
</description>
    </item>
    
    <item>
        <title>Reading Validation for Pronunciation Evaluation in the Digitala Project</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2033.PDF</link>
        <description>We describe a recognition, validation and segmentation system as an
intelligent preprocessor for automatic pronunciation evaluation. The
system is developed for large-scale high stake foreign language tests,
where it is necessary to reduce human workload and ensure fair evaluation.
</description>
    </item>
    
    <item>
        <title>Conversing with Social Agents That Smile and Laugh</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/3003.PDF</link>
        <description>Our aim is to create virtual conversational partners. As such we have
developed computational models to enrich virtual characters with socio-emotional
capabilities that are communicated through multimodal behaviors. The
approach we follow to build interactive and expressive interactants
relies on theories from human and social sciences as well as data analysis
and  user-perception-based design. We have explored specific social
signals such as smile and laughter, capturing their variation in production
but also their different communicative functions and their impact in
human-agent interaction. Lately we have been interested in modeling
agents with social attitudes. Our aim is to model how social attitudes
color the multimodal behaviors of the agents. We have gathered a corpus
of dyads that was annotated along two layers: social attitudes and
nonverbal behaviors. By applying sequence mining methods we have extracted
behavior patterns involved in the change of perception of an attitude.
We are particularly interested in capturing the behaviors that correspond
to a change of perception of an attitude. In this talk I will present
the GRETA/VIB platform where our research is implemented.
</description>
    </item>
    
    <item>
        <title>Team ELISA System for DARPA LORELEI Speech Evaluation 2016</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0180.PDF</link>
        <description>In this paper, we describe the system designed and developed by team
ELISA for DARPA&amp;#8217;s LORELEI (Low Resource Languages for Emergent
Incidents) pilot speech evaluation. The goal of the LORELEI program
is to guide rapid resource deployment for humanitarian relief (e.g.
for natural disasters), with a focus on &amp;#8220;low-resource&amp;#8221;
language locations, where the cost of developing technologies for automated
human language tools can be prohibitive both in monetary terms and
timewise. In this phase of the program, the speech evaluation consisted
of three separate tasks: detecting presence of an incident, classifying
incident type, and classifying incident type along with identifying
the location where it occurs. The performance metric was area under
curve of precision-recall curves. Team ELISA competed against five
other teams and won all the subtasks.
</description>
    </item>
    
    <item>
        <title>First Results in Developing a Medieval Latin Language Charter Dictation System for the East-Central Europe Region</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>The Motivation and Development of  MPAi, a M&amp;#257;ori Pronunciation Aid</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0215.PDF</link>
        <description>This paper outlines the motivation and development of a pronunciation
aid ( MPAi) for the M&amp;#257;ori language, the language of the indigenous
people of New Zealand. M&amp;#257;ori is threatened and after a break in
transmission the language is currently undergoing revitalization. The
data for the aid has come from a corpus of 60 speakers (men and women).
The language aid allows users to model their speech against exemplars
from young speakers or older speakers of M&amp;#257;ori. This is important,
because of the status of the elders in the M&amp;#257;ori speaking community,
but it also recognizes that M&amp;#257;ori is undergoing substantial vowel
change. The pronunciation aid gives feedback on vowel production via
formant analysis, and selected words via speech recognition. The evaluation
of the aid by 22 language teachers is presented and the resulting changes
are discussed.
</description>
    </item>
    
    <item>
        <title>On the Linguistic Relevance of Speech Units Learned by Unsupervised Acoustic Modeling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0300.PDF</link>
        <description>Unsupervised acoustic modeling is an important and challenging problem
in spoken language technology development for low-resource languages.
It aims at automatically learning a set of speech units from un-transcribed
data. These learned units are expected to be related to fundamental
linguistic units that constitute the concerned language. Formulated
as a clustering problem, unsupervised acoustic modeling methods are
often evaluated in terms of average purity or similar types of performance
measures. They do not provide detailed insights on the fitness of individual
learned units and the relation between them. This paper presents an
investigation on the linguistic relevance of learned speech units based
on Kullback-Leibler (KL) divergence. A symmetric KL divergence metric
is used to measure the distance between each pair of learned unit and
ground-truth phoneme of the target language. Experimental analysis
on a multilingual database shows that KL divergence is consistent with
purity in evaluating clustering results. The deviation between a learned
unit and its closest ground-truth phoneme is comparable to the inherent
variability of the phoneme. The learned speech units have a good coverage
of linguistically defined phonemes. However, there are certain phonemes
that can not be covered, for example, the retroflex final /er/ in Mandarin.
</description>
    </item>
    
    <item>
        <title>Deep Auto-Encoder Based Multi-Task Learning Using Probabilistic Transcriptions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0582.PDF</link>
        <description>We examine a scenario where we have no access to native transcribers
in the target language. This is typical of language communities that
are under-resourced. However, turkers (online crowd workers) available
in online marketplaces can serve as valuable alternative resources
for providing transcripts in the target language. We assume that the
turkers neither speak nor have any familiarity with the target language.
Thus, they are unable to distinguish all phone pairs in the target
language; their transcripts therefore specify, at best, a probability
distribution called a probabilistic transcript (PT). Standard deep
neural network (DNN) training using PTs do not necessarily improve
error rates. Previously reported results have demonstrated some success
by adopting the multi-task learning (MTL) approach. In this study,
we report further improvements by introducing a deep auto-encoder based
MTL. This method leverages large amounts of untranscribed data in the
target language in addition to the PTs obtained from turkers. Furthermore,
to encourage transfer learning in the feature space, we also examine
the effect of using monophones from transcripts in well-resourced languages.
We report consistent improvement in phone error rates (PER) for Swahili,
Amharic, Dinka, and Mandarin.
</description>
    </item>
    
    <item>
        <title>Areal and Phylogenetic Features for Multilingual Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0160.PDF</link>
        <description>We introduce phylogenetic and areal language features to the domain
of multilingual text-to-speech synthesis. Intuitively, enriching the
existing universal phonetic features with cross-lingual shared representations
should benefit the multilingual acoustic models and help to address
issues like data scarcity for low-resource languages. We investigate
these representations using the acoustic models based on long short-term
memory recurrent neural networks. Subjective evaluations conducted
on eight languages from diverse language families show that sometimes
phylogenetic and areal representations lead to significant multilingual
synthesis quality improvements. To help better leverage these novel
features, improving the baseline phonetic representation may be necessary.
</description>
    </item>
    
    <item>
        <title> SLPAnnotator: Tools for Implementing Sign Language Phonetic Annotation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>The LENA System Applied to Swedish: Reliability of the Adult Word Count Estimate</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1287.PDF</link>
        <description>The Language Environment Analysis system LENA is used to capture day-long
recordings of children&amp;#8217;s natural audio environment. The system
performs automated segmentation of the recordings and provides estimates
for various measures. One of those measures is Adult Word Count (AWC),
an approximation of the number of words spoken by adults in close proximity
to the child. The LENA system was developed for and trained on American
English, but it has also been evaluated on its performance when applied
to Spanish, Mandarin and French. The present study is the first evaluation
of the LENA system applied to Swedish, and focuses on the AWC estimate.
Twelve five-minute segments were selected at random from each of four
day-long recordings of 30-month-old children. Each of these 48 segments
was transcribed by two transcribers, and both number of words and number
of vowels were calculated (inter-transcriber reliability for words:
r = .95, vowels: r = .93). Both counts correlated with the LENA system&amp;#8217;s
AWC estimate for the same segments (words: r = .67, vowels: r = .66).
The reliability of the AWC as estimated by the LENA system when applied
to Swedish is therefore comparable to its reliability for Spanish,
Mandarin and French.
</description>
    </item>
    
    <item>
        <title>What do Babies Hear? Analyses of Child- and Adult-Directed Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1409.PDF</link>
        <description>Child-directed speech is argued to facilitate language development,
and is found cross-linguistically and cross-culturally to varying degrees.
However, previous research has generally focused on short samples of
child-caregiver interaction, often in the lab or with experimenters
present. We test the generalizability of this phenomenon with an initial
descriptive analysis of the speech heard by young children in a large,
unique collection of naturalistic, daylong home recordings. Trained
annotators coded automatically-detected adult speech &amp;#8216;utterances&amp;#8217;
from 61 homes across 4 North American cities, gathered from children
(age 2&amp;#8211;24 months) wearing audio recorders during a typical day.
Coders marked the speaker gender (male/female) and intended addressee
(child/adult), yielding 10,886 addressee and gender tags from 2,523
minutes of audio (cf. HB-CHAAC Interspeech ComParE challenge; Schuller
et al., in press). Automated speaker-diarization (LENA) incorrectly
gender-tagged 30% of male adult utterances, compared to manually-coded
consensus. Furthermore, we find effects of SES and gender on child-directed
and overall speech, increasing child-directed speech with child age,
and interactions of speaker gender, child gender, and child age: female
caretakers increased their child-directed speech more with age than
male caretakers did, but only for male infants. Implications for language
acquisition and existing classification algorithms are discussed.
</description>
    </item>
    
    <item>
        <title>A New Workflow for Semi-Automatized Annotations: Tests with Long-Form Naturalistic Recordings of Childrens Language Environments</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1418.PDF</link>
        <description>Interoperable annotation formats are fundamental to the utility, expansion,
and sustainability of collective data repositories. In language development
research, shared annotation schemes have been critical to facilitating
the transition from raw acoustic data to searchable, structured corpora.
Current schemes typically require comprehensive and manual annotation
of utterance boundaries and orthographic speech content, with an additional,
optional range of tags of interest. These schemes have been enormously
successful for datasets on the scale of dozens of recording hours but
are untenable for long-format recording corpora, which routinely contain
hundreds to thousands of audio hours. Long-format corpora would benefit
greatly from (semi-)automated analyses, both on the earliest steps
of annotation &amp;#8212; voice activity detection, utterance segmentation,
and speaker diarization &amp;#8212; as well as later steps &amp;#8212; e.g.,
classification-based codes such as child-vs-adult-directed speech,
and speech recognition to produce phonetic/orthographic representations.
We present an annotation workflow specifically designed for long-format
corpora which can be tailored by individual researchers and which interfaces
with the current dominant scheme for short-format recordings. The workflow
allows semi-automated annotation and analyses at higher linguistic
levels. We give one example of how the workflow has been successfully
implemented in a large cross-database project.
</description>
    </item>
    
    <item>
        <title>Top-Down versus Bottom-Up Theories of Phonological Acquisition: A Big Data Approach</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1443.PDF</link>
        <description>Recent work has made available a number of standardized meta-analyses
bearing on various aspects of infant language processing. We utilize
data from two such meta-analyses (discrimination of vowel contrasts
and word segmentation, i.e., recognition of word forms extracted from
running speech) to assess whether the published body of empirical evidence
supports a bottom-up versus a top-down theory of early phonological
development by leveling the power of results from thousands of infants.
We predicted that if infants can rely purely on auditory experience
to develop their phonological categories, then vowel discrimination
and word segmentation should develop in parallel, with the latter being
potentially lagged compared to the former. However, if infants crucially
rely on word form information to build their phonological categories,
then development at the word level must precede the acquisition of
native sound categories. Our results do not support the latter prediction.
We discuss potential implications and limitations, most saliently that
word forms are only one top-down level proposed to affect phonological
development, with other proposals suggesting that top-down pressures
emerge from lexical (i.e., word-meaning pairs) development. This investigation
also highlights general procedures by which standardized meta-analyses
may be reused to answer theoretical questions spanning across phenomena.
</description>
    </item>
    
    <item>
        <title>Which Acoustic and Phonological Factors Shape Infants&amp;#8217; Vowel Discrimination? Exploiting Natural Variation in InPhonDB</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1468.PDF</link>
        <description>A key research question in early language acquisition concerns the
development of infants&amp;#8217; ability to discriminate sounds, and the
factors structuring discrimination abilities. Vowel discrimination,
in particular, has been studied using a range of tasks, experimental
paradigms, and stimuli over the past 40 years, work recently compiled
in a meta-analysis. We use this meta-analysis to assess whether there
is statistical evidence for the following factors affecting effect
sizes across studies: (1) the order in which the two vowel stimuli
are presented; and (2) the distance between the vowels, measured acoustically
in terms of spectral and quantity differences. The magnitude of effect
sizes analysis revealed order effects consistent with the Natural Referent
Vowels framework, with greater effect sizes when the second vowel was
more peripheral than the first. Additionally, we find that spectral
acoustic distinctiveness is a consistent predictor of studies&amp;#8217;
effect sizes, while temporal distinctiveness did not predict effect
size magnitude. None of these factors interacted significantly with
age. We discuss implications of these results for language acquisition,
and more generally developmental psychology, research.
</description>
    </item>
    
    <item>
        <title>The ABAIR Initiative: Bringing Spoken Irish into the Digital Space</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1407.PDF</link>
        <description>The processes of language demise take hold when a language ceases to
belong to the mainstream of life&amp;#8217;s activities. Digital communication
technology increasingly pervades all aspects of modern life. Languages
not digitally &amp;#8216;available&amp;#8217; are ever more marginalised, whereas
a digital presence often yields unexpected opportunities to integrate
the language into the mainstream. The ABAIR initiative embraces three
central aspects of speech technology development for Irish (Gaelic):
the provision of technology-oriented linguistic-phonetic resources;
the building and perfecting of core speech technologies; and the development
of technology applications, which exploit both the technologies and
the linguistic resources. The latter enable the public, learners, and
those with disabilities to integrate Irish into their day-to-day usage.
This paper outlines some of the specific linguistic and sociolinguistic
challenges and the approaches adopted to address them. Although machine-learning
approaches are helping to speed up the process of technology provision,
the ABAIR experience highlights how phonetic-linguistic resources are
also crucial to the development process. For the endangered language,
linguistic resources are central to many applications that impact on
language usage. The sociolinguistic context and the needs of potential
end users should be central considerations in setting research priorities
and deciding on methods.
</description>
    </item>
    
    <item>
        <title>Very Low Resource Radio Browsing for Agile Developmental and Humanitarian Monitoring</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0880.PDF</link>
        <description>We present a radio browsing system developed on a very small corpus
of annotated speech by using semi-supervised training of multilingual
DNN/HMM acoustic models. This system is intended to support relief
and developmental programmes by the United Nations (UN) in parts of
Africa where the spoken languages are extremely under resourced. We
assume the availability of 12 minutes of annotated speech in the target
language, and show how this can best be used to develop an acoustic
model. First, a multilingual DNN/HMM is trained using Acholi as the
target language and Luganda, Ugandan English and South African English
as source languages. We show that the lowest word error rates are achieved
by using this model to label further untranscribed target language
data and then developing SGMM acoustic model from the extended dataset.
The performance of an ASR system trained in this way is sufficient
for keyword detection that yields useful and actionable near real-time
information to developmental organisations.
</description>
    </item>
    
    <item>
        <title>Extracting Situation Frames from Non-English Speech: Evaluation Framework and Pilot Results</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0226.PDF</link>
        <description>This paper describes the first evaluation framework for the extraction
of Situation Frames &amp;#8212; structures describing humanitarian assistance
needs &amp;#8212; from non-English speech audio, conducted for the DARPA
LORELEI (Low Resource Languages for Emergent Incidents) program. Participants
in LORELEI had to process audio from a variety of sources, in non-English
languages, and extract the information required to populate Situation
Frames describing whether any need is mentioned, the type of need present
and where the need exists. The evaluation was conducted over a period
of 10 days and attracted submissions from 6 teams, each team spanning
multiple organizations. Performance was evaluated using precision-recall
curves. The results are encouraging, with most teams showing some capability
to detect the type of situation discussed, but more work will be required
to connect needs to specific locations.
</description>
    </item>
    
    <item>
        <title>Eliciting Meaningful Units from Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0855.PDF</link>
        <description>Elicitation of information structure from speech is a crucial step
in automatic speech understanding. In terms of both production and
perception, we consider intonational phrase to be the basic meaningful
unit of information structure in speech. The current paper presents
a method of detecting these units in speech by processing both the
recorded speech and its textual representation. Using syntactic information,
we split text into small groups of words closely connected with each
other. Assuming that intonational phrases are built from these small
groups, we use acoustic information to reveal their actual boundaries.
The procedure was initially developed for processing Russian speech,
and we have achieved the best published results for this language with
F1 equal to 0.91. We assume that it may be adapted for other languages
that have some amount of read speech resources, including under-resourced
languages. For comparison we have evaluated it on English material
(Boston University Radio Speech Corpus). Our results, F1 of 0.76, are
comparable with the top systems designed for English.
</description>
    </item>
    
    <item>
        <title>Unsupervised Speech Signal to Symbol Transformation for Zero Resource Speech Applications</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1476.PDF</link>
        <description>Zero resource speech processing refers to a scenario where no or minimal
transcribed data is available. In this paper, we propose a three-step
unsupervised approach to zero resource speech processing, which does
not require any other information/dataset. In the first step, we segment
the speech signal into phoneme-like units, resulting in a large number
of varying length segments. The second step involves clustering the
varying-length segments into a finite number of clusters so that each
segment can be labeled with a cluster index. The unsupervised transcriptions,
thus obtained, can be thought of as a sequence of virtual phone labels.
In the third step, a deep neural network classifier is trained to map
the feature vectors extracted from the signal to its corresponding
virtual phone label. The virtual phone posteriors extracted from the
DNN are used as features in the zero resource speech processing. The
effectiveness of the proposed approach is evaluated on both ABX and
spoken term discovery tasks (STD) using spontaneous American English
and Tsonga language datasets, provided as part of zero resource 2015
challenge. It is observed that the proposed system outperforms baselines,
supplied along the datasets, in both the tasks without any task specific
modifications.
</description>
    </item>
    
    <item>
        <title>Machine Assisted Analysis of Vowel Length Contrasts in Wolof</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0268.PDF</link>
        <description>Growing digital archives and improving algorithms for automatic analysis
of text and speech create new research opportunities for fundamental
research in phonetics. Such empirical approaches allow statistical
evaluation of a much larger set of hypothesis about phonetic variation
and its conditioning factors (among them geographical / dialectal variants).
This paper illustrates this vision and proposes to challenge automatic
methods for the analysis of a not easily observable phenomenon: vowel
length contrast. We focus on Wolof, an under-resourced language from
Sub-Saharan Africa. In particular, we propose multiple features to
make a fine evaluation of the degree of length contrast under different
factors such as: read  vs semi-spontaneous speech; standard  vs dialectal
Wolof. Our measures made fully automatically on more than 20k vowel
tokens show that our proposed features can highlight different degrees
of contrast for each vowel considered. We notably show that contrast
is weaker in semi-spontaneous speech and in a non standard semi-spontaneous
dialect.
</description>
    </item>
    
    <item>
        <title>Leveraging Text Data for Word Segmentation for Underresourced Languages</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1262.PDF</link>
        <description>In this contribution we show how to exploit text data to support word
discovery from audio input in an underresourced target language. Given
audio, of which a certain amount is transcribed at the word level,
and additional unrelated text data, the approach is able to learn a
probabilistic mapping from acoustic units to characters and utilize
it to segment the audio data into words without the need of a pronunciation
dictionary. This is achieved by three components: an unsupervised acoustic
unit discovery system, a supervisedly trained acoustic unit-to-grapheme
converter, and a word discovery system, which is initialized with a
language model trained on the text data. Experiments for multiple setups
show that the initialization of the language model with text data improves
the word segmentation performance by a large margin.
</description>
    </item>
    
    <item>
        <title>Improving DNN Bluetooth Narrowband Acoustic Models by Cross-Bandwidth and Cross-Lingual Initialization</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1129.PDF</link>
        <description>The success of deep neural network (DNN) acoustic models is partly
owed to large amounts of training data available for different applications.
This work investigates ways to improve DNN acoustic models for Bluetooth
narrowband mobile applications when relatively small amounts of in-domain
training data are available. To address the challenge of limited in-domain
data, we use cross-bandwidth and cross-lingual transfer learning methods
to leverage knowledge from other domains with more training data (different
bandwidth and/or languages). Specifically, narrowband DNNs in a target
language are initialized using the weights of DNNs trained on bandlimited
wide-band data in the same language or those trained on a different
(resource-rich) language. We investigate multiple recipes involving
such methods with different data resources. For all languages in our
experiments, these recipes achieve up to 45% relative WER reduction,
compared to training solely on the Bluetooth narrowband data in the
target language. Furthermore, these recipes are very beneficial even
when over two hundred hours of manually transcribed in-domain data
is available, and we can achieve better accuracy than the baselines
with as little as 20 hours of in-domain data.
</description>
    </item>
    
    <item>
        <title>Joint Estimation of Articulatory Features and Acoustic Models for Low-Resource Languages</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1028.PDF</link>
        <description>Using articulatory features for speech recognition improves the performance
of low-resource languages. One way to obtain articulatory features
is by using an articulatory classifier (pseudo-articulatory features).
The performance of the articulatory features depends on the efficacy
of this classifier. But, training such a robust classifier for a low-resource
language is constrained due to the limited amount of training data.
We can overcome this by training the articulatory classifier using
a high resource language. This classifier can then be used to generate
articulatory features for the low-resource language. However, this
technique fails when high and low-resource languages have mismatches
in their environmental conditions. In this paper, we address both the
aforementioned problems by jointly estimating the articulatory features
and low-resource acoustic model. The experiments were performed on
two low-resource Indian languages namely, Hindi and Tamil. English
was used as the high-resource language. A relative improvement of 23%
and 10% were obtained for Hindi and Tamil, respectively.
</description>
    </item>
    
    <item>
        <title>Transfer Learning and Distillation Techniques to Improve the Acoustic Modeling of Low Resource Languages</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1009.PDF</link>
        <description>Deep neural networks (DNN) require large amount of training data to
build robust acoustic models for speech recognition tasks. Our work
is intended in improving the low-resource language acoustic model to
reach a performance comparable to that of a high-resource scenario
with the help of data/model parameters from other high-resource languages.
We explore transfer learning and distillation methods, where a complex
high resource model guides or supervises the training of low resource
model. The techniques include (i) multi-lingual framework of borrowing
data from high-resource language while training the low-resource acoustic
model. The KL divergence based constraints are added to make the model
biased towards low-resource language, (ii) distilling knowledge from
the complex high-resource model to improve the low-resource acoustic
model. The experiments were performed on three Indian languages namely
Hindi, Tamil and Kannada. All the techniques gave improved performance
and the multi-lingual framework with KL divergence regularization giving
the best results. In all the three languages a performance close to
or better than high-resource scenario was obtained.
</description>
    </item>
    
    <item>
        <title>Building an ASR Corpus Using Althingi&amp;#8217;s Parliamentary Speeches</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0903.PDF</link>
        <description>Acoustic data acquisition for under-resourced languages is an important
and challenging task. In the Icelandic parliament, Althingi, all performed
speeches are transcribed manually and published as text on Althingi&amp;#8217;s
web page. To reduce the manual work involved, an automatic speech recognition
system is being developed for Althingi. In this paper the development
of a speech corpus suitable for the training of a parliamentary ASR
system is described. Text and audio data of manually transcribed speeches
were processed to build an aligned, segmented corpus, whereby language
specific tasks had to be developed specially for Icelandic. The resulting
corpus of 542 hours of speech is freely available on http://www.malfong.is.
First experiments with an ASR system trained on the Althingi corpus
have been conducted, showing promising results. Word error rate of
16.38% was obtained using time-delay deep neural network (TD-DNN) and
14.76% was obtained using long-short term memory recurrent neural network
(LSTM-RNN) architecture. The Althingi corpus is to our knowledge the
largest speech corpus currently available in Icelandic. The corpus
as well as the developed methods for corpus creation constitute a valuable
resource for further developments within Icelandic language technology.
</description>
    </item>
    
    <item>
        <title>Implementation of a Radiology Speech Recognition System for Estonian Using Open Source Software</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Building ASR Corpora Using Eyra</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1352.PDF</link>
        <description>Building acoustic databases for speech recognition is very important
for under-resourced languages. To build a speech recognition system,
a large amount of speech data from a considerable number of participants
needs to be collected. Eyra is a toolkit that can be used to gather
acoustic data from a large number of participants in a relatively straight
forward fashion. Predetermined prompts are downloaded onto a client,
typically run on a smartphone, where the participant reads them aloud
so that the recording and its corresponding prompt can be uploaded.
This paper presents the Eyra toolkit, its quality control routines
and annotation mechanism. The quality control relies on a forced-alignment
module, which gives feedback to the participant, and an annotation
module which allows data collectors to rate the read prompts after
they are uploaded to the system. The paper presents an analysis of
the performance of the quality control and describes two data collections
for Icelandic and Javanese.
</description>
    </item>
    
    <item>
        <title>Rapid Development of TTS Corpora for Four South African Languages</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1139.PDF</link>
        <description>This paper describes the development of text-to-speech corpora for
four South African languages. The approach followed investigated the
possibility of using low-cost methods including informal recording
environments and untrained volunteer speakers. This objective and the
additional future goal of expanding the corpus to increase coverage
of South Africa&amp;#8217;s 11 official languages necessitated experimenting
with multi-speaker and code-switched data. The process and relevant
observations are detailed throughout. The latest version of the corpora
are available for download under an open-source licence and will likely
see further development and refinement in future.
</description>
    </item>
    
    <item>
        <title>Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0037.PDF</link>
        <description>Acquiring data for text-to-speech (TTS) systems is expensive. This
typically requires large amounts of training data, which is not available
for low-resourced languages. Sometimes small amounts of data can be
collected, while often no data may be available at all. This paper
presents an acoustic modeling approach utilizing long short-term memory
(LSTM) recurrent neural networks (RNN) aimed at partially addressing
the language data scarcity problem. Unlike speaker-adaptation systems
that aim to preserve speaker similarity across languages, the salient
feature of the proposed approach is that, once constructed, the resulting
system does not need retraining to cope with the previously unseen
languages. This is due to language and speaker-agnostic model topology
and universal linguistic feature set. Experiments on twelve languages
show that the system is able to produce intelligible and sometimes
natural output when a language is unseen. We also show that, when small
amounts of training data are available, pooling the data sometimes
improves the overall intelligibility and naturalness. Finally, we show
that sometimes having a multilingual system with no prior exposure
to the language is better than building single-speaker system from
small amounts of data for that language.
</description>
    </item>
    
    <item>
        <title>Nativization of Foreign Names in TTS for Automatic Reading of World News in Swahili</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1398.PDF</link>
        <description>When a text-to-speech (TTS) system is required to speak world news,
a large fraction of the words to be spoken will be proper names originating
in a wide variety of languages. Phonetization of these names based
on target language letter-to-sound rules will typically be inadequate.
This is detrimental not only during synthesis, when inappropriate phone
sequences are produced, but also during training, if the system is
trained on data from the same domain. This is because poor phonetization
during forced alignment based on hidden Markov models can pollute the
whole model set, resulting in degraded alignment even of normal target-language
words. This paper presents four techniques designed to address this
issue in the context of a Swahili TTS system: automatic transcription
of proper names based on a lexicon from a better-resourced language;
the addition of a parallel phone set and special part-of-speech tag
exclusively dedicated to proper names; a manually-crafted phone mapping
which allows substitutions for potentially more accurate phones in
proper names during forced alignment; the addition in proper names
of a grapheme-derived frame-level feature, supplementing the standard
phonetic inputs to the acoustic model. We present results from objective
and subjective evaluations of systems built using these four techniques.
</description>
    </item>
    
    <item>
        <title>Multi-Task Learning for Mispronunciation Detection on Singapore Children&amp;#8217;s Mandarin Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0520.PDF</link>
        <description>Speech technology for children is more challenging than for adults,
because there is a lack of children&amp;#8217;s speech corpora. Moreover,
there is higher heterogeneity in children&amp;#8217;s speech due to variability
in anatomy across age and gender, larger variance in speaking rate
and vocal effort, and immature command of word usage, grammar, and
linguistic structure. Speech productions from Singapore children possess
even more variability due to the multilingual environment in the city-state,
causing inter-influences from Chinese languages (e.g., Hokkien and
Mandarin), English dialects (e.g., American and British), and Indian
languages (e.g., Hindi and Tamil). In this paper, we show that acoustic
modeling of children&amp;#8217;s speech can leverage on a larger set of
adult data. We compare two data augmentation approaches for children&amp;#8217;s
acoustic modeling. The first approach disregards the child and adult
categories and consolidates the two datasets together as one entire
set. The second approach is multi-task learning: during training the
acoustic characteristics of adults and children are jointly learned
through shared hidden layers of the deep neural network, yet they still
retain their respective targets using two distinct softmax layers.
We empirically show that the multi-task learning approach outperforms
the baseline in both speech recognition and computer-assisted pronunciation
training.
</description>
    </item>
    
    <item>
        <title>Relating Unsupervised Word Segmentation to Reported Vocabulary Acquisition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0937.PDF</link>
        <description>A range of computational approaches have been used to model the discovery
of word forms from continuous speech by infants. Typically, these algorithms
are evaluated with respect to the ideal &amp;#8216;gold standard&amp;#8217;
word segmentation and lexicon. These metrics assess how well an algorithm
matches the adult state, but may not reflect the intermediate states
of the child&amp;#8217;s lexical development. We set up a new evaluation
method based on the correlation between word frequency counts derived
from the application of an algorithm onto a corpus of child-directed
speech, and the proportion of infants knowing those words, according
to parental reports. We evaluate a representative set of 4 algorithms,
applied to transcriptions of the Brent corpus, which have been phonologized
using either phonemes or syllables as basic units. Results show remarkable
variation in the extent to which these 8 algorithm-unit combinations
predicted infant vocabulary, with some of these predictions surpassing
those derived from the adult gold standard segmentation. We argue that
infant vocabulary prediction provides a useful complement to traditional
evaluation; for example, the best predictor model was also one of the
worst in terms of segmentation score, and there was no clear relationship
between token or boundary F-score and vocabulary prediction.
</description>
    </item>
    
    <item>
        <title>Modelling the Informativeness of Non-Verbal Cues in Parent-Child Interaction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1143.PDF</link>
        <description>Non-verbal cues from speakers, such as eye gaze and hand positions,
play an important role in word learning [1]. This is consistent with
the notion that for meaning to be reconstructed, acoustic patterns
need to be linked to time-synchronous patterns from at least one other
modality [2]. In previous studies of a multimodally annotated corpus
of parent-child interaction, we have shown that parents interacting
with infants at the early word-learning stage (7&amp;#8211;9 months) display
a large amount of time-synchronous patterns, but that this behaviour
tails off with increasing age of the children [3]. Furthermore, we
have attempted to quantify the  informativeness of the different non-verbal
cues, that is, to what extent they actually help to discriminate between
different possible referents, and how critical the timing of the cues
is [4]. The purpose of this paper is to generalise our earlier model
by quantifying informativeness resulting from non-verbal cues occurring
both before and after their associated verbal references.
</description>
    </item>
    
    <item>
        <title>Computational Simulations of Temporal Vocalization Behavior in Adult-Child Interaction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1289.PDF</link>
        <description>The purpose of the present study was to introduce a computational simulation
of timing in child-adult interaction. The simulation uses temporal
information from real adult-child interactions as default temporal
behavior of two simulated agents. Dependencies between the agents&amp;#8217;
behavior are added, and how the simulated interactions compare to real
interaction data as a result is investigated. In the present study,
the real data consisted of transcriptions of a mother interacting with
her 12-month-old child, and the data simulated was vocalizations. The
first experiment shows that although the two agents generate vocalizations
according to the temporal characteristics of the interlocutors in the
real data, simulated interaction with no contingencies between the
two agents&amp;#8217; behavior differs from real interaction data. In the
second experiment, a contingency was introduced to the simulation:
the likelihood that the adult agent initiated a vocalization if the
child agent was already vocalizing. Overall, the simulated data is
more similar to the real interaction data when the adult agent is less
likely to start speaking while the child agent vocalizes. The results
are in line with previous studies on turn-taking in parent-child interaction
at comparable ages. This illustrates that computational simulations
are useful tools when investigating parent-child interactions.
</description>
    </item>
    
    <item>
        <title>Approximating Phonotactic Input in Children&#8217;s Linguistic Environments from Orthographic Transcripts</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Learning Weakly Supervised Multimodal Phoneme Embeddings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1689.PDF</link>
        <description>Recent works have explored deep architectures for learning multimodal
speech representation (e.g. audio and images, articulation and audio)
in a supervised way. Here we investigate the role of combining different
speech modalities, i.e. audio and visual information representing the
lips&amp;#8217; movements, in a weakly supervised way using Siamese networks
and lexical same-different side information. In particular, we ask
whether one modality can benefit from the other to provide a richer
representation for phone recognition in a weakly supervised setting.
We introduce mono-task and multi-task methods for merging speech and
visual modalities for phone recognition. The mono-task learning consists
in applying a Siamese network on the concatenation of the two modalities,
while the multi-task learning receives several different combinations
of modalities at train time. We show that multi-task learning enhances
discriminability for visual and multimodal inputs while minimally impacting
auditory inputs. Furthermore, we present a qualitative analysis of
the obtained phone embeddings, and show that cross-modal visual input
can improve the discriminability of phonological features which are
visually discernable (rounding, open/close, labial place of articulation),
resulting in representations that are closer to abstract linguistic
features than those based on audio only.
</description>
    </item>
    
    <item>
        <title>Personalized Quantification of Voice Attractiveness in Multidimensional Merit Space</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>The Role of Temporal Amplitude Modulations in the Political Arena: Hillary Clinton vs. Donald Trump</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Perceptual Ratings of Voice Likability Collected Through In-Lab Listening Tests vs. Mobile-Based Crowdsourcing</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0326.PDF</link>
        <description>Human perceptions of speaker characteristics, needed to perform automatic
predictions from speech features, have generally been collected by
conducting demanding in-lab listening tests under controlled conditions.
Concurrently, crowdsourcing has emerged as a valuable approach for
running user studies through surveys or quantitative ratings. Micro-task
crowdsourcing markets enable the completion of small tasks (commonly
of minutes or seconds), rewarding users with micro-payments. This paradigm
permits effortless collection of user input from a large and diverse
pool of participants at low cost. This paper presents different auditory
tests for collecting perceptual voice likability ratings employing
a common set of 30 male and female voices. These tests are based on
direct scaling and on paired-comparisons, and were conducted in the
laboratory and via crowdsourcing using micro-tasks. Design considerations
are proposed for adapting the laboratory listening tests to a mobile-based
crowdsourcing platform to obtain trustworthy listeners&amp;#8217; answers.
Our likability scores obtained by the different test approaches are
highly correlated. This outcome motivates the use of crowdsourcing
for future listening tests investigating e.g. speaker characterization,
reducing the efforts involved in engaging participants and administering
the tests on-site.
</description>
    </item>
    
    <item>
        <title>Attractiveness of French Voices for German Listeners &amp;#8212; Results from Native and Non-Native Read Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0367.PDF</link>
        <description>This study investigated how the perceived attractiveness of voices
was influenced by a foreign language, a foreign accent, and the level
of fluency in the foreign language. Stimuli were taken from a French-German
corpus of read speech with German native speakers as raters. Additional
factors were stimulus length (syllable or entire sentence) and sex
(of the raters and speakers). Results with German native raters reveal
that stimuli spanning just a syllable were judged significantly less
attractive than those containing a sentence, and that stimuli from
French speakers were assessed as more attractive than those of German
speakers. This backs the clich&amp;#233; that French has an attractive
image for German listeners. An analysis of the best vs. the worst rated
sentences suggest that an individual mix of voice quality, disfluency
management, prosodic behaviour and pronunciation precision is responsible
for the results.
</description>
    </item>
    
    <item>
        <title>Social Attractiveness in Dialogs</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>A Gender Bias in the Acoustic-Melodic Features of Charismatic Speech?</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1349.PDF</link>
        <description>Previous studies proved the immense importance of nonverbal skills
when it comes to being persuasive and coming across as charismatic.
It was also found that men sound more convincing and persuasive (i.e.
altogether more charismatic) than women under otherwise comparable
conditions. This gender bias is investigated in the present study by
analyzing and comparing acoustic-melodic charisma features of male
and female business executives. In line with the gender bias in perception,
our results show that female CEOs who are judged to be similarly charismatic
as their male counterpart(s) produce more and stronger acoustic charisma
cues. This suggests that there is a gender bias which is compensated
for by making a greater effort on the part of the female speakers.
</description>
    </item>
    
    <item>
        <title>Pitch Convergence as an Effect of Perceived Attractiveness and Likability</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1520.PDF</link>
        <description>While there is a growing body of research on which and how pitch features
are perceived as attractive or likable, there are few studies investigating
how the impression of a speaker as attractive or likable affects the
speech behavior of his/her interlocutor. Recent studies have shown
that perceived attractiveness and likability may not only have an effect
on a speaker&amp;#8217;s pitch features in isolation but also on the prosodic
entrainment. It has been shown that how speakers synchronize their
pitch features relatively to their interlocutor is affected by such
impressions. This study investigates pitch convergence, examining whether
speakers become more similar over the course of a conversation depending
on perceived attractiveness and/or likability. The expected pitch convergence
is thereby investigated on two levels, over the entire conversation
(globally) as well as turn-wise (locally). The results from a speed
dating experiment with 98 mixed-sex dialogues of heterosexual singles
show that speakers become more similar globally and locally over time
both in register and range. Furthermore, the degree of pitch convergence
is greatly affected by perceived attractiveness and likability with
effects differing between attractiveness and likability as well as
between the global and the local level.
</description>
    </item>
    
    <item>
        <title>Does Posh English Sound Attractive?</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1691.PDF</link>
        <description>Poshness refers to how much a British English speaker sounds upper
class when they talk. Popular descriptions of posh English mostly focus
on vocabulary, accent and phonology. This study tests the hypothesis
that, as a social index, poshness is also manifested via phonetic properties
known to encode vocal attractiveness. Specifically, posh English, because
of its impression of being detached, authoritative and condescending,
would more closely resemble an attractive male voice than an attractive
female voice. In four experiments, we tested this hypothesis by acoustically
manipulating Cambridge-accented English utterances by a male and a
female speaker through PSOLA resynthesis, and having native speakers
of British English judge how posh or attractive each utterance sounds.
The manipulated acoustic dimensions are formant dispersion, pitch shift
and speech rate. Initial results from the first two experiments showed
a trend in the hypothesized direction for the male speakers&amp;#8217;
utterances. But for the female utterances there was a ceiling effect
due to the frequent alternation of speaker gender within the same test
session. When the two speakers&amp;#8217; utterances were separated by
blocks in the third and fourth experiments, a clearer support for the
main hypothesis was found.
</description>
    </item>
    
    <item>
        <title>Large-Scale Speaker Ranking from Crowdsourced Pairwise Listener Ratings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1697.PDF</link>
        <description>Speech quality and likability is a multi-faceted phenomenon consisting
of a combination of perceptory features that cannot easily be computed
nor weighed automatically. Yet, it is often easy to decide which of
two voices one likes better, even though it would be hard to describe
why, or to name the underlying basic perceptory features. Although
likability is inherently subjective and individual preferences differ
frequently, generalizations are useful and there is often a broad intersubjective
consensus about whether one speaker is more likable than another. However,
breaking down likability rankings into pairwise comparisons leads to
a quadratic explosion of rating pairs. We present a methodology and
software to efficiently create a likability ranking for many speakers
from crowdsourced pairwise likability ratings. We collected pairwise
likability ratings for many (&amp;#62;220) speakers from many raters (&amp;#62;160)
and turn these ratings into one likability ranking. We investigate
the resulting speaker ranking stability under different conditions:
limiting the number of ratings and the dependence on rater and speaker
characteristics. We also analyze the ranking wrt. acoustic correlates
to find out what factors influence likability. We publish our ranking
and the underlying ratings in order to facilitate further research.
</description>
    </item>
    
    <item>
        <title>Aerodynamic Features of French Fricatives</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0285.PDF</link>
        <description>The present research investigates the aerodynamic features of French
fricative consonants using direct measurement of subglottal air pressure
by tracheal puncture (Ps) synchronized with intraoral air pressure
(Po), oral airflow (Oaf) and acoustic measurements. Data were collected
from four Belgian French speakers&amp;#8217; productions of CVCV pseudowords
including voiceless and voiced fricatives [f, v, s, z, &amp;#643;, &amp;#658;].
The goals of this study are: (i) to predict the starting, central,
and releasing points of frication based on the measurements of Ps,
Po, and Oaf; (ii) to compare voiceless and voiced fricatives and their
places of articulation; and (iii) to provide reference values for the
aerodynamic features of fricatives for further linguistic, clinical,
physical and computational modeling research.
</description>
    </item>
    
    <item>
        <title>Inter-Speaker Variability: Speaker Normalisation and Quantitative Estimation of Articulatory Invariants in Speech Production for French</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1126.PDF</link>
        <description>Speech production can be analysed in terms of universal articulatory-acoustic
phonemic units shared between speakers. However, morphological differences
between speakers and idiosyncratic articulatory strategies lead to
large inter-speaker articulatory variability. Relationships between
strategy and morphology have already been pinpointed in the literature.
This study aims thus at generalising existing results on a larger database
for the entire vocal tract (VT) and at quantifying phoneme-specific
inter-speaker articulatory invariants. Midsagittal MRI of 11 French
speakers for 62 vowels and consonants were recorded and VT contours
manually edited. A procedure of normalisation of VT contours between
speakers, based on the use of mean VT contours, led to an overall reduction
of inter-speaker VT contours variance of 88%. On the opposite, the
sagittal function (i.e. the transverse sagittal distance along the
VT midline), which is the main determinant of the acoustic output,
had an overall amplitude variance decrease of only 37%, suggesting
that the speakers adapt their strategy to their morphology to achieve
proper acoustic goals. Moreover, articulatory invariants were identified
on the sagittal variance distribution along the VT as the regions with
lower variability. These regions correspond to the classical places
of articulation and are associated with higher acoustic sensitivity
function levels.
</description>
    </item>
    
    <item>
        <title>Comparison of Basic Beatboxing Articulations Between Expert and Novice Artists Using Real-Time Magnetic Resonance Imaging</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1190.PDF</link>
        <description>Real-time Magnetic Resonance Imaging (rtMRI) was used to examine mechanisms
of sound production in five beatboxers. rtMRI was found to be an effective
tool with which to study the articulatory dynamics of this form of
human vocal production; it provides a dynamic view of the entire midsagittal
vocal tract and at a frame rate (83 fps) sufficient to observe the
movement and coordination of critical articulators. The artists&amp;#8217;
repertoires included percussion elements generated using a wide range
of articulatory and airstream mechanisms. Analysis of three common
beatboxing sounds resulted in the finding that advanced beatboxers
produce stronger ejectives and have greater control over different
airstreams than novice beatboxers, to enhance the quality of their
sounds. No difference in production mechanisms between males and females
was observed. These data offer insights into the ways in which articulators
can be trained and used to achieve specific acoustic goals.
</description>
    </item>
    
    <item>
        <title>Speaker-Specific Biomechanical Model-Based Investigation of a Simple Speech Task Based on Tagged-MRI</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1576.PDF</link>
        <description>We create two 3D biomechanical speaker models matched to medical image
data of two healthy English speakers. We use a new, hybrid registration
technique that morphs a generic 3D, biomechanical model to medical
images. The generic model of the head and neck includes jaw, tongue,
soft-palate, epiglottis, lips and face, and is capable of simulating
upper-airway biomechanics. We use cine and tagged magnetic resonance
(MR) images captured while our volunteers repeated a simple utterance
(/&amp;#x259;-gis/) synchronized to a metronome. We simulate our models
based on internal tongue tissue trajectories that we extract from tagged
MR images, and use in an inverse solver. For areas without tracked
data points, the registered generic model moves based on the computed
muscle activations. Our modeling efforts include a wide range of speech
organs illustrating the coupling complexity between the oral anatomy
during simple speech utterances.
</description>
    </item>
    
    <item>
        <title>Sounds of the Human Vocal Tract</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>A Simulation Study on the Effect of Glottal Boundary Conditions on Vocal Tract Formants</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1675.PDF</link>
        <description>In the source-filter theory, the complete closure of the glottis is
assumed as a glottal boundary condition. However, such assumption of
glottal closure in the source-filter theory is not strictly satisfied
in actual utterance. Therefore, it is considered that acoustic features
of the glottis and the subglottal region may affect vocal tract formants.
In this study, we investigated how differences in the glottal boundary
conditions affect vocal tract formants by speech synthesis simulation
using speech production model. We synthesized five Japanese vowels
using the speech production model in consideration of the source-filter
interaction. This model consisted of the glottal area polynomial model
and the acoustic tube model in the concatenation of the vocal tract,
glottis, and the subglottis. From the results, it was found that the
first formant frequency was affected more strongly by the boundary
conditions, and also found that the open quotient may give the formant
stronger effect than the maximum glottal width. In addition, formant
frequencies were also affected more strongly by subglottal impedance
when the maximum glottal area was wider.
</description>
    </item>
    
    <item>
        <title>A Robust and Alternative Approach to Zero Frequency Filtering Method for Epoch Extraction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Improving YANGsaf F0 Estimator with Adaptive Kalman Filter</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0021.PDF</link>
        <description>We present improvements to the refinement stage of YANGsaf[1] (Yet
ANother Glottal source analysis framework), a recently published F0
estimation algorithm by Kawahara  et al., for noisy/breathy speech
signals. The baseline system, based on time-warping and weighted average
of multi-band instantaneous frequency estimates, is still sensitive
to additive noise when none of the harmonic provide reliable frequency
estimate at low SNR. We alleviate this problem by calibrating the weighted
averaging process based on statistics gathered from a Monte-Carlo simulation,
and applying Kalman filtering to refined F0 trajectory with time-varying
measurement and process distributions. The improved algorithm, adYANGsaf
(adaptive Yet ANother Glottal source analysis framework), achieves
significantly higher accuracy and smoother F0 trajectory on noisy speech
while retaining its accuracy on clean speech, with little computational
overhead introduced.
</description>
    </item>
    
    <item>
        <title>A Spectro-Temporal Demodulation Technique for Pitch Estimation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1138.PDF</link>
        <description>We consider a two-dimensional demodulation framework for spectro-temporal
analysis of the speech signal. We construct narrowband (NB) speech
spectrograms, and demodulate them using the Riesz transform, which
is a two-dimensional extension of the Hilbert transform. The demodulation
results in time-frequency envelope (amplitude modulation or AM) and
time-frequency carrier (frequency modulation or FM). The AM corresponds
to the vocal tract and is referred to as the vocal tract spectrogram.
The FM corresponds to the underlying excitation and is referred to
as the carrier spectrogram. The carrier spectrogram exhibits a high
degree of time-frequency consistency for voiced sounds. For unvoiced
sounds, such a structure is lacking. In addition, the carrier spectrogram
reflects the fundamental frequency (F0) variation of the speech signal.
We develop a technique to determine the F0 from the carrier spectrogram.
The time-frequency consistency is used to determine which time-frequency
regions correspond to voiced segments. Comparisons with the state-of-the-art
F0 estimation algorithms show that the proposed F0 estimator has high
accuracy for telephone channel speech and is robust to noise.
</description>
    </item>
    
    <item>
        <title>Robust Method for Estimating F<SUB>0</SUB> of Complex Tone Based on Pitch Perception of Amplitude Modulated Signal</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Low-Complexity Pitch Estimation Based on Phase Differences Between Low-Resolution Spectra</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Harvest: A High-Performance Fundamental Frequency Estimator from Speech Signals</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0068.PDF</link>
        <description>A fundamental frequency (F0) estimator named  Harvest is described.
The unique points of Harvest are that it can obtain a reliable F0 contour
and reduce the error that the voiced section is wrongly identified
as the unvoiced section. It consists of two steps: estimation of F0
candidates and generation of a reliable F0 contour on the basis of
these candidates. In the first step, the algorithm uses fundamental
component extraction by many band-pass filters with different center
frequencies and obtains the basic F0 candidates from filtered signals.
After that, basic F0 candidates are refined and scored by using the
instantaneous frequency, and then several F0 candidates in each frame
are estimated. Since the frame-by-frame processing based on the fundamental
component extraction is not robust against temporally local noise,
a connection algorithm using neighboring F0s is used in the second
step. The connection takes advantage of the fact that the F0 contour
does not precipitously change in a short interval. We carried out an
evaluation using two speech databases with electroglottograph (EGG)
signals to compare Harvest with several state-of-the-art algorithms.
Results showed that Harvest achieved the best performance of all algorithms.
</description>
    </item>
    
    <item>
        <title>Prosodic Event Recognition Using Convolutional Neural Networks with Context Information</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1159.PDF</link>
        <description>This paper demonstrates the potential of convolutional neural networks
(CNN) for detecting and classifying prosodic events on words, specifically
pitch accents and phrase boundary tones, from frame-based acoustic
features. Typical approaches use not only feature representations of
the word in question but also its surrounding context. We show that
adding position features indicating the current word benefits the CNN.
In addition, this paper discusses the generalization from a speaker-dependent
modelling approach to a speaker-independent setup. The proposed method
is simple and efficient and yields strong results not only in speaker-dependent
but also speaker-independent cases.
</description>
    </item>
    
    <item>
        <title>Prosodic Facilitation and Interference While Judging on the Veracity of Synthesized Statements</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0453.PDF</link>
        <description>Two primary sources of information are provided in human speech. On
the one hand, the verbal channel encodes linguistic content, while
on the other hand, the vocal channel transmits paralinguistic information,
mainly through prosody. In line with several studies that induce a
conflict between these two channels to better understand the role of
prosody, we conducted an experiment in which subjects had to listen
to a series of statements synthesized with varying prosody and indicate
if they believed them to be true or false. We find evidence suggesting
that acoustic/prosodic (a/p) features of the synthesized statements
affect response times (a well-known proxy for cognitive load). Our
results suggest that prosody in synthesized speech may play a role
of either facilitation or interference when subjects judge the truthfulness
of a statement. Furthermore, we find that this pattern is amplified
when the a/p features of the synthesized statements are analyzed relative
to the subjects&amp;#8217; own a/p features. This suggests that the entrainment
of TTS voices has serious implications in the perceived trustworthiness
of the system&amp;#8217;s skills.
</description>
    </item>
    
    <item>
        <title>An Investigation of Pitch Matching Across Adjacent Turns in a Corpus of Spontaneous German</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0811.PDF</link>
        <description>Speakers in conversations may adapt their turn pitch relative to that
of preceding turns to signal alignment with their interlocutor. However,
the reference frame for pitch matching across turns is still unclear.
Researchers studying pitch in the context of conversation have argued
for an initializing approach, in which turn pitch must be judged relative
to pitch in preceding turns. However, perceptual studies have indicated
that listeners are able to reliably identify the location of pitch
values within an individual speaker&amp;#8217;s range; that is, even without
conversational context, they are able to normalize to speakers. This
would imply that speakers might match normalized pitch instead of absolute
pitch. Using a combined quantitative-qualitative approach, we investigate
the relationship between pitch in adjacent turns in spontaneous German
conversation. We use two different methods of evaluating pitch in adjacent
turns, reflecting normalizing and initializing approaches respectively.
We find that the results are well correlated with conversational participants&amp;#8217;
evaluation of the conversation. Furthermore, evaluating locations with
matched or mismatched pitch can help distinguish between blind and
face-to-face conversational situations, as well as identifying locations
where specific discourse strategies (such as tag questions) have been
deployed.
</description>
    </item>
    
    <item>
        <title>The Relationship Between F0 Synchrony and Speech Convergence in Dyadic Interaction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0795.PDF</link>
        <description>Speech accommodation happens when two people engage in verbal conversation.
In this paper two types of accommodation are investigated &amp;#8212; one
dependent on cognitive, physiological, functional and social constraints
(Convergence), the other dependent on linguistic and paralinguistic
factors (Synchrony). Convergence refers to the situation when two speakers&amp;#8217;
speech characteristics move towards a common point. Synchrony happens
if speakers&amp;#8217; prosodic features become correlated over time. Here
we analyze relations between the two phenomena at the single word level.
Although calculation of Synchrony is fairly straightforward, measuring
Convergence is even more problematic as proved by a long history of
debates on how to define it. In this paper we consider Convergence
as an emergent behavior and investigate it by developing a robust and
automatic method based on Gaussian Mixture Model (GMM). Our results
show that high Synchrony of F0 between two speakers leads to greater
amount of Convergence. This provides robust support for the idea that
Synchrony and Convergence are interrelated processes, particularly
in female participants.
</description>
    </item>
    
    <item>
        <title>The Role of Linguistic and Prosodic Cues on the Prediction of Self-Reported Satisfaction in Contact Centre Phone Calls</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0424.PDF</link>
        <description>Call Centre data is typically collected by organizations and corporations
in order to ensure the quality of service, supporting for example mining
capabilities for monitoring customer satisfaction. In this work, we
analyze the significance of various acoustic features extracted from
customer-agents&amp;#8217; spoken interaction in predicting self-reported
satisfaction by the customer. We also investigate whether speech prosodic
features can deliver complementary information to speech transcriptions
provided by an ASR. We explore the possibility of using a deep neural
architecture to perform early feature fusion on both prosodic and linguistic
information. Convolutional Neural Networks are trained on a combination
of word embedding and acoustic features for the binary classification
task of &amp;#8220;low&amp;#8221; and &amp;#8220;high&amp;#8221; satisfaction prediction.
We conducted our experiments analysing real call-centre interactions
of a large corporation in a Spanish spoken country. Our experiments
show that linguistic features can predict self-reported satisfaction
more accurately than those based on prosodic and conversational descriptors.
We also find that dialog turn-level conversational features generally
outperforms frame-level signal descriptors. Finally, the fusion of
linguistic and prosodic features reports the best performance in our
experiments, suggesting the complementarity of the information conveyed
by each set of behavioral representation.
</description>
    </item>
    
    <item>
        <title>Cross-Linguistic Study of the Production of Turn-Taking Cues in American English and Argentine Spanish</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0124.PDF</link>
        <description>We present the results of a series of machine learning experiments
aimed at exploring the differences and similarities in the production
of turn-taking cues in American English and Argentine Spanish. An analysis
of prosodic features automatically extracted from 21 dyadic conversations
(12 En, 9 Sp) revealed that, when signaling Holds, speakers of both
languages tend to use roughly the same combination of cues, characterized
by a sustained final intonation, a shorter duration of turn-final inter-pausal
units, and a distinct voice quality. However, in speech preceding Smooth
Switches or Backchannels, we observe the existence of the same set
of prosodic turn-taking cues in both languages, although the ways in
which these cues are combined together to form complex signals differ.
Still, we find that these differences do not degrade below chance the
performance of cross-linguistic systems for automatically detecting
turn-taking signals. These results are relevant to the construction
of multilingual spoken dialogue systems, which need to adapt not only
their ASR modules but also the way prosodic turn-taking cues are synthesized
and recognized.
</description>
    </item>
    
    <item>
        <title>Emotional Features for Speech Overlaps Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0087.PDF</link>
        <description>One interesting phenomenon of natural conversation is overlapping speech.
Besides causing difficulties in automatic speech processing, such overlaps
carry information on the state of the overlapper: competitive overlaps
(i.e. &amp;#8220;interruptions&amp;#8221;) can signal disagreement or the feeling
of being overlooked, and cooperative overlaps (i.e. supportive interjections)
can signal agreement and interest. These hints can be used to improve
human-machine interaction. In this paper we present an approach for
automatic classification of competitive and cooperative overlaps using
the emotional content of the speakers&amp;#8217; utterances before and
after the overlap. For these experiments, we use real-world data from
human-human interactions in call centres. We also compare our approach
to standard acoustic classification on the same data and come to the
conclusion, that emotional features are clearly superior to acoustic
features for this task, resulting in an unweighted average f-measure
of 71.9%. But we also find that acoustic features should not be entirely
neglected: using a late fusion procedure, we can further improve the
unweighted average f-measure by 2.6%.
</description>
    </item>
    
    <item>
        <title>Computing Multimodal Dyadic Behaviors During Spontaneous Diagnosis Interviews Toward Automatic Categorization of Autism Spectrum Disorder</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0563.PDF</link>
        <description>Autism spectrum disorder (ASD) is a highly-prevalent neural developmental
disorder often characterized by social communicative deficits and restricted
repetitive interest. The heterogeneous nature of ASD in its behavior
manifestations encompasses broad syndromes such as, Classical Autism
(AD), High-functioning Autism (HFA), and Asperger syndrome (AS). In
this work, we compute a variety of multimodal behavior features, including
body movements, acoustic characteristics, and turn-taking events dynamics,
of the participant, the investigator and the interaction between the
two directly from audio-video recordings by leveraging the Autism Diagnostic
Observational Schedule (ADOS) as a clinically-valid behavior data elicitation
technique. Several of these signal-derived behavioral measures show
statistically significant differences among the three syndromes. Our
analyses indicate that these features may be pointing to the underlying
differences in the behavior characterizations of social functioning
between AD, AS, and HFA &amp;#8212; corroborating some of the previous
literature. Further, our signal-derived behavior measures achieve competitive,
sometimes exceeding, recognition accuracies in discriminating between
the three syndromes of ASD when compared to investigator&amp;#8217;s clinical-rating
on participant&amp;#8217;s social and communicative behaviors during ADOS.
</description>
    </item>
    
    <item>
        <title>Deriving  Dyad-Level Interaction Representation Using Interlocutors Structural and Expressive Multimodal Behavior Features</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0569.PDF</link>
        <description>The overall interaction atmosphere is often a result of complex interplay
between individual interlocutor&amp;#8217;s behavior expressions and joint
manifestation of dyadic interaction dynamics. There is very limited
work, if any, that has computationally analyzed a human interaction
at the dyad-level. Hence, in this work, we propose to compute an extensive
novel set of features representing multi-faceted aspects of a dyadic
interaction. These features are grouped into two broad categories:
 expressive and  structural behavior dynamics, where each captures
information about within-speaker behavior manifestation, inter-speaker
behavior dynamics, durational and transitional statistics providing
holistic behavior quantifications at the dyad-level. We carry out an
experiment of recognizing targeted  affective atmosphere using the
proposed  expressive and  structural behavior dynamics features derived
from audio and video modalities. Our experiment shows that the inclusion
of both  expressive and  structural behavior dynamics is essential
in achieving promising recognition accuracies across six different
classes (72.5%), where  structural-based features improve the recognition
rates on classes of  sad and  surprise. Further analyses reveal important
aspects of multimodal behavior dynamics within dyadic interactions
that are related to the affective atmospheric scene.
</description>
    </item>
    
    <item>
        <title>Spotting Social Signals in Conversational Speech over IP: A Deep Learning Perspective</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0635.PDF</link>
        <description>The automatic detection and classification of social signals is an
important task, given the fundamental role nonverbal behavioral cues
play in human communication. We present the first cross-lingual study
on the detection of  laughter and  fillers in conversational and spontaneous
speech collected &amp;#8216;in the wild&amp;#8217; over IP (internet protocol).
Further, this is the first comparison of LSTM and GRU networks to shed
light on their performance differences. We report frame-based results
in terms of the unweighted-average area-under-the-curve (UAAUC) measure
and will shortly discuss its suitability for this task. In the mono-lingual
setup our best deep BLSTM system achieves 87.0% and 86.3% UAAUC for
English and German, respectively. Interestingly, the cross-lingual
results are only slightly lower, yielding 83.7% for a system trained
on English, but tested on German, and 85.0% in the opposite case. We
show that LSTM and GRU architectures are valid alternatives for e.
g., on-line and compute-sensitive applications, since their application
incurs a relative UAAUC decrease of only approximately 5% with respect
to our best systems. Finally, we apply additional smoothing to correct
for erroneous spikes and drops in the posterior trajectories to obtain
an additional gain in all setups.
</description>
    </item>
    
    <item>
        <title>Optimized Time Series Filters for Detecting Laughter and Filler Events</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0932.PDF</link>
        <description>Social signal detection, that is, the task of identifying vocalizations
like laughter and filler events is a popular task within computational
paralinguistics. Recent studies have shown that besides applying state-of-the-art
machine learning methods, it is worth making use of the contextual
information and adjusting the frame-level scores based on the local
neighbourhood. In this study we apply a weighted average time series
smoothing filter for laughter and filler event identification, and
set the weights using a state-of-the-art optimization method, namely
the Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Our results
indicate that this is a viable way of improving the Area Under the
Curve (AUC) scores: our resulting scores are much better than the accuracy
scores of the raw likelihoods produced by Deep Neural Networks trained
on three different feature sets, and we also significantly outperform
standard time series filters as well as DNNs used for smoothing. Our
score achieved on the test set of a public English database containing
spontaneous mobile phone conversations is the highest one published
so far that was realized by feed-forward techniques.
</description>
    </item>
    
    <item>
        <title>Visual, Laughter, Applause and Spoken Expression Features for Predicting Engagement Within TED Talks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1633.PDF</link>
        <description>There is an enormous amount of audio-visual content available on-line
in the form of talks and presentations. The prospective users of the
content face difficulties in finding the right content for them. However,
automatic detection of interesting (engaging vs. non-engaging) content
can help users to find the videos according to their preferences. It
can also be helpful for a recommendation and personalised video segmentation
system. This paper presents a study of engagement based on TED talks
(1338 videos) which are rated by on-line viewers (users). It proposes
novel models to predict the user&amp;#8217;s (on-line viewers) engagement
using high-level visual features (camera angles), the audience&amp;#8217;s
laughter and applause, and the presenter&amp;#8217;s speech expressions.
The results show that these features contribute towards the prediction
of user engagement in these talks. However, finding the engaging speech
expressions can also help a system in making summaries of TED Talks
(video summarization) and creating feedback to presenters about their
speech expressions during talks.
</description>
    </item>
    
    <item>
        <title>Large-Scale Domain Adaptation via Teacher-Student Learning</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0519.PDF</link>
        <description>High accuracy speech recognition requires a large amount of transcribed
data for supervised training. In the absence of such data, domain adaptation
of a well-trained acoustic model can be performed, but even here, high
accuracy usually requires significant labeled data from the target
domain. In this work, we propose an approach to domain adaptation that
does not require transcriptions but instead uses a corpus of unlabeled
parallel data, consisting of pairs of samples from the source domain
of the well-trained model and the desired target domain. To perform
adaptation, we employ teacher/student (T/S) learning, in which the
posterior probabilities generated by the source-domain model can be
used in lieu of labels to train the target-domain model. We evaluate
the proposed approach in two scenarios, adapting a clean acoustic model
to noisy speech and adapting an adults&amp;#8217; speech acoustic model
to children&amp;#8217;s speech. Significant improvements in accuracy are
obtained, with reductions in word error rate of up to 44% over the
original source model without the need for transcribed data in the
target domain. Moreover, we show that increasing the amount of unlabeled
data results in additional model robustness, which is particularly
beneficial when using simulated training data in the target-domain.
</description>
    </item>
    
    <item>
        <title>Improving Children&amp;#8217;s Speech Recognition Through Explicit Pitch Scaling Based on Iterative Spectrogram Inversion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0302.PDF</link>
        <description>The task of transcribing children&amp;#8217;s speech using statistical
models trained on adults&amp;#8217; speech is very challenging. Large mismatch
in the acoustic and linguistic attributes of the training and test
data is reported to degrade the performance. In such speech recognition
tasks, the differences in pitch (or fundamental frequency) between
the two groups of speakers is one among several mismatch factors. To
overcome the pitch mismatch, an existing pitch scaling technique based
on iterative spectrogram inversion is explored in this work. Explicit
pitch scaling is found to improve the recognition of children&amp;#8217;s
speech under mismatched setup. In addition to that, we have also studied
the effect of discarding the phase information during spectrum reconstruction.
This is motivated by the fact that the dominant acoustic feature extraction
techniques make use of the magnitude spectrum only. On evaluating the
effectiveness under mismatched testing scenario, the existing as well
as the modified pitch scaling techniques result in very similar recognition
performances. Furthermore, we have explored the role of pitch scaling
on another speech recognition system which is trained on speech data
from both adult and child speakers. Pitch scaling is noted to be effective
for children&amp;#8217;s speech recognition in this case as well.
</description>
    </item>
    
    <item>
        <title>RNN-LDA Clustering for Feature Based DNN Adaptation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0368.PDF</link>
        <description>Model based deep neural network (DNN) adaptation approaches often require
multi-pass decoding in test time. Input feature based DNN adaptation,
for example, based on latent Dirichlet allocation (LDA) clustering,
provide a more efficient alternative. In conventional LDA clustering,
the transition and correlation between neighboring clusters is ignored.
In order to address this issue, a recurrent neural network (RNN) based
clustering scheme is proposed to learn both the standard LDA cluster
labels and their natural correlation over time in this paper. In addition
to directly using the resulting RNN-LDA as input features during DNN
adaptation, a range of techniques were investigated to condition the
DNN hidden layer parameters or activation outputs on the RNN-LDA features.
On a DARPA Gale Mandarin Chinese broadcast speech transcription task,
the proposed RNN-LDA cluster features adapted DNN system outperformed
both the baseline un-adapted DNN system and conventional LDA features
adapted DNN system by 8% relative on the most difficult Phoenix TV
subset. Consistent improvements were also obtained after further combination
with model based adaptation approaches.
</description>
    </item>
    
    <item>
        <title>Robust Online i-Vectors for Unsupervised Adaptation of DNN Acoustic Models: A Study in the Context of Digital Voice Assistants</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1342.PDF</link>
        <description>Supplementing log filter-bank energies with i-vectors is a popular
method for adaptive training of deep neural network acoustic models.
While  offline i-vectors (the target utterance or other relevant adaptation
material is available for i-vector extraction prior to decoding) have
been well studied, there is little analysis of  online i-vectors and
their robustness in multi-user scenarios where speaker changes can
be frequent and unpredictable. The authors of [1] showed that online
adaptation could be achieved through segmental i-vectors computed using
the hidden Markov model (HMM) state alignments of utterances decoded
in the recent past. While this approach works well in general, it could
be rendered ineffective by speaker changes. In this paper, we study
robust extensions of the ideas proposed in [1] by: (a) updating i-vectors
on a per-frame basis based on the incoming target utterance, and (b)
using lattice posteriors instead of one-best HMM state alignments.
Experiments with different i-vector implementations show that: (a)
when speaker changes occur, lattice-based frame-level i-vectors provide
up to 6% word error rate reduction relative to the baseline [1], and
(b) online i-vectors are more effective, in general, when the microphone
characteristics of test utterances are not seen in training.
</description>
    </item>
    
    <item>
        <title>Semi-Supervised Learning with Semantic Knowledge Extraction for Improved Speech Recognition in Air Traffic Control</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1446.PDF</link>
        <description>Automatic Speech Recognition (ASR) can introduce higher levels of automation
into Air Traffic Control (ATC), where spoken language is still the
predominant form of communication. While ATC uses standard phraseology
and a limited vocabulary, we need to adapt the speech recognition systems
to local acoustic conditions and vocabularies at each airport to reach
optimal performance. Due to continuous operation of ATC systems, a
large and increasing amount of untranscribed speech data is available,
allowing for semi-supervised learning methods to build and adapt ASR
models. In this paper, we first identify the challenges in building
ASR systems for specific ATC areas and propose to utilize out-of-domain
data to build baseline ASR models. Then we explore different methods
of data selection for adapting baseline models by exploiting the continuously
increasing untranscribed data. We develop a basic approach capable
of exploiting semantic representations of ATC commands. We achieve
relative improvement in both word error rate (23.5%) and concept error
rates (7%) when adapting ASR models to different ATC conditions in
a semi-supervised manner.
</description>
    </item>
    
    <item>
        <title>Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0556.PDF</link>
        <description>Layer normalization is a recently introduced technique for normalizing
the activities of neurons in deep neural networks to improve the training
speed and stability. In this paper, we introduce a new layer normalization
technique called   Dynamic Layer Normalization (DLN) for adaptive neural
acoustic modeling in speech recognition. By dynamically generating
the scaling and shifting parameters in layer normalization, DLN adapts
neural acoustic models to the acoustic variability arising from various
factors such as speakers, channel noises, and environments. Unlike
other adaptive acoustic models, our proposed approach does not require
additional adaptation data or speaker information such as i-vectors.
Moreover, the model size is fixed as it dynamically generates adaptation
parameters. We apply our proposed DLN to deep bidirectional LSTM acoustic
models and evaluate them on two benchmark datasets for large vocabulary
ASR experiments: WSJ and TED-LIUM release 2. The experimental results
show that our DLN improves neural acoustic models in terms of transcription
accuracy by dynamically adapting to various speakers and environments.
</description>
    </item>
    
    <item>
        <title>An Entrained Rhythm&#8217;s Frequency, Not Phase, Influences Temporal Sampling of Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Context Regularity Indexed by Auditory N1 and P2 Event-Related Potentials</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0658.PDF</link>
        <description>It is still a question of debate whether the N1-P2 complex is an index
of low-level auditory processes or whether it can capture higher-order
information encoded in the immediate context. To address this issue,
the current study examined the morphology of the N1-P2 complex as a
function of context regularities instantiated at the sublexical level.
We presented two types of speech targets in isolation and in contexts
comprising sequences of Cantonese words sharing either the entire rime
units or just the rime segments (thus lacking lexical tone consistency).
Results revealed a pervasive yet unequal attenuation of the N1 and
P2 components: The degree of N1 attenuation tended to decrease while
that of P2 increased due to enhanced detectability of more regular
speech patterns, as well as their enhanced predictability in the immediate
context. The distinct behaviors of N1 and P2 event-related potentials
could be explained by the influence of perceptual experience and the
hierarchical encoding of context regularities.
</description>
    </item>
    
    <item>
        <title>Discovering Language in Marmoset Vocalization</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0842.PDF</link>
        <description>Various studies suggest that marmosets ( Callithrix jacchus) show behavior
similar to that of humans in many aspects. Analyzing their calls would
not only enable us to better understand these species but would also
give insights into the evolution of human languages and vocal tract.
This paper describes a technique to discover the patterns in marmoset
vocalization in an unsupervised fashion. The proposed unsupervised
clustering approach operates in two stages. Initially, voice activity
detection (VAD) is applied to remove silences and non-voiced regions
from the audio. This is followed by a group-delay based segmentation
on the voiced regions to obtain smaller segments. In the second stage,
a two-tier clustering is performed on the segments obtained. Individual
hidden Markov models (HMMs) are built for each of the segments using
a  multiple frame size and  multiple frame rate. The HMMs are then
clustered until each cluster is made up of a large number of segments.
Once all the clusters get enough number of segments, one Gaussian mixture
model (GMM) is built for each of the clusters. These clusters are then
merged using Kullback-Leibler (KL) divergence. The algorithm converges
to the total number of distinct sounds in the audio, as evidenced by
listening tests.
</description>
    </item>
    
    <item>
        <title>Subject-Independent Classification of Japanese Spoken Sentences by Multiple Frequency Bands Phase Pattern of EEG Response During Speech Perception</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0854.PDF</link>
        <description>Recent speech perception models propose that neural oscillations in
theta band show phase locking to speech envelope to extract syllabic
information and rapid temporal information is processed by the corresponding
higher frequency band (e.g., low gamma). It is suggested that phase-locked
responses to acoustic features show consistent patterns across subjects.
Previous magnetoencephalographic (MEG) experiment showed that subject-dependent
template matching classification by theta phase patterns could discriminate
three English spoken sentences. In this paper, we adopt electroencephalography
(EEG) to the spoken sentence discrimination on Japanese language, and
we investigate the performances in various different settings by using:
(1) template matching and support vector machine (SVM) classifiers;
(2) subject dependent and independent models; (3) multiple frequency
bands including theta, alpha, beta, low gamma, and the combination
of all frequency bands. The performances in almost settings were higher
than the chance level. While performances of SVM and template matching
did not differ, the performance with combination of multiple frequency
bands outperformed the one that trained only on single frequency bands.
Best accuracies in subject dependent and independent models achieved
55.2% by SVM on the combination of all frequency bands and 44.0% by
template matching on the combination of all frequency bands, respectively.
</description>
    </item>
    
    <item>
        <title>The Phonological Status of the French Initial Accent and its Role in Semantic Processing: An Event-Related Potentials Study</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0934.PDF</link>
        <description>French accentuation is held to belong to the level of the phrase. Consequently
French is considered &amp;#8216;a language without accent&amp;#8217; with speakers
that are &amp;#8216;deaf to stress&amp;#8217;. Recent ERP-studies investigating
the French initial accent (IA) however demonstrate listeners not only
discriminate between different stress patterns, but also prefer words
to be marked with IA early in the process of speech comprehension.
Still, as words were presented in isolation, it remains unclear whether
the preference applied to the lexical or to the phrasal level. In the
current ERP-study, we address this ambiguity and manipulate IA on words
embedded in a sentence. Furthermore, we orthogonally manipulate semantic
congruity to investigate the interplay between accentuation and later
speech processing stages. Preliminary results on 14 participants reveal
a significant interaction effect: the centro-frontally located N400
was larger for words without IA, with a bigger effect for semantically
incongruent sentences. This indicates that IA is encoded at a lexical
level and facilitates semantic processing. Furthermore, as participants
attended to the semantic content of the sentences, the finding underlines
the automaticity of stress processing. In sum, we demonstrate accentuation
plays an important role in French speech comprehension and call for
the traditional view to be reconsidered.
</description>
    </item>
    
    <item>
        <title>A Neuro-Experimental Evidence for the Motor Theory of Speech Perception</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1741.PDF</link>
        <description>The somatotopic activation in the sensorimotor cortex during speech
comprehension has been redundantly documented and largely explained
by the notion of embodied semantics, which suggests that processing
auditory words referring to body movements recruits the same somatotopic
regions for that action execution. For this issue, the motor theory
of speech perception provided another explanation, suggesting that
the perception of speech sounds produced by a specific articulator
movement may recruit the motor representation of that articulator in
the precentral gyrus. To examine the latter theory, we used a set of
Chinese synonyms with different articulatory features, involving lip
gestures (LipR) or not (LipN), and recorded the electroencephalographic
(EEG) signals while subjects passively listened to them. It was found
that at about 200 ms post-onset, the event-related potential of LipR
and LipN showed a significant polarity reversal near the precentral
lip motor areas. EEG source reconstruction results also showed more
obvious somatotopic activation in the lip region for the LipR than
the LipN. Our results provide a positive support for the effect of
articulatory simulation on speech comprehension and basically agree
with the motor theory of speech perception.
</description>
    </item>
    
    <item>
        <title>Speech Representation Learning Using Unsupervised Data-Driven Modulation Filtering for Robust ASR</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0901.PDF</link>
        <description>The performance of an automatic speech recognition (ASR) system degrades
severely in noisy and reverberant environments in part due to the lack
of robustness in the underlying representations used in the ASR system.
On the other hand, the auditory processing studies have shown the importance
of modulation filtered spectrogram representations in robust human
speech recognition. Inspired by these evidences, we propose a speech
representation learning paradigm using data-driven 2-D spectro-temporal
modulation filter learning. In particular, multiple representations
are derived using the convolutional restricted Boltzmann machine (CRBM)
model in an unsupervised manner from the input speech spectrogram.
A filter selection criteria based on average number of active hidden
units is also employed to select the representations for ASR. The experiments
are performed on Wall Street Journal (WSJ) Aurora-4 database with clean
and multi condition training setup. In these experiments, the ASR results
obtained from the proposed modulation filtering approach shows significant
robustness to noise and channel distortions compared to other feature
extraction methods (average relative improvements of 19% over baseline
features in clean training). Furthermore, the ASR experiments performed
on reverberant speech data from the REVERB challenge corpus highlight
the benefits of the proposed representation learning scheme for far
field speech recognition.
</description>
    </item>
    
    <item>
        <title>Combined Multi-Channel NMF-Based Robust Beamforming for Noisy Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0642.PDF</link>
        <description>We propose a novel acoustic beamforming method using blind source separation
(BSS) techniques based on non-negative matrix factorization (NMF).
In conventional mask-based approaches, hard or soft masks are estimated
and beamforming is performed using speech and noise spatial covariance
matrices calculated from masked noisy observations, but the phase information
of the target speech is not adequately preserved. In the proposed method,
we perform complex-domain source separation based on multi-channel
NMF with rank-1 spatial model (rank-1 MNMF) to obtain a speech spatial
covariance matrix for estimating a steering vector for the target speech
utilizing the separated speech observation in each time-frequency bin.
This accurate steering vector estimation is effectively combined with
our novel noise mask prediction method using multi-channel robust NMF
(MRNMF) to construct a Maximum Likelihood (ML) beamformer that achieved
a better speech recognition performance than a state-of-the-art DNN-based
beamformer with no environment-specific training. Superiority of the
phase preserving source separation to real-valued masks in beamforming
is also confirmed through ASR experiments.
</description>
    </item>
    
    <item>
        <title>Recognizing Multi-Talker Speech with Permutation Invariant Training</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0305.PDF</link>
        <description>In this paper, we propose a novel technique for direct recognition
of multiple speech streams given the single channel of mixed speech,
without first separating them. Our technique is based on permutation
invariant training (PIT) for automatic speech recognition (ASR). In
PIT-ASR, we compute the average cross entropy (CE) over all frames
in the whole utterance for each possible output-target assignment,
pick the one with the minimum CE, and optimize for that assignment.
PIT-ASR forces all the frames of the same speaker to be aligned with
the same output layer. This strategy elegantly solves the label permutation
problem and speaker tracing problem in one shot. Our experiments on
artificially mixed AMI data showed that the proposed approach is very
promising.
</description>
    </item>
    
    <item>
        <title>Coupled Initialization of Multi-Channel Non-Negative Matrix Factorization Based on Spatial and Spectral Information</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0061.PDF</link>
        <description>Multi-channel non-negative matrix factorization (MNMF) is a multi-channel
extension of NMF and often outperforms NMF because it can deal with
spatial and spectral information simultaneously. On the other hand,
MNMF has a larger number of parameters and its performance heavily
depends on the initial values. MNMF factorizes an observation matrix
into four matrices: spatial correlation, basis, cluster-indicator latent
variables, and activation matrices. This paper proposes effective initialization
methods for these matrices. First, the spatial correlation matrix,
which shows the largest initial value dependencies, is initialized
using the cross-spectrum method from enhanced speech by binary masking.
Second, when the target is speech, constructing bases from phonemes
existing in an utterance can improve the performance: this paper proposes
a speech bases selection by using automatic speech recognition (ASR).
Third, we also propose an initialization method for the cluster-indicator
latent variables that couple the spatial and spectral information,
which can achieve the simultaneous optimization of above two matrices.
Experiments on a noisy ASR task show that the proposed initialization
significantly improves the performance of MNMF by reducing the initial
value dependencies.
</description>
    </item>
    
    <item>
        <title>Channel Compensation in the Generalised Vector Taylor Series Approach to Robust ASR</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0211.PDF</link>
        <description>Vector Taylor Series (VTS) is a powerful technique for robust ASR but,
in its standard form, it can only be applied to log-filter bank and
MFCC features. In earlier work, we presented a generalised VTS (gVTS)
that extends the applicability of VTS to front-ends which employ a
power transformation non-linearity. gVTS was shown to provide performance
improvements in both clean and additive noise conditions. This paper
makes two novel contributions. Firstly, while the previous gVTS formulation
assumed that noise was purely additive, we now derive gVTS formulae
for the case of speech in the presence of both additive noise and channel
distortion. Second, we propose a novel iterative method for estimating
the channel distortion which utilises gVTS itself and converges after
a few iterations. Since the new gVTS blindly assumes the existence
of both additive noise and channel effects, it is important not to
introduce extra distortion when either are absent. Experimental results
conducted on LVCSR Aurora-4 database show that the new formulation
passes this test. In the presence of channel noise only, it provides
relative WER reductions of up to 30% and 26%, compared with previous
gVTS and multi-style training with cepstral mean normalisation, respectively.
</description>
    </item>
    
    <item>
        <title>Robust Speech Recognition via Anchor Word Representations</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1570.PDF</link>
        <description>A challenge for speech recognition for voice-controlled household devices,
like the Amazon Echo or Google Home, is robustness against interfering
background speech. Formulated as a far-field speech recognition problem,
another person or media device in proximity can produce background
speech that can interfere with the device-directed speech. We expand
on our previous work on device-directed speech detection in the far-field
speech setting and introduce two approaches for robust acoustic modeling.
Both methods are based on the idea of using an anchor word taken from
the device directed speech. Our first method employs a simple yet effective
normalization of the acoustic features by subtracting the mean derived
over the anchor word. The second method utilizes an encoder network
projecting the anchor word onto a fixed-size embedding, which serves
as an additional input to the acoustic model. The encoder network and
acoustic model are jointly trained. Results on an in-house dataset
reveal that, in the presence of background speech, the proposed approaches
can achieve up to 35% relative word error rate reduction.
</description>
    </item>
    
    <item>
        <title>Towards Zero-Shot Frame Semantic Parsing for Domain Scaling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0518.PDF</link>
        <description>State-of-the-art slot filling models for goal-oriented human/ machine
conversational language understanding systems rely on deep learning
methods. While multi-task training of such models alleviates the need
for large in-domain annotated datasets, bootstrapping a semantic parsing
model for a new domain using only the semantic frame, such as the back-end
API or knowledge graph schema, is still one of the holy grail tasks
of language understanding for dialogue systems. This paper proposes
a deep learning based approach that can utilize  only the slot description
in context without the need for any labeled or unlabeled in-domain
examples, to quickly bootstrap a new domain. The main idea of this
paper is to leverage the encoding of the slot names and descriptions
within a multi-task deep learned slot filling model, to implicitly
align slots across domains. The proposed approach is promising for
solving the domain scaling problem and eliminating the need for any
manually annotated data or explicit schema alignment. Furthermore,
our experiments on multiple domains show that this approach results
in significantly better slot-filling performance when compared to using
only in-domain data, especially in the low data regime.
</description>
    </item>
    
    <item>
        <title>ClockWork-RNN Based Architectures for Slot Filling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1075.PDF</link>
        <description>A prevalent and challenging task in spoken language understanding is
slot filling. Currently, the best approaches in this domain are based
on recurrent neural networks (RNNs). However, in their simplest form,
RNNs cannot learn long-term dependencies in the data. In this paper,
we propose the use of ClockWork recurrent neural network (CW-RNN) architectures
in the slot-filling domain. CW-RNN is a multi-timescale implementation
of the simple RNN architecture, which has proven to be powerful since
it maintains relatively small model complexity. In addition, CW-RNN
exhibits a great ability to model long-term memory inherently. In our
experiments on the ATIS benchmark data set, we also evaluate several
novel variants of CW-RNN and we find that they significantly outperform
simple RNNs and they achieve results among the state-of-the-art, while
retaining smaller complexity.
</description>
    </item>
    
    <item>
        <title>Investigating the Effect of ASR Tuning on Named Entity Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1482.PDF</link>
        <description>Information retrieval from speech is a key technology for many applications,
as it allows access to large amounts of audio data. This technology
requires two major components: an automatic speech recognizer (ASR)
and a text-based information retrieval module such as a key word extractor
or a named entity recognizer (NER). When combining the two components,
the resulting final application needs to be globally optimized. However,
ASR and information retrieval are usually developed and optimized separately.
The ASR tends to be optimized to reduce the word error rate (WER),
a metric which does not take into account the contextual and syntactic
roles of the words, which are valuable information for information
retrieval systems. In this paper we investigate different ways to tune
the ASR for a speech-based NER system. In an end-to-end configuration
we also tested several ASR metrics, including WER, NE-WER and ATENE,
as well as the use of an oracle during the development step. Our results
show that using a NER oracle to tune the system reduces the named entity
recognition error rate by more than 1% absolute, and using the ATENE
metric allows us to reduce it by more than 0.75%. We also show that
these optimization approaches favor a higher ASR language model weight
which entails an overall gain in NER performance, despite a local increase
of the WER.
</description>
    </item>
    
    <item>
        <title>Label-Dependency Coding in Simple Recurrent Networks for Spoken Language Understanding</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1480.PDF</link>
        <description>Modeling target label dependencies is important for sequence labeling
tasks. This may become crucial in the case of Spoken Language Understanding
(SLU) applications, especially for the slot-filling task where models
have to deal often with a high number of target labels. Conditional
Random Fields (CRF) were previously considered as the most efficient
algorithm in these conditions. More recently, different architectures
of Recurrent Neural Networks (RNNs) have been proposed for the SLU
slot-filling task. Most of them, however, have been successfully evaluated
on the simple ATIS database, on which it is difficult to draw significant
conclusions. In this paper we propose new variants of RNNs able to
learn efficiently and effectively label dependencies by integrating
label embeddings. We show first that modeling label dependencies is
useless on the (simple) ATIS database and unstructured models can produce
state-of-the-art results on this benchmark. On ATIS our new variants
achieve the same results as state-of-the-art models, while being much
simpler. On the other hand, on the MEDIA benchmark, we show that the
modification introduced in the proposed RNN outperforms traditional
RNNs and CRF models.
</description>
    </item>
    
    <item>
        <title>Minimum Semantic Error Cost Training of Deep Long Short-Term Memory Networks for Topic Spotting on Conversational Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0590.PDF</link>
        <description>The topic spotting performance on spontaneous conversational speech
can be significantly improved by operating a support vector machine
with a latent semantic rational kernel (LSRK) on the decoded word lattices
(i.e., weighted finite-state transducers) of the speech [1]. In this
work, we propose the minimum semantic error cost (MSEC) training of
a deep bidirectional long short-term memory (BLSTM)-hidden Markov model
acoustic model for generating lattices that are semantically accurate
and are better suited for topic spotting with LSRK. With the MSEC training,
the expected semantic error cost of all possible word sequences on
the lattices is minimized given the reference. The word-word semantic
error cost is first computed from either the latent semantic analysis
or distributed vector-space word representations learned from the recurrent
neural networks and is then accumulated to form the expected semantic
error cost of the hypothesized word sequences. The proposed method
achieves 3.5%&amp;#8211;4.5% absolute topic classification accuracy improvement
over the baseline BLSTM trained with cross-entropy on Switchboard-1
Release 2 dataset.
</description>
    </item>
    
    <item>
        <title>Topic Identification for Speech Without ASR</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1093.PDF</link>
        <description>Modern topic identification (topic ID) systems for speech use automatic
speech recognition (ASR) to produce speech transcripts, and perform
supervised classification on such ASR outputs. However, under resource-limited
conditions, the manually transcribed speech required to develop standard
ASR systems can be severely limited or unavailable. In this paper,
we investigate alternative unsupervised solutions to obtaining tokenizations
of speech in terms of a vocabulary of automatically discovered word-like
or phoneme-like units, without depending on the supervised training
of ASR systems. Moreover, using automatic phoneme-like tokenizations,
we demonstrate that a convolutional neural network based framework
for learning spoken document representations provides competitive performance
compared to a standard bag-of-words representation, as evidenced by
comprehensive topic ID evaluations on both single-label and multi-label
classification tasks.
</description>
    </item>
    
    <item>
        <title>An End-to-End Trainable Neural Network Model with Belief Tracking for Task-Oriented Dialog</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1326.PDF</link>
        <description>We present a novel end-to-end trainable neural network model for task-oriented
dialog systems. The model is able to track dialog state, issue API
calls to knowledge base (KB), and incorporate structured KB query results
into system responses to successfully complete task-oriented dialogs.
The proposed model produces well-structured system responses by jointly
learning belief tracking and KB result processing conditioning on the
dialog history. We evaluate the model in a restaurant search domain
using a dataset that is converted from the second Dialog State Tracking
Challenge (DSTC2) corpus. Experiment results show that the proposed
model can robustly track dialog state given the dialog history. Moreover,
our model demonstrates promising results in producing appropriate system
responses, outperforming prior end-to-end trainable neural network
models using per-response accuracy evaluation metrics.
</description>
    </item>
    
    <item>
        <title>Deep Reinforcement Learning of Dialogue Policies with Less Weight Updates</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1060.PDF</link>
        <description>Deep reinforcement learning dialogue systems are attractive because
they can jointly learn their feature representations and policies without
manual feature engineering. But its application is challenging due
to slow learning. We propose a two-stage method for accelerating the
induction of single or multi-domain dialogue policies. While the first
stage reduces the amount of weight updates over time, the second stage
uses very limited minibatches (of as much as two learning experiences)
sampled from experience replay memories. The former frequently updates
the weights of the neural nets at early stages of training, and decreases
the amount of updates as training progresses by performing updates
during exploration and by skipping updates during exploitation. The
learning process is thus accelerated through less weight updates in
both stages. An empirical evaluation in three domains (restaurants,
hotels and tv guide) confirms that the proposed method trains policies
5 times faster than a baseline without the proposed method. Our findings
are useful for training larger-scale neural-based spoken dialogue systems.
</description>
    </item>
    
    <item>
        <title>Towards End-to-End Spoken Dialogue Systems with Turn Embeddings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1574.PDF</link>
        <description>Training task-oriented dialogue systems requires significant amount
of manual effort and integration of many independently built components;
moreover, the pipeline is prone to error-propagation. End-to-end training
has been proposed to overcome these problems by training the whole
system over the utterances of both dialogue parties. In this paper
we present an end-to-end spoken dialogue system architecture that is
based on turn embeddings. Turn embeddings encode a robust representation
of user turns with a local dialogue history and they are trained using
sequence-to-sequence models. Turn embeddings are trained by generating
the previous and the next turns of the dialogue and additionally perform
spoken language understanding. The end-to-end spoken dialogue system
is trained using the pre-trained turn embeddings in a stateful architecture
that considers the whole dialogue history. We observe that the proposed
spoken dialogue system architecture outperforms the models based on
local-only dialogue history and it is robust to automatic speech recognition
errors.
</description>
    </item>
    
    <item>
        <title>Speech and Text Analysis for Multimodal Addressee Detection in Human-Human-Computer Interaction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0501.PDF</link>
        <description>The necessity of addressee detection arises in multiparty spoken dialogue
systems which deal with human-human-computer interaction. In order
to cope with this kind of interaction, such a system is supposed to
determine whether the user is addressing the system or another human.
The present study is focused on multimodal addressee detection and
describes three levels of speech and text analysis: acoustical, syntactical,
and lexical. We define the connection between different levels of analysis
and the classification performance for different categories of speech
and determine the dependence of addressee detection performance on
speech recognition accuracy. We also compare the obtained results with
the results of the original research performed by the authors of the
Smart Video Corpus which we use in our computations. Our most effective
meta-classifier working with acoustical, syntactical, and lexical features
reaches an unweighted average recall equal to 0.917 showing almost
a nine percent advantage over the best baseline model, though this
baseline classifier additionally uses head orientation data. We also
propose a universal meta-model based on acoustical and syntactical
analysis, which may theoretically be applied in different domains.
</description>
    </item>
    
    <item>
        <title> Rushing to Judgement: How do Laypeople Rate Caller Engagement in Thin-Slice Videos of Human&amp;#8211;Machine Dialog?</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1205.PDF</link>
        <description>We analyze the efficacy of a small crowd of na&amp;#239;ve human raters
in rating engagement during human&amp;#8211;machine dialog interactions.
Each rater viewed multiple 10 second, thin-slice videos of non-native
English speakers interacting with a computer-assisted language learning
(CALL) system and rated how engaged and disengaged those callers were
while interacting with the automated agent. We observe how the crowd&amp;#8217;s
ratings compared to callers&amp;#8217; self ratings of engagement, and
further study how the distribution of these rating assignments vary
as a function of whether the automated system or the caller was speaking.
Finally, we discuss the potential applications and pitfalls of such
a crowdsourced paradigm in designing, developing and analyzing engagement-aware
dialog systems.
</description>
    </item>
    
    <item>
        <title>Hyperarticulation of Corrections in Multilingual Dialogue Systems</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0753.PDF</link>
        <description>This present paper aims at answering the question whether there are
distinctive cross-linguistic differences associated with hyperarticulated
speech in correction dialogue acts. The objective is to assess the
effort for adaptation of a multilingual dialogue system in 9 different
languages, regarding the recovery strategies, particularly corrections.
If the presence of hyperarticulation significantly differs across languages,
it will have a significant impact on the dialogue design and recovery
strategies.
</description>
    </item>
    
    <item>
        <title>Multitask Sequence-to-Sequence Models for Grapheme-to-Phoneme Conversion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1436.PDF</link>
        <description>Recently, neural sequence-to-sequence (Seq2Seq) models have been applied
to the problem of grapheme-to-phoneme (G2P) conversion. These models
offer a straightforward way of modeling the conversion by jointly learning
the alignment and translation of input to output tokens in an end-to-end
fashion. However, until now this approach did not show improved error
rates on its own compared to traditional joint-sequence based n-gram
models for G2P. In this paper, we investigate how multitask learning
can improve the performance of Seq2Seq G2P models. A single Seq2Seq
model is trained on multiple phoneme lexicon datasets containing multiple
languages and phonetic alphabets. Although multi-language learning
does not show improved error rates, combining standard datasets and
crawled data with different phonetic alphabets of the same language
shows promising error reductions on English and German Seq2Seq G2P
conversion. Finally, combining Seq2seq G2P models with standard n-grams
based models yields significant improvements over using either model
alone.
</description>
    </item>
    
    <item>
        <title>Acoustic Data-Driven Lexicon Learning Based on a Greedy Pronunciation Selection Framework</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0588.PDF</link>
        <description>Speech recognition systems for irregularly-spelled languages like English
normally require hand-written pronunciations. In this paper, we describe
a system for automatically obtaining pronunciations of words for which
pronunciations are not available, but for which transcribed data exists.
Our method integrates information from the letter sequence and from
the acoustic evidence. The novel aspect of the problem that we address
is the problem of how to prune entries from such a lexicon (since,
empirically, lexicons with too many entries do not tend to be good
for ASR performance). Experiments on various ASR tasks show that, with
the proposed framework, starting with an initial lexicon of several
thousand words, we are able to learn a lexicon which performs close
to a full expert lexicon in terms of WER performance on test data,
and is better than lexicons built using G2P alone or with a pruning
criterion based on pronunciation probability.
</description>
    </item>
    
    <item>
        <title>Semi-Supervised Learning of a Pronunciation Dictionary from Disjoint Phonemic Transcripts and Text</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1081.PDF</link>
        <description>While the performance of automatic speech recognition systems has recently
approached human levels in some tasks, the application is still limited
to specific domains. This is because system development relies on extensive
supervised training and expert tuning in the target domain. To solve
this problem, systems must become more self-sufficient, having the
ability to learn directly from speech and adapt to new tasks. One open
question in this area is how to learn a pronunciation dictionary containing
the appropriate vocabulary. Humans can recognize words, even ones they
have never heard before, by reading text and understanding the context
in which a word is used. However, this ability is missing in current
speech recognition systems. In this work, we propose a new framework
that automatically expands an initial pronunciation dictionary using
independently sampled acoustic and textual data. While the task is
very challenging and in its initial stage, we demonstrate that a model
based on Bayesian learning of Dirichlet processes can acquire word
pronunciations from phone transcripts and text of the WSJ data set.
</description>
    </item>
    
    <item>
        <title>Improved Subword Modeling for WFST-Based Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0103.PDF</link>
        <description>Because in agglutinative languages the number of observed word forms
is very high, subword units are often utilized in speech recognition.
However, the proper use of subword units requires careful consideration
of details such as silence modeling, position-dependent phones, and
combination of the units. In this paper, we implement subword modeling
in the Kaldi toolkit by creating modified lexicon by finite-state transducers
to represent the subword units correctly. We experiment with multiple
types of word boundary markers and achieve the best results by adding
a marker to the left or right side of a subword unit whenever it is
not preceded or followed by a word boundary, respectively. We also
compare three different toolkits that provide data-driven subword segmentations.
In our experiments on a variety of Finnish and Estonian datasets, the
best subword models do outperform word-based models and naive subword
implementations. The largest relative reduction in WER is a 23% over
word-based models for a Finnish read speech dataset. The results are
also better than any previously published ones for the same datasets,
and the improvement on all datasets is more than 5%.
</description>
    </item>
    
    <item>
        <title>Pronunciation Learning with RNN-Transducers</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0047.PDF</link>
        <description>Most speech recognition systems rely on pronunciation dictionaries
to provide accurate transcriptions. Typically, some pronunciations
are carved manually, but many are produced using pronunciation learning
algorithms. Successful algorithms must have the ability to generate
rich pronunciation variants, e.g. to accommodate words of foreign origin,
while being robust to artifacts of the training data, e.g. noise in
the acoustic segments from which the pronunciations are learned if
the method uses acoustic signals. We propose a general finite-state
transducer (FST) framework to describe such algorithms. This representation
is flexible enough to accommodate a wide variety of pronunciation learning
algorithms, including approaches that rely on the availability of acoustic
data, and methods that only rely on the spelling of the target words.
In particular, we show that the pronunciation FST can be built from
a recurrent neural network (RNN) and tuned to provide rich yet constrained
pronunciations. This new approach reduces the number of incorrect pronunciations
learned from Google Voice traffic by up to 25% relative.
</description>
    </item>
    
    <item>
        <title>Learning Similarity Functions for Pronunciation Variations</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1117.PDF</link>
        <description>A significant source of errors in Automatic Speech Recognition (ASR)
systems is due to pronunciation variations which occur in spontaneous
and conversational speech. Usually ASR systems use a finite lexicon
that provides one or more pronunciations for each word. In this paper,
we focus on learning a similarity function between two pronunciations.
The pronunciations can be the canonical and the surface pronunciations
of the same word or they can be two surface pronunciations of different
words. This task generalizes problems such as lexical access (the problem
of learning the mapping between words and their possible pronunciations),
and defining word neighborhoods. It can also be used to dynamically
increase the size of the pronunciation lexicon, or in predicting ASR
errors. We propose two methods, which are based on recurrent neural
networks, to learn the similarity function. The first is based on binary
classification, and the second is based on learning the ranking of
the pronunciations. We demonstrate the efficiency of our approach on
the task of lexical access using a subset of the Switchboard conversational
speech corpus. Results suggest that on this task our methods are superior
to previous methods which are based on graphical Bayesian methods.
</description>
    </item>
    
    <item>
        <title>Spoken Language Identification Using LSTM-Based Angular Proximity</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1334.PDF</link>
        <description>This paper describes the design of an acoustic language identification
(LID) system based on LSTMs that directly maps a sequence of acoustic
features to a vector in a vector space where the angular proximity
corresponds to a measure of language/dialect similarity. A specific
architecture for the LSTM-based language vector extractor is introduced
along with the angular proximity loss function to train it. This new
LSTM-based LID system is quicker to train than a standard RNN topology
using stacked layers trained with the cross-entropy loss function and
obtains significantly lower language error rates. Experiments compare
this approach to our previous developments on the subject, as well
as to two widely used LID techniques: a phonotactic system using DNN
acoustic models and an i-vector system. Results are reported on two
different data sets: the 14 languages of NIST LRE07 and the 20 closely
related languages and dialects of NIST LRE15. In addition to reporting
the NIST Cavg metric which served as the primary metric for the LRE07
and LRE15 evaluations, the average LER is provided.
</description>
    </item>
    
    <item>
        <title>End-to-End Language Identification Using High-Order Utterance Representation with Bilinear Pooling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Dialect Recognition Based on Unsupervised Bottleneck Features</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0576.PDF</link>
        <description>Recently, bottleneck features (BNF) with an i-Vector strategy has been
used for state-of-the-art language/dialect identification. However,
traditional bottleneck extraction requires an additional transcribed
corpus which is used for acoustic modeling. Alternatively, an unsupervised
BNF extraction diagram is proposed in our study, which is derived from
the traditional structure but trained with an estimated phonetic label.
The proposed method is evaluated on a 4-way Chinese dialect dataset
and a 5-way closely spaced Pan-Arabic corpus. Compared to a baseline
i-Vector system based on acoustic features MFCCs, the proposed unsupervised
BNF consistently achieves better performance across two corpora. Specifically,
the EER and overall performance C_avg * 100 are improved by a relative
+48% and +52%, respectively. Even under the condition with limited
training data, the proposed feature still achieves up to 24% relative
improvement compared to baseline, all without the need of a secondary
transcribed corpus.
</description>
    </item>
    
    <item>
        <title>Investigating Scalability in Hierarchical Language Identification System</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Improving Sub-Phone Modeling for Better Native Language Identification with Non-Native English Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0245.PDF</link>
        <description>Identifying a speaker&amp;#8217;s native language with his speech in a
second language is useful for many human-machine voice interface applications.
In this paper, we use a sub-phone-based i-vector approach to identify
non-native English speakers&amp;#8217; native languages by their English
speech input. Time delay deep neural networks (TDNN) are trained on
LVCSR corpora for improving the alignment of speech utterances with
their corresponding sub-phonemic &amp;#8220;senone&amp;#8221; sequences. The
phonetic variability caused by a speaker&amp;#8217;s native language can
be better modeled with the sub-phone models than the conventional phone
model based approach. Experimental results on the database released
for the 2016 Interspeech ComParE Native Language challenge with 11
different L1s show that our system outperforms the best system by a
large margin (87.2% UAR compared to 81.3% UAR for the best system from
the 2016 ComParE challenge).
</description>
    </item>
    
    <item>
        <title>QMDIS: QCRI-MIT Advanced Dialect Identification System</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1391.PDF</link>
        <description>As a continuation of our efforts towards tackling the problem of spoken
Dialect Identification (DID) for Arabic languages, we present the QCRI-MIT
Advanced Dialect Identification System (QMDIS). QMDIS is an automatic
spoken DID system for Dialectal Arabic (DA). In this paper, we report
a comprehensive study of the three main components used in the spoken
DID task: phonotactic, lexical and acoustic. We use Support Vector
Machines (SVMs), Logistic Regression (LR) and Convolutional Neural
Networks (CNNs) as backend classifiers throughout the study. We perform
all our experiments on a publicly available dataset and present new
state-of-the-art results. QMDIS discriminates between the five most
widely used dialects of Arabic: namely Egyptian, Gulf, Levantine, North
African, and Modern Standard Arabic (MSA).We report &amp;#8776;73% accuracy
for system combination. All the data and the code used in our experiments
are publicly available for research.
</description>
    </item>
    
    <item>
        <title>Detection of Replay Attacks Using Single Frequency Filtering Cepstral Coefficients</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0256.PDF</link>
        <description>Automatic speaker verification systems are vulnerable to spoofing attacks.
Recently, various countermeasures have been developed for detecting
high technology attacks such as speech synthesis and voice conversion.
However, there is a wide gap in dealing with replay attacks. In this
paper, we propose a new feature for replay attack detection based on
single frequency filtering (SFF), which provides high temporal and
spectral resolution at each instant. Single frequency filtering cepstral
coefficients (SFFCC) with Gaussian mixture model classifier are used
for the experimentation on the standard BTAS-2016 corpus. The previously
reported best result, which is based on constant Q cepstral coefficients
(CQCC) achieved a half total error rate of 0.67% on this data-set.
Our proposed method outperforms the state of the art (CQCC) with a
half total error rate of 0.0002%.
</description>
    </item>
    
    <item>
        <title>Unsupervised Representation Learning Using Convolutional Restricted Boltzmann Machine for Spoof Speech Detection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1393.PDF</link>
        <description>Speech Synthesis (SS) and Voice Conversion (VC) presents a genuine
risk of attacks for Automatic Speaker Verification (ASV) technology.
In this paper, we use our recently proposed unsupervised filterbank
learning technique using Convolutional Restricted Boltzmann Machine
(ConvRBM) as a front-end feature representation. ConvRBM is trained
on training subset of ASV spoof 2015 challenge database. Analyzing
the filterbank trained on this dataset shows that ConvRBM learned more
low-frequency subband filters compared to training on natural speech
database such as TIMIT. The spoofing detection experiments were performed
using Gaussian Mixture Models (GMM) as a back-end classifier. ConvRBM-based
cepstral coefficients (ConvRBM-CC) perform better than hand crafted
Mel Frequency Cepstral Coefficients (MFCC). On the evaluation set,
ConvRBM-CC features give an absolute reduction of 4.76% in Equal Error
Rate (EER) compared to MFCC features. Specifically, ConvRBM-CC features
significantly perform better in both known attacks (1.93%) and unknown
attacks (5.87%) compared to MFCC features.
</description>
    </item>
    
    <item>
        <title>Independent Modelling of High and Low Energy Speech Frames for Spoofing Detection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0836.PDF</link>
        <description>Spoofing detection systems for automatic speaker verification have
moved from only modelling voiced frames to modelling all speech frames.
Unvoiced speech has been shown to carry information about spoofing
attacks and anti-spoofing systems may further benefit by treating voiced
and unvoiced speech differently. In this paper, we separate speech
into low and high energy frames and independently model the distributions
of both to form two spoofing detection systems that are then fused
at the score level. Experiments conducted on the ASVspoof 2015, BTAS
2016 and Spoofing and Anti-Spoofing (SAS) corpora demonstrate that
the proposed approach of fusing two independent high and low energy
spoofing detection systems consistently outperforms the standard approach
that does not distinguish between high and low energy frames.
</description>
    </item>
    
    <item>
        <title>Improving Speaker Verification Performance in Presence of Spoofing Attacks Using Out-of-Domain Spoofed Data</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1758.PDF</link>
        <description>Automatic speaker verification (ASV) systems are vulnerable to spoofing
attacks using speech generated by voice conversion and speech synthesis
techniques. Commonly, a countermeasure (CM) system is integrated with
an ASV system for improved protection against spoofing attacks. But
integration of the two systems is challenging and often leads to increased
false rejection rates. Furthermore, the performance of CM severely
degrades if in-domain development data are unavailable. In this study,
therefore, we propose a solution that uses two separate background
models &amp;#8212; one from human speech and another from spoofed data.
During test, the ASV score for an input utterance is computed as the
difference of the log-likelihood against the target model and the combination
of the log-likelihoods against two background models. Evaluation experiments
are conducted using the joint ASV and CM protocol of ASVspoof 2015
corpus consisting of text-independent ASV tasks with short utterances.
Our proposed system reduces error rates in the presence of spoofing
attacks by using out-of-domain spoofed data for system development,
while maintaining the performance for zero-effort imposter attacks
compared to the baseline system.
</description>
    </item>
    
    <item>
        <title>VoxCeleb: A Large-Scale Speaker Identification Dataset</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Call My Net Corpus: A Multilingual Corpus for Evaluation of Speaker Recognition Technology</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1521.PDF</link>
        <description>The Call My Net 2015 (CMN15) corpus presents a new resource for Speaker
Recognition Evaluation and related technologies. The corpus includes
conversational telephone speech recordings for a total of 220 speakers
spanning 4 languages: Tagalog, Cantonese, Mandarin and Cebuano. The
corpus includes 10 calls per speaker made under a variety of noise
conditions. Calls were manually audited for language, speaker identity
and overall quality. The resulting data has been used in the NIST 2016
SRE Evaluation and will be published in the Linguistic Data Consortium
catalog. We describe the goals of the CMN15 corpus, including details
of the collection protocol and auditing procedure and discussion of
the unique properties of this corpus compared to prior NIST SRE evaluation
corpora.
</description>
    </item>
    
    <item>
        <title>Sequence-to-Sequence Models Can Directly Translate Foreign Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Structured-Based Curriculum Learning for End-to-End English-Japanese Speech Translation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0944.PDF</link>
        <description>Sequence-to-sequence attentional-based neural network architectures
have been shown to provide a powerful model for machine translation
and speech recognition. Recently, several works have attempted to extend
the models for end-to-end speech translation task. However, the usefulness
of these models were only investigated on language pairs with similar
syntax and word order (e.g., English-French or English-Spanish). In
this work, we focus on end-to-end speech translation tasks on syntactically
distant language pairs (e.g., English-Japanese) that require distant
word reordering. To guide the encoder-decoder attentional model to
learn this difficult problem, we propose a structured-based curriculum
learning strategy. Unlike conventional curriculum learning that gradually
emphasizes difficult data examples, we formalize learning strategies
from easier network structures to more difficult network structures.
Here, we start the training with end-to-end encoder-decoder for speech
recognition or text-based machine translation task then gradually move
to end-to-end speech translation task. The experiment results show
that the proposed approach could provide significant improvements in
comparison with the one without curriculum learning.
</description>
    </item>
    
    <item>
        <title>Assessing the Tolerance of Neural Machine Translation Systems Against Speech Recognition Errors</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1690.PDF</link>
        <description>Machine translation systems are conventionally trained on textual resources
that do not model phenomena that occur in spoken language. While the
evaluation of neural machine translation systems on textual inputs
is actively researched in the literature, little has been discovered
about the complexities of translating spoken language data with neural
models. We introduce and motivate interesting problems one faces when
considering the translation of automatic speech recognition (ASR) outputs
on neural machine translation (NMT) systems. We test the robustness
of sentence encoding approaches for NMT encoder-decoder modeling, focusing
on word-based over byte-pair encoding. We compare the translation of
utterances containing ASR errors in state-of-the-art NMT encoder-decoder
systems against a strong phrase-based machine translation baseline
in order to better understand which phenomena present in ASR outputs
are better represented under the NMT framework than approaches that
represent translation as a linear model.
</description>
    </item>
    
    <item>
        <title>Toward Expressive Speech Translation: A Unified Sequence-to-Sequence LSTMs Approach for Translating Words and Emphasis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0896.PDF</link>
        <description>Emphasis is an important piece of paralinguistic information that is
used to express different intentions, attitudes, or convey emotion.
Recent works have tried to translate emphasis by developing additional
emphasis estimation and translation components apart from an existing
speech-to-speech translation (S2ST) system. Although these approaches
can preserve emphasis, they introduce more complexity to the translation
pipeline. The emphasis translation component has to wait for the target
language sentence and word alignments derived from a machine translation
system, resulting in a significant translation delay. In this paper,
we proposed an approach that jointly trains and predicts words and
emphasis in a unified architecture based on sequence-to-sequence models.
The proposed model not only speeds up the translation pipeline but
also allows us to perform joint training. Our experiments on the emphasis
and word translation tasks showed that we could achieve comparable
performance for both tasks compared with previous approaches while
eliminating complex dependencies.
</description>
    </item>
    
    <item>
        <title>NMT-Based Segmentation and Punctuation Insertion for Real-Time Spoken Language Translation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Tight Integration of Spatial and Spectral Features for BSS with Deep Clustering Embeddings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0187.PDF</link>
        <description>Recent advances in discriminatively trained mask estimation networks
to extract a single source utilizing beamforming techniques demonstrate,
that the integration of statistical models and deep neural networks
(DNNs) are a promising approach for robust automatic speech recognition
(ASR) applications. In this contribution we demonstrate how discriminatively
trained embeddings on spectral features can be tightly integrated into
statistical model-based source separation to separate and transcribe
overlapping speech. Good generalization to unseen spatial configurations
is achieved by estimating a statistical model at test time, while still
leveraging discriminative training of deep clustering embeddings on
a separate training set. We formulate an expectation maximization (EM)
algorithm which jointly estimates a model for deep clustering embeddings
and complex-valued spatial observations in the short time Fourier transform
(STFT) domain at test time. Extensive simulations confirm, that the
integrated model outperforms (a) a deep clustering model with a subsequent
beamforming step and (b) an EM-based model with a beamforming step
alone in terms of signal to distortion ratio (SDR) and perceptually
motivated metric (PESQ) gains. ASR results on a reverberated dataset
further show, that the aforementioned gains translate to reduced word
error rates (WERs) even in reverberant environments.
</description>
    </item>
    
    <item>
        <title>Speaker-Aware Neural Network Based Beamformer for Speaker Extraction in Speech Mixtures</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0667.PDF</link>
        <description>In this work, we address the problem of extracting one target speaker
from a multichannel mixture of speech. We use a neural network to estimate
masks to extract the target speaker and derive beamformer filters using
these masks, in a similar way as the recently proposed approach for
extraction of speech in presence of noise. To overcome the permutation
ambiguity of neural network mask estimation, which arises in presence
of multiple speakers, we propose to inform the neural network about
the target speaker so that it learns to follow the speaker characteristics
through the utterance. We investigate and compare different methods
of passing the speaker information to the network such as making one
layer of the network dependent on speaker characteristics. Experiments
on mixture of two speakers demonstrate that the proposed scheme can
track and extract a target speaker for both closed and open speaker
set cases.
</description>
    </item>
    
    <item>
        <title>Eigenvector-Based Speech Mask Estimation Using Logistic Regression</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1186.PDF</link>
        <description>In this paper, we use a logistic regression to learn a speech mask
from the dominant eigenvector of the  Power Spectral Density (PSD)
matrix of a multi-channel speech signal corrupted by ambient noise.
We employ this speech mask to construct the  Generalized Eigenvalue
(GEV) beamformer and a Wiener postfilter. Further, we extend the beamformer
to compensate for speech distortions. We do not make any assumptions
about the array geometry or the characteristics of the speech and noise
sources. Those parameters are learned from training data. Our assumptions
are that the speaker may move slowly in the near-field of the array,
and that the noise is in the far-field. We compare our speech enhancement
system against recent contributions using the CHiME4 corpus. We show
that our approach yields superior results, both in terms of perceptual
speech quality and speech mask estimation error.
</description>
    </item>
    
    <item>
        <title>Real-Time Speech Enhancement with GCC-NMF</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1458.PDF</link>
        <description>We develop an online variant of the GCC-NMF blind speech enhancement
algorithm and study its performance on two-channel mixtures of speech
and real-world noise from the SiSEC separation challenge. While GCC-NMF
performs enhancement independently for each time frame, the NMF dictionary,
its activation coefficients, and the target TDOA are derived using
the entire mixture signal, thus precluding its use online. Pre-learning
the NMF dictionary using the CHiME dataset and inferring its activation
coefficients online yields similar overall PEASS scores to the mixture-learned
method, thus generalizing to new speakers, acoustic environments, and
noise conditions. Surprisingly, if we forgo coefficient inference altogether,
this approach outperforms both the mixture-learned method and most
algorithms from the SiSEC challenge to date. Furthermore, the trade-off
between interference suppression and target fidelity may be controlled
online by adjusting the target TDOA window width. Finally, integrating
online target localization with max-pooled GCC-PHAT yields only somewhat
decreased performance compared to offline localization. We test a real-time
implementation of the online GCC-NMF blind speech enhancement system
on a variety of hardware platforms, with performance made to degrade
smoothly with decreasing computational power using smaller pre-learned
dictionaries.
</description>
    </item>
    
    <item>
        <title>Coherence-Based Dual-Channel Noise Reduction Algorithm in a Complex Noisy Environment</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1464.PDF</link>
        <description>In this paper, a coherence-based noise reduction algorithm is proposed
for a dual-channel speech enhancement system operating in a complex
noise environment. The spatial coherence between two omnidirectional
microphones is one of the crucial information for the dual-channel
speech enhancement system. In this paper, we introduce a new model
of coherence function for the complex noise environment in which a
target speech coexists with a coherent interference and diffuse noise
around. From the coherence model, three numerical methods of computing
the normalized signal to interference plus diffuse noise ratio (SINR),
which is related to the Wiener filter gain, are derived. Objective
parameters measured from the enhanced speech demonstrate superior performance
of the proposed algorithm in terms of speech quality and intelligibility,
over the conventional coherence-based noise reduction algorithm.
</description>
    </item>
    
    <item>
        <title>Glottal Model Based Speech Beamforming for ad-hoc Microphone Arrays</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1659.PDF</link>
        <description>We are interested in the task of speech beamforming in conference room
meetings, with microphones built in the electronic devices brought
and casually placed by meeting participants. This task is challenging
because of the inaccuracy in position and interference calibration
due to random microphone configuration, variance of microphone quality,
reverberation etc. As a result, not many beamforming algorithms perform
better than simply picking the closest microphone in this setting.
We propose a beamforming called Glottal Residual Assisted Beamforming
(GRAB). It does not rely on any position or interference calibration.
Instead, it incorporates a source-filter speech model and minimizes
the energy that cannot be accounted for by the model. Objective and
subjective evaluations on both simulation and real-world data show
that GRAB is able to suppress noise effectively while keeping the speech
natural and dry. Further analyses reveal that GRAB can distinguish
contaminated or reverberant channels and take appropriate action accordingly.
</description>
    </item>
    
    <item>
        <title>Acoustic Assessment of Disordered Voice with Continuous Speech Based on Utterance-Level ASR Posterior Features</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0280.PDF</link>
        <description>Most previous studies on acoustic assessment of disordered voice were
focused on extracting perturbation features from isolated vowels produced
with steady-state phonation. Natural speech, however, is considered
to be more preferable in the aspects of flexibility, effectiveness
and reliability for clinical practice. This paper presents an investigation
on applying automatic speech recognition (ASR) technology to disordered
voice assessment of Cantonese speakers. A DNN-based ASR system is trained
using phonetically-rich continuous utterances from normal speakers.
It was found that frame-level phone posteriors obtained from the ASR
system are strongly correlated with the severity level of voice disorder.
Phone posteriors in utterances with severe disorder exhibit significantly
larger variation than those with mild disorder. A set of utterance-level
posterior features are computed to quantify such variation for pattern
recognition purpose. An SVM based classifier is used to classify an
input utterance into the categories of mild, moderate and severe disorder.
The two-class classification accuracy for mild and severe disorders
is 90.3%, and significant confusion between mild and moderate disorders
is observed. For some of the subjects with severe voice disorder, the
classification results are highly inconsistent among individual utterances.
Furthermore, short utterances tend to have more classification errors.
</description>
    </item>
    
    <item>
        <title>Multi-Stage DNN Training for Automatic Recognition of Dysarthric Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0303.PDF</link>
        <description>Incorporating automatic speech recognition (ASR) in individualized
speech training applications is becoming more viable thanks to the
improved generalization capabilities of neural network-based acoustic
models. The main problem in developing applications for dysarthric
speech is the relative in-domain data scarcity. Collecting representative
amounts of dysarthric speech data is difficult due to rigorous ethical
and medical permission requirements, problems in accessing patients
who are generally vulnerable and often subject to altering health conditions
and, last but not least, the high variability in speech resulting from
different pathological conditions. Developing such applications is
even more challenging for languages which in general have fewer resources,
fewer speakers and, consequently, also fewer patients than English,
as in the case of a mid-sized language like Dutch. In this paper, we
investigate a multi-stage deep neural network (DNN) training scheme
aimed at obtaining better modeling of dysarthric speech by using only
a small amount of in-domain training data. The results show that the
system employing the proposed training scheme considerably improves
the recognition of Dutch dysarthric speech compared to a baseline system
with single-stage training only on a large amount of normal speech
or a small amount of in-domain data.
</description>
    </item>
    
    <item>
        <title>Improving Child Speech Disorder Assessment by Incorporating Out-of-Domain Adult Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0455.PDF</link>
        <description>This paper describes the continued development of a system to provide
early assessment of speech development issues in children and better
triaging to professional services. Whilst corpora of children&amp;#8217;s
speech are increasingly available, recognition of disordered children&amp;#8217;s
speech is still a data-scarce task. Transfer learning methods have
been shown to be effective at leveraging out-of-domain data to improve
ASR performance in similar data-scarce applications. This paper combines
transfer learning, with previously developed methods for constrained
decoding based on expert speech pathology knowledge and knowledge of
the target text. Results of this study show that transfer learning
with out-of-domain adult speech can improve phoneme recognition for
disordered children&amp;#8217;s speech. Specifically, a Deep Neural Network
(DNN) trained on adult speech and fine-tuned on a corpus of disordered
children&amp;#8217;s speech reduced the phoneme error rate (PER) of a DNN
trained on a children&amp;#8217;s corpus from 16.3% to 14.2%. Furthermore,
this fine-tuned DNN also improved the performance of a Hierarchal Neural
Network based acoustic model previously used by the system with a PER
of 19.3%. We close with a discussion of our planned future developments
of the system.
</description>
    </item>
    
    <item>
        <title>On Improving Acoustic Models for TORGO Dysarthric Speech Database</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0878.PDF</link>
        <description>Assistive technologies based on speech have been shown to improve the
quality of life of people affected with dysarthria, a motor speech
disorder. Multiple ways to improve Gaussian mixture model-hidden Markov
model (GMM-HMM) and deep neural network (DNN) based automatic speech
recognition (ASR) systems for TORGO database for dysarthric speech
are explored in this paper. Past attempts in developing ASR systems
for TORGO database were limited to training just monophone models and
doing speaker adaptation over them. Although a recent work attempted
training triphone and neural network models, parameters like the number
of context dependent states, dimensionality of the principal component
features etc were not properly tuned. This paper develops speaker-specific
ASR models for each dysarthric speaker in TORGO database by tuning
parameters of GMM-HMM model, number of layers and hidden nodes in DNN.
Employing dropout scheme and sequence discriminative training in DNN
also gave significant gains. Speaker adapted features like feature-space
maximum likelihood linear regression (FMLLR) are used to pass the speaker
information to DNNs. To the best of our knowledge, this paper presents
the best recognition accuracies for TORGO database till date.
</description>
    </item>
    
    <item>
        <title>Glottal Source Features for Automatic Speech-Based Depression Assessment</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1251.PDF</link>
        <description>Depression is one of the most prominent mental disorders, with an increasing
rate that makes it the fourth cause of disability worldwide. The field
of automated depression assessment has emerged to aid clinicians in
the form of a decision support system. Such a system could assist as
a pre-screening tool, or even for monitoring high risk populations.
Related work most commonly involves multimodal approaches, typically
combining audio and visual signals to identify depression presence
and/or severity. The current study explores categorical assessment
of depression using audio features alone. Specifically, since depression-related
vocal characteristics impact the glottal source signal, we examine
Phase Distortion Deviation which has previously been applied to the
recognition of voice qualities such as hoarseness, breathiness and
creakiness, some of which are thought to be features of depressed speech.
The proposed method uses as features DCT-coefficients of the Phase
Distortion Deviation for each frequency band. An automated machine
learning tool, Just Add Data, is used to classify speech samples. The
method is evaluated on a benchmark dataset (AVEC2014), in two conditions:
read-speech and spontaneous-speech. Our findings indicate that Phase
Distortion Deviation is a promising audio-only feature for automated
detection and assessment of depressed speech.
</description>
    </item>
    
    <item>
        <title>Speech Processing Approach for Diagnosing Dementia in an Early Stage</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Effectively Building Tera Scale MaxEnt Language Models Incorporating Non-Linguistic Signals</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1203.PDF</link>
        <description>Maximum Entropy (MaxEnt) language models are powerful models that can
incorporate linguistic and non-linguistic contextual signals in a unified
framework with a convex loss. MaxEnt models also have the advantage
of scaling to large model and training data sizes We present the following
two contributions to MaxEnt training: (1) By leveraging smaller amounts
of transcribed data, we demonstrate that a MaxEnt LM trained on various
types of corpora can be easily adapted to better match the test distribution
of Automatic Speech Recognition (ASR); (2) A novel  adaptive-training
approach that efficiently models multiple types of non-linguistic features
in a universal model. We evaluate the impact of these approaches on
Google&amp;#8217;s state-of-the-art ASR for the task of voice-search transcription
and dictation. Training 10B parameter models utilizing a corpus of
up to 1T words, we show large reductions in word error rate from adaptation
across multiple languages. Also, human evaluations show significant
improvements on a wide range of domains from using non-linguistic features.
For example, adapting to geographical domains (e.g., US States and
cities) affects about 4% of test utterances, with 2:1 win to loss ratio.
</description>
    </item>
    
    <item>
        <title>Semi-Supervised Adaptation of RNNLMs by Fine-Tuning with Domain-Specific Auxiliary Features</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Approximated and Domain-Adapted LSTM Language Models for First-Pass Decoding in Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0147.PDF</link>
        <description>Traditionally, short-range Language Models (LMs) like the conventional
n-gram models have been used for language model adaptation. Recent
work has improved performance for such tasks using adapted long-span
models like Recurrent Neural Network LMs (RNNLMs). With the first pass
performed using a large background n-gram LM, the adapted RNNLMs are
mostly used to rescore lattices or N-best lists, as a second step in
the decoding process. Ideally, these adapted RNNLMs should be applied
for first-pass decoding. Thus, we introduce two ways of applying adapted
long-short-term-memory (LSTM) based RNNLMs for first-pass decoding.
Using available techniques to convert LSTMs to approximated versions
for first-pass decoding, we compare approximated LSTMs adapted in a
Fast Marginal Adaptation framework (FMA) and an approximated version
of architecture-based-adaptation of LSTM. On a conversational speech
recognition task, these differently approximated and adapted LSTMs
combined with a trigram LM outperform other adapted and unadapted LMs.
Here, the architecture-adapted LSTM combination obtains a 35.9% word
error rate (WER) and is outperformed by FMA-based LSTM combination
obtaining the overall lowest WER of 34.4%.
</description>
    </item>
    
    <item>
        <title>Sparse Non-Negative Matrix Language Modeling: Maximum Entropy Flexibility on the Cheap</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Multi-Scale Context Adaptation for Improving Child Automatic Speech Recognition in Child-Adult Spoken Interactions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0426.PDF</link>
        <description>The mutual influence of participant behavior in a dyadic interaction
has been studied for different modalities and quantified by computational
models. In this paper, we consider the task of automatic recognition
for children&amp;#8217;s speech, in the context of child-adult spoken interactions
during interviews of children suspected to have been maltreated. Our
long-term goal is to provide insights within this immensely important,
sensitive domain through large-scale lexical and paralinguistic analysis.
We demonstrate improvement in child speech recognition accuracy by
conditioning on both the domain and the interlocutor&amp;#8217;s (adult)
speech. Specifically, we use information from the automatic speech
recognizer outputs of the adult&amp;#8217;s speech, for which we have more
reliable estimates, to modify the recognition system of child&amp;#8217;s
speech in an unsupervised manner. By learning first at session level,
and then at the utterance level, we demonstrate an absolute improvement
of upto 28% WER and 55% perplexity over the baseline results. We also
report results of a parallel human speech recognition (HSR) experiment
where annotators are asked to transcribe child&amp;#8217;s speech under
two conditions: with and without contextual speech information. Demonstrated
ASR improvements and the HSR experiment illustrate the importance of
context in aiding child speech recognition, whether by humans or computers.
</description>
    </item>
    
    <item>
        <title>Using Knowledge Graph and Search Query Click Logs in Statistical Language Model for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1790.PDF</link>
        <description>This paper demonstrates how Knowledge Graph (KG) and Search Query Click
Logs (SQCL) can be leveraged in statistical language models to improve
named entity recognition for online speech recognition systems. Due
to the missing in the training data, some named entities may be recognized
as other common words that have the similar pronunciation. KG and SQCL
cover comprehensive and fresh named entities and queries that can be
used to mitigate the wrong recognition. First, all the entities located
in the same area in KG are clustered together, and the queries that
contain the entity names are selected from SQCL as the training data
of a geographical statistical language model for each entity cluster.
These geographical language models make the unseen named entities less
likely to occur during the model training, and can be dynamically switched
according to the user location in the recognition phase. Second, if
any named entities are identified in the previous utterances within
a conversational dialog, the probability of the n-best word sequence
paths that contain their related entities will be increased for the
current utterance by utilizing the entity relationships from KG and
SQCL. This way can leverage the long-term contexts within the dialog.
Experiments for the proposed approach on voice queries from a spoken
dialog system yielded a 12.5% relative perplexity reduction in the
language model measurement, and a 1.1% absolute word error rate reduction
in the speech recognition measurement.
</description>
    </item>
    
    <item>
        <title>Developing On-Line Speaker Diarization System</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0166.PDF</link>
        <description>In this paper we describe the process of converting a research prototype
system for Speaker Diarization into a fully deployed product running
in real time and with low latency. The deployment is a part of the
IBM Cloud Speech-to-Text (STT) Service. First, the prototype system
is described and the requirements for the on-line, deployable system
are introduced. Then we describe the technical approaches we took to
satisfy these requirements and discuss some of the challenges we have
faced. In particular, we present novel ideas for speeding up the system
by using Automatic Speech Recognition (ASR) transcripts as an input
to diarization, we introduce a concept of active window to keep the
computational complexity linear, we improve the speaker model using
a new speaker-clustering algorithm, we automatically keep track of
the number of active speakers and we enable the users to set an operating
point on a continuous scale between low latency and optimal accuracy.
The deployed system has been tuned on real-life data reaching average
Speaker Error Rates around 3% and improving over the prototype system
by about 10% relative.
</description>
    </item>
    
    <item>
        <title>Comparison of Non-Parametric Bayesian Mixture Models for Syllable Clustering and Zero-Resource Speech Processing</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0339.PDF</link>
        <description>Zero-resource speech processing (ZS) systems aim to learn structural
representations of speech without access to labeled data. A starting
point for these systems is the extraction of syllable tokens utilizing
the rhythmic structure of a speech signal. Several recent ZS systems
have therefore focused on clustering such syllable tokens into linguistically
meaningful units. These systems have so far used heuristically set
number of clusters, which can, however, be highly dataset dependent
and cannot be optimized in actual unsupervised settings. This paper
focuses on improving the flexibility of ZS systems using Bayesian non-parametric
(BNP) mixture models that are capable of simultaneously learning the
cluster models as well as their number based on the properties of the
dataset. We also compare different model design choices, namely priors
over the weights and the cluster component models, as the impact of
these choices is rarely reported in the previous studies. Experiments
are conducted using conversational speech from several languages. The
models are first evaluated in a separate syllable clustering task and
then as a part of a full ZS system in order to examine the potential
of BNP methods and illuminate the relative importance of different
model design choices.
</description>
    </item>
    
    <item>
        <title>Automatic Evaluation of Children Reading Aloud on Sentences and Pseudowords</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1541.PDF</link>
        <description>Reading aloud performance in children is typically assessed by teachers
on an individual basis, manually marking reading time and incorrectly
read words. A computational tool that assists with recording reading
tasks, automatically analyzing them and providing performance metrics
could be a significant help. Towards that goal, this work presents
an approach to automatically predicting the overall reading aloud ability
of primary school children (6&amp;#8211;10 years old), based on the reading
of sentences and pseudowords. The opinions of primary school teachers
were gathered as ground truth of performance, who provided 0&amp;#8211;5
scores closely related to the expectations at the end of each grade.
To predict these scores automatically, features based on reading speed
and number of disfluencies were extracted, after an automatic disfluency
detection. Various regression models were trained, with Gaussian process
regression giving best results for automatic features. Feature selection
from both sentence and pseudoword reading tasks gave the closest predictions,
with a correlation of 0.944. Compared to the use of manual annotation
with the best correlation being 0.952, automatic annotation was only
0.8% worse. Furthermore, the error rate of predicted scores relative
to ground truth was found to be smaller than the deviation of evaluators&amp;#8217;
opinion per child.
</description>
    </item>
    
    <item>
        <title>Off-Topic Spoken Response Detection with Word Embeddings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0388.PDF</link>
        <description>In this study, we developed an automated off-topic response detection
system as a supplementary module for an automated proficiency scoring
system for non-native English speakers&amp;#8217; spontaneous speech. Given
a spoken response, the system first generates an automated transcription
using an ASR system trained on non-native speech, and then generates
a set of features to assess similarity to the question. In contrast
to previous studies which required a large set of training responses
for each question, the proposed system only requires the question text,
thus increasing the practical impact of the system, since new questions
can be added to a test dynamically. However, questions are typically
short and the traditional approach based on exact word matching does
not perform well. In order to address this issue, a set of features
based on neural embeddings and a convolutional neural network (CNN)
were used. A system based on the combination of all features achieved
an accuracy of 87% on a balanced dataset, which was substantially higher
than the accuracy of a baseline system using question-based vector
space models (49%). Additionally, this system almost reached the accuracy
of vector space based model using a large set of responses to test
questions (93%).
</description>
    </item>
    
    <item>
        <title>Improving Mispronunciation Detection for Non-Native Learners with Multisource Information and LSTM-Based Deep Models</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0464.PDF</link>
        <description>In this paper, we utilize manner and place of articulation features
and deep neural network models (DNNs) with long short-term memory (LSTM)
to improve the detection performance of phonetic mispronunciations
produced by second language learners. First, we show that speech attribute
scores are complementary to conventional phone scores, so they can
be concatenated as features to improve a baseline system based only
on phone information. Next, pronunciation representation, usually calculated
by frame-level averaging in a DNN, is now learned by LSTM, which directly
uses sequential context information to embed a sequence of pronunciation
scores into a pronunciation vector to improve the performance of subsequent
mispronunciation detectors. Finally, when both proposed techniques
are incorporated into the baseline phone-based GOP (goodness of pronunciation)
classifier system trained on the same data, the integrated system reduces
the false acceptance rate (FAR) and false rejection rate (FRR) by 37.90%
and 38.44% (relative), respectively, from the baseline system.
</description>
    </item>
    
    <item>
        <title>Automatic Explanation Spot Estimation Method Targeted at Text and Figures in Lecture Slides</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0750.PDF</link>
        <description>Because of the spread of the Internet in recent years, e-learning,
which is a form of learning through the Internet, has been used in
school education. Many lecture videos delivered at The Open University
of Japan show lecturers and lecture slides alternately. In such video
style, it is hard to understand where on the slide the lecturer is
explaining. In this paper, we examined methods to automatically estimate
spots where the lecturer explains on the slide using lecture speech
and slide data. This technology is expected to help learners to study
the lectures. For itemized text slides, using DTW with word embedding
based distance, we obtained higher estimation accuracy than a previous
work. For slides containing figures, we estimated explanation spots
using image classification results and text in the charts. In addition,
we modified the lecture browsing system to indicate estimation results
on slides, and investigated the usefulness of indicating explanation
spots by subjective evaluation with the system.
</description>
    </item>
    
    <item>
        <title>Multiview Representation Learning via Deep CCA for Silent Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0952.PDF</link>
        <description>Silent speech recognition (SSR) converts non-audio information such
as articulatory (tongue and lip) movements to text. Articulatory movements
generally have less information than acoustic features for speech recognition,
and therefore, the performance of SSR may be limited. Multiview representation
learning, which can learn better representations by analyzing multiple
information sources simultaneously, has been recently successfully
used in speech processing and acoustic speech recognition. However,
it has rarely been used in SSR. In this paper, we investigate SSR based
on multiview representation learning via canonical correlation analysis
(CCA). When both acoustic and articulatory data are available during
training, it is possible to effectively learn a representation of articulatory
movements from the multiview data with CCA. To further represent the
complex structure of the multiview data, we apply deep CCA, where the
functional form of the feature mapping is a deep neural network. This
approach was evaluated in a speaker-independent SSR task using a data
set collected from seven English speakers using an electromagnetic
articulograph (EMA). Experimental results showed the effectiveness
of the multiview representation learning via deep CCA over the CCA-based
multiview approach as well as baseline articulatory movement data on
Gaussian mixture model and deep neural network-based SSR systems.
</description>
    </item>
    
    <item>
        <title>Use of Graphemic Lexicons for Spoken Language Assessment</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0978.PDF</link>
        <description>Automatic systems for practice and exams are essential to support the
growing worldwide demand for learning English as an additional language.
Assessment of spontaneous spoken English is, however, currently limited
in scope due to the difficulty of achieving sufficient automatic speech
recognition (ASR) accuracy. &amp;#8220;Off-the-shelf&amp;#8221; English ASR
systems cannot model the exceptionally wide variety of accents, pronunciations
and recording conditions found in non-native learner data. Limited
training data for different first languages (L1s), across all proficiency
levels, often with (at most) crowd-sourced transcriptions, limits the
performance of ASR systems trained on non-native English learner speech.
This paper investigates whether the effect of one source of error in
the system, lexical modelling, can be mitigated by using graphemic
lexicons in place of phonetic lexicons based on native speaker pronunciations.
Graphemic-based English ASR is typically worse than phonetic-based
due to the irregularity of English spelling-to-pronunciation but here
lower word error rates are consistently observed with the graphemic
ASR. The effect of using graphemes on automatic assessment is assessed
on different grader feature sets: audio and fluency derived features,
including some phonetic level features; and phone/grapheme distance
features which capture a measure of pronunciation ability.
</description>
    </item>
    
    <item>
        <title>Distilling Knowledge from an Ensemble of Models for Punctuation Prediction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>A Mostly Data-Driven Approach to Inverse Text Normalization</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1274.PDF</link>
        <description>For an automatic speech recognition system to produce sensibly formatted,
readable output, the spoken-form token sequence produced by the core
speech recognizer must be converted to a written-form string. This
process is known as inverse text normalization (ITN). Here we present
a mostly data-driven ITN system that leverages a set of simple rules
and a few hand-crafted grammars to cast ITN as a labeling problem.
To this labeling problem, we apply a compact bi-directional LSTM. We
show that the approach performs well using practical amounts of training
data.
</description>
    </item>
    
    <item>
        <title>Mismatched Crowdsourcing from Multiple Annotator Languages for Recognizing Zero-Resourced Languages: A Nullspace Clustering Approach</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1567.PDF</link>
        <description>It is extremely challenging to create training labels for building
acoustic models of zero-resourced languages, in which conventional
resources required for model training &amp;#8212; lexicons, transcribed
audio, or in extreme cases even orthographic system or a viable phone
set design for the language &amp;#8212; are unavailable. Here, language
mismatched transcripts, in which audio is transcribed in the orthographic
system of a completely different language by possibly non-speakers
of the target language may play a vital role. Such mismatched transcripts
have recently been successfully obtained through crowdsourcing and
shown to be beneficial to ASR performance. This paper further studies
this problem of using mismatched crowdsourced transcripts in a tonal
language for which we have no standard orthography, and in which we
may not even know the phoneme inventory. It proposes methods to project
the multilingual mismatched transcriptions of a tonal language to the
target phone segments. The results tested on Cantonese and Singapore
Hokkien have shown that the reconstructed phone sequences&amp;#8217; accuracies
have absolute increment of more than 3% from those of previously proposed
monolingual probabilistic transcription methods. 
</description>
    </item>
    
    <item>
        <title>Experiments in Character-Level Neural Network Models for Punctuation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1710.PDF</link>
        <description>We explore character-level neural network models for inferring punctuation
from text-only input. Punctuation inference is treated as a sequence
tagging problem where the input is a sequence of un-punctuated characters,
and the output is a corresponding sequence of punctuation tags. We
experiment with six architectures, all of which use a long short-term
memory (LSTM) network for sequence modeling. They differ in the way
the context and lookahead for a given character is derived: from simple
character embedding and delayed output to enable lookahead, to complex
convolutional neural networks (CNN) to capture context. We demonstrate
that the accuracy of proposed character-level models are competitive
with the accuracy of a state-of-the-art word-level Conditional Random
Field (CRF) baseline with carefully crafted features.
</description>
    </item>
    
    <item>
        <title>Multi-Channel Apollo Mission Speech Transcripts Calibration</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1778.PDF</link>
        <description>NASA&amp;#8217;s Apollo program is a great achievement of mankind in the
20th century. Previously we had introduced UTD-CRSS Apollo data digitization
initiative where we proposed to digitize Apollo mission speech data
(&amp;#126;100,000 hours) and develop Spoken Language Technology based
algorithms to analyze and understand various aspects of conversational
speech[1]. A new 30 track analog audio decoder is designed to decode
30 track Apollo analog tapes and is mounted on to the NASA Soundscriber
analog audio decoder (in place of single channel decoder). Using the
new decoder all 30 channels of data can be decoded simultaneously thereby
reducing the digitization time significantly. We have digitized 19,000
hours of data from Apollo missions (including entire Apollo-11, most
of Apollo-13, Apollo-1, and Gemini-8 missions). Each audio track corresponds
to a specific personnel/position in NASA mission control room or astronauts
in space. Since many of the planned Apollo related spoken language
technology approaches need transcripts we have developed an Apollo
mission specific custom Deep Neural Networks (DNN) based Automatic
Speech Recognition (ASR) system. Apollo specific language models are
developed. Most audio channels are degraded due to high channel noise,
system noise, attenuated signal bandwidth, transmission noise, cosmic
noise, analog tape static noise, noise due to tape aging, etc,. In
this paper we propose a novel method to improve the transcript quality
by using Signal-to-Noise ratio of channels and N-Gram sentence similarity
metrics across data channels. The proposed method shows significant
improvement in transcript quality of noisy channels. The Word Error
Rate (WER) analysis of transcripts across channels shows significant
reduction.
</description>
    </item>
    
    <item>
        <title>Calibration Approaches for Language Detection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0530.PDF</link>
        <description>To date, automatic spoken language detection research has largely been
based on a closed-set paradigm, in which the languages to be detected
are known prior to system application. In actual practice, such systems
may face previously unseen languages (out-of-set (OOS) languages) which
should be rejected, a common problem that has received limited attention
from the research community. In this paper, we focus on situations
in which either (1) the system-modeled languages are not observed during
use or (2) the test data contains OOS languages that are unseen during
modeling or calibration. In these situations, the common multi-class
objective function for calibration of language-detection scores is
problematic. We describe how the assumptions of multi-class calibration
are not always fulfilled in a practical sense and explore applying
global and language-dependent binary objective functions to relax system
constraints. We contrast the benefits and sensitivities of the calibration
approaches on practical scenarios by presenting results using both
LRE09 data and 14 languages from the BABEL dataset. We show that the
global binary approach is less sensitive to the characteristics of
the training data and that OOS modeling with individual detectors is
the best option when OOS test languages are not known to the system.
</description>
    </item>
    
    <item>
        <title>Bidirectional Modelling for Short Duration Language Identification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0286.PDF</link>
        <description>Language identification (LID) systems typically employ i-vectors as
fixed length representations of utterances. However, it may not be
possible to reliably estimate i-vectors from short utterances, which
in turn could lead to reduced language identification accuracy. Recently,
Long Short Term Memory networks (LSTMs) have been shown to better model
short utterances in the context of language identification. This paper
explores the use of bidirectional LSTMs for language identification
with the aim of modelling temporal dependencies between past and future
frame based features in short utterances. Specifically, an end-to-end
system for short duration language identification employing bidirectional
LSTM models of utterances is proposed. Evaluations on both NIST 2007
and 2015 LRE show state-of-the-art performance.
</description>
    </item>
    
    <item>
        <title>Conditional Generative Adversarial Nets Classifier for Spoken Language Identification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0553.PDF</link>
        <description>The i-vector technique using deep neural network has been successfully
applied in spoken language identification systems. Neural network modeling
showed its effectiveness as both discriminant feature transformation
and classification in many tasks, in particular with a large training
data set. However, on a small data set, neural networks suffer from
the overfitting problem which degrades the performance. Many strategies
have been investigated and used to improve the regularization for deep
neural networks, for example, weigh decay, dropout, data augmentation.
In this paper, we study and use conditional generative adversarial
nets as a classifier for the spoken language identification task. Unlike
the previous works on GAN for image generation, our purpose is to focus
on improving regularization of the neural network by jointly optimizing
the &amp;#8220;Real/Fake&amp;#8221; objective function and the categorical
objective function. Compared with dropout and data augmentation methods,
the proposed method obtained 29.7% and 31.8% relative improvement on
NIST 2015 i-vector challenge data set for spoken language identification.
</description>
    </item>
    
    <item>
        <title>Tied Hidden Factors in Neural Networks for End-to-End Speaker Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1314.PDF</link>
        <description>In this paper we propose a method to model speaker and session variability
and able to generate likelihood ratios using neural networks in an
end-to-end phrase dependent speaker verification system. As in Joint
Factor Analysis, the model uses tied hidden variables to model speaker
and session variability and a MAP adaptation of some of the parameters
of the model. In the training procedure our method jointly estimates
the network parameters and the values of the speaker and channel hidden
variables. This is done in a two-step backpropagation algorithm, first
the network weights and factor loading matrices are updated and then
the hidden variables, whose gradients are calculated by aggregating
the corresponding speaker or session frames, since these hidden variables
are tied. The last layer of the network is defined as a linear regression
probabilistic model whose inputs are the previous layer outputs. This
choice has the advantage that it produces likelihoods and additionally
it can be adapted during the enrolment using MAP without the need of
a gradient optimization. The decisions are made based on the ratio
of the output likelihoods of two neural network models, speaker adapted
and universal background model. The method was evaluated on the RSR2015
database.
</description>
    </item>
    
    <item>
        <title>Speaker Clustering by Iteratively Finding Discriminative Feature Space and Cluster Labels</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0923.PDF</link>
        <description>This paper presents a speaker clustering framework by iteratively performing
two stages: a discriminative feature space is obtained given a cluster
label set, and the cluster label set is updated using a clustering
algorithm given the feature space. In the iterations of two stages,
the cluster labels may be different from the true labels, and thus
the obtained feature space based on the labels may be inaccurately
discriminated. However, by iteratively performing above two stages,
more accurate cluster labels and more discriminative feature space
can be obtained, and finally they are converged. In this research,
the linear discriminant analysis is used for discriminating the i-vector
feature space, and the variational Bayesian expectation-maximization
on Gaussian mixture model is used for clustering the i-vectors. Our
iterative clustering framework was evaluated using the database of
keyword utterances and compared with the recently-published approaches.
In all experiments, the results show that our framework outperforms
the other approaches and converges in a few iterations.
</description>
    </item>
    
    <item>
        <title>Domain Adaptation of PLDA Models in Broadcast Diarization by Means of Unsupervised Speaker Clustering</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0084.PDF</link>
        <description>This work presents a new strategy to perform diarization dealing with
high variability data, such as multimedia information in broadcast.
This variability is highly noticeable among domains (inter-domain variability
among chapters, shows, genres, etc.). Therefore, each domain requires
its own specific model to obtain the optimal results. We propose to
adapt the PLDA models of our diarization system with in-domain unlabeled
data. To do it, we estimate pseudo-speaker labels by unsupervised speaker
clustering. This new method has been included in a PLDA-based diarization
system and evaluated on the Multi-Genre Broadcast 2015 Challenge data.
Given an audio, the system computes short-time i-vectors and clusters
them using a variational Bayesian PLDA model with hidden labels. The
proposed method improves 25.41% relative w.r.t. the system without
PLDA adaptation.
</description>
    </item>
    
    <item>
        <title>LSTM Neural Network-Based Speaker Segmentation Using Acoustic and Language Modelling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0407.PDF</link>
        <description>This paper presents a new speaker change detection system based on
Long Short-Term Memory (LSTM) neural networks using acoustic data and
linguistic content. Language modelling is combined with two different
Joint Factor Analysis (JFA) acoustic approaches: i-vectors and speaker
factors. Both of them are compared with a baseline algorithm that uses
cosine distance to detect speaker turn changes. LSTM neural networks
with both linguistic and acoustic features have been able to produce
a robust speaker segmentation. The experimental results show that our
proposal clearly outperforms the baseline system.
</description>
    </item>
    
    <item>
        <title>Acoustic Pairing of Original and Dubbed Voices in the Context of Video Game Localization</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1311.PDF</link>
        <description>The aim of this research work is the development of an automatic voice
recommendation system for assisted voice casting. In this article,
we propose preliminary work on acoustic pairing of original and dubbed
voices. The voice segments are taken from a video game released in
two different languages. The paired voice segments come from different
languages but belong to the same video game character. Our wish is
to exploit the relationship between a set of paired segments in order
to model the perceptual aspects of a given character depending on the
target language. We use a state-of-the-art approach in speaker recognition
( i.e. based on the paradigm i-vector/PLDA). We first evaluate pairs
of i-vectors using two different acoustic spaces, one for each of the
targeted languages. Secondly, we perform a transformation in order
to project the source-language i-vector into the target language. The
results showed that this latest approach is able to improve significantly
the accuracy. Finally, we challenge the system ability to model the
latent information that holds the video-game character independently
of the speaker, the linguistic content and the language.
</description>
    </item>
    
    <item>
        <title>Homogeneity Measure Impact on Target and Non-Target Trials in Forensic Voice Comparison</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0152.PDF</link>
        <description>It is common to see mobile recordings being presented as a forensic
trace in a court. In such cases, a forensic expert is asked to analyze
both suspect and criminal&amp;#8217;s voice samples in order to determine
the strength-of-evidence. This process is known as  Forensic Voice
Comparison (FVC). The  Likelihood ratio (LR) framework is commonly
used by the experts and quite often required by the expert&amp;#8217;s
associations &amp;#8220;best practice guides&amp;#8221;. Nevertheless, the
LR accepts some practical limitations due both to intrinsic aspects
of its estimation process and the information used during the FVC process.
These aspects are embedded in a more general one, the lack of knowledge
on FVC reliability. The question of reliability remains a major challenge,
particularly for FVC systems where numerous variation factors like
duration, noise, linguistic content or&amp;#8230; within-speaker variability
are not taken into account. Recently, we proposed an information theory-based
criterion able to estimate one of these factors, the homogeneity of
information between the two sides of a FVC trial. Thanks to this new
criterion, we wish to explore new aspects of homogeneity in this article.
We wish to question the impact of homogeneity on reliability separately
on target and non-target trials. The study is performed using FABIOLE,
a publicly available database dedicated to this kind of studies with
a large number of recordings per target speaker. Our experiments report
large differences of homogeneity impact between FVC genuine and impostor
trials. These results show clearly the importance of intra-speaker
variability effects in FVC reliability estimation. This study confirms
also the interest of homogeneity measure for FVC reliability.
</description>
    </item>
    
    <item>
        <title>Null-Hypothesis LLR: A Proposal for Forensic Automatic Speaker Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>The Opensesame NIST 2016 Speaker Recognition Evaluation System</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0997.PDF</link>
        <description>Last two decades have witnessed a significant progress in speaker recognition,
as evidenced by the improving performance in the speaker recognition
evaluations (SRE) hosted by NIST. Despite the progress, only a few
research is focused on speaker recognition with short duration and
language mismatch condition, which often leads to poor recognition
performance. In NIST SRE2016, these concerns were first systematically
investigated by the speaker recognition community. In this study, we
address these challenges from the viewpoint of feature extraction and
modeling. In particular, we improve the robustness of features by combining
GMM and DNN based iVector extraction approaches, and improve the reliability
of the back-end model by exploiting symmetric SVM that can effectively
leverage the unlabeled data. Finally, we introduce distance metric
learning to improve the generalization capacity of the development
data that is usually in limited size. Then a fusion strategy is adopted
to collectively boost the performance. The effectiveness of the proposed
scheme for speaker recognition is demonstrated on SRE2016 evaluation
data: compared with DNN-iVector PLDA baseline system, our method yields
25.6% relative improvement in terms of min_Cprimary.
</description>
    </item>
    
    <item>
        <title>IITG-Indigo System for NIST 2016 SRE Challenge</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1307.PDF</link>
        <description>This paper describes the speaker verification (SV) system submitted
to the NIST 2016 speaker recognition evaluation (SRE) challenge by
Indian Institute of Technology Guwahati (IITG) under the fixed training
condition task. Various SV systems are developed following the idea-level
collaboration with two other Indian institutions. Unlike the previous
SREs, this time the focus was on developing SV system using non-target
language speech data and a small amount unlabeled data from target
language/ dialects. For addressing these novel challenges, we tried
exploring the fusion of systems created using different features, data
conditioning, and classifiers. On NIST 2016 SRE evaluation data, the
presented fused system resulted in actual detection cost function (
actDCF) and equal error rate ( EER) of 0.81 and 12.91%, respectively.
Post-evaluation, we explored a recently proposed pairwise support vector
machine classifier and applied adaptive S-norm to the decision scores
before fusion. With these changes, the final system achieves the  actDCF
and  EER of 0.67 and 11.63%, respectively.
</description>
    </item>
    
    <item>
        <title>Locally Weighted Linear Discriminant Analysis for Robust Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0581.PDF</link>
        <description>Channel compensation is an integral part for any state-of-the-art speaker
recognition system. Typically, Linear Discriminant Analysis (LDA) is
used to suppress directions containing channel information. LDA assumes
a unimodal Gaussian distribution of the speaker samples to maximize
the ratio of the between-speaker variance to within-speaker variance.
However, when speaker samples have multi-modal non-Gaussian distributions
due to channel or noise distortions, LDA fails to provide optimal performance.
In this study, we propose Locally Weighted Linear Discriminant Analysis
(LWLDA). LWLDA computes the within-speaker scatter in a pairwise manner
and then scales it by an affinity matrix so as to preserve the within-class
local structure. This is in contrast to another recently proposed non-parametric
discriminant analysis method called NDA. We show that LWLDA not only
performs better than NDA but also is computationally much less expensive.
Experiments are performed using the DARPA Robust Automatic Transcription
of Speech (RATS) corpus. Results indicate that LWLDA consistently outperforms
both LDA and NDA on all trial conditions.
</description>
    </item>
    
    <item>
        <title>Recursive Whitening Transformation for Speaker Recognition on Language Mismatched Condition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0545.PDF</link>
        <description>Recently in speaker recognition, performance degradation due to the
channel domain mismatched condition has been actively addressed. However,
the mismatches arising from language is yet to be sufficiently addressed.
This paper proposes an approach which employs recursive whitening transformation
to mitigate the language mismatched condition. The proposed method
is based on the multiple whitening transformation, which is intended
to remove un-whitened residual components in the dataset associated
with i-vector length normalization. The experiments were conducted
on the Speaker Recognition Evaluation 2016 trials of which the task
is non-English speaker recognition using development dataset consist
of both a large scale out-of-domain (English) dataset and an extremely
low-quantity in-domain (non-English) dataset. For performance comparison,
we develop a state-of-the-art system using deep neural network and
bottleneck feature, which is based on a phonetically aware model. From
the experimental results, along with other prior studies, effectiveness
of the proposed method on language mismatched condition is validated.
</description>
    </item>
    
    <item>
        <title>Query-by-Example Search with Discriminative Neural Acoustic Word Embeddings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1592.PDF</link>
        <description>Query-by-example search often uses dynamic time warping (DTW) for comparing
queries and proposed matching segments. Recent work has shown that
comparing speech segments by representing them as fixed-dimensional
vectors &amp;#8212; acoustic word embeddings &amp;#8212; and measuring their
vector distance (e.g., cosine distance) can discriminate between words
more accurately than DTW-based approaches. We consider an approach
to query-by-example search that embeds both the query and database
segments according to a neural model, followed by nearest-neighbor
search to find the matching segments. Earlier work on embedding-based
query-by-example, using template-based acoustic word embeddings, achieved
competitive performance. We find that our embeddings, based on recurrent
neural networks trained to optimize word discrimination, achieve substantial
improvements in performance and run-time efficiency over the previous
approaches.
</description>
    </item>
    
    <item>
        <title>Constructing Acoustic Distances Between Subwords and States Obtained from a Deep Neural Network for Spoken Term Detection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0634.PDF</link>
        <description>The detection of out-of-vocabulary (OOV) query terms is a crucial problem
in spoken term detection (STD), because OOV query terms are likely.
To enable search of OOV query terms in STD systems, a query subword
sequence is compared with subword sequences generated using an automatic
speech recognizer against spoken documents. When comparing two subword
sequences, the edit distance is a typical distance between any two
subwords. We previously proposed an acoustic distance defined from
statistics between states of the hidden Markov model (HMM) and showed
its effectiveness in STD [4]. This paper proposes an acoustic distance
between subwords and HMM states where the posterior probabilities output
by a deep neural network are used to improve the STD accuracy for OOV
query terms. Experiments are conducted to evaluate the performance
of the proposed method, using the open test collections for the &amp;#8220;Spoken&amp;amp;Doc&amp;#8221;
tasks of the NTCIR-9 [13] and NTCIR-10 [14] workshops. The proposed
method shows improvements in mean average precision.
</description>
    </item>
    
    <item>
        <title>Fast and Accurate OOV Decoder on High-Level Features</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1367.PDF</link>
        <description>This work proposes a novel approach to out-of-vocabulary (OOV) keyword
search (KWS) task. The proposed approach is based on using high-level
features from an automatic speech recognition (ASR) system, so called
 phoneme posterior based ( PPB) features, for decoding. These features
are obtained by calculating time-dependent phoneme posterior probabilities
from word lattices, followed by their smoothing. For the PPB features
we developed a special novel very fast, simple and efficient OOV decoder.
Experimental results are presented on the Georgian language from the
IARPA Babel Program, which was the test language in the OpenKWS 2016
evaluation campaign. The results show that in terms of maximum term
weighted value (MTWV) metric and computational speed, for single ASR
systems, the proposed approach significantly outperforms the state-of-the-art
approach based on using in-vocabulary proxies for OOV keywords in the
indexed database. The comparison of the two OOV KWS approaches on the
fusion results of the nine different ASR systems demonstrates that
the proposed OOV decoder outperforms the proxy-based approach in terms
of MTWV metric given the comparable processing speed. Other important
advantages of the OOV decoder include extremely low memory consumption
and simplicity of its implementation and parameter optimization.
</description>
    </item>
    
    <item>
        <title>Exploring the Use of Significant Words Language Modeling for Spoken Document Retrieval</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0612.PDF</link>
        <description>Owing to the rapid global access to tremendous amounts of multimedia
associated with speech information on the Internet, spoken document
retrieval (SDR) has become an emerging application recently. Apart
from much effort devoted to developing robust indexing and modeling
techniques for spoken documents, a recent line of research targets
at enriching and reformulating query representations in an attempt
to enhance retrieval effectiveness. In practice, pseudo-relevance feedback
is by far the most prevalent paradigm for query reformulation, which
assumes that top-ranked feedback documents obtained from the initial
round of retrieval are potentially relevant and can be exploited to
reformulate the original query. Continuing this line of research, the
paper presents a novel modeling framework, which aims at discovering
significant words occurring in the feedback documents, to infer an
enhanced query language model for SDR. Formally, the proposed framework
targets at extracting the essential words representing a common notion
of relevance (i.e., the significant words which occur in almost all
of the feedback documents), so as to deduce a new query language model
that captures these significant words and meanwhile modulates the influence
of both highly frequent words and too specific words. Experiments conducted
on a benchmark SDR task demonstrate the performance merits of our proposed
framework.
</description>
    </item>
    
    <item>
        <title>Incorporating Acoustic Features for Spontaneous Speech Driven Content Retrieval</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0893.PDF</link>
        <description>A speech-driven information retrieval system is expected to be useful
for gathering information with greater ease. In a conventional system,
users have to decide on the contents of their utterance before speaking,
which takes quite a long time when their request is complicated. To
overcome that problem, it is required for the retrieval system to handle
a spontaneously spoken query directly. In this work, we propose an
extension technique of spoken content retrieval (SCR) for effectively
using spontaneously spoken queries. Acoustic features of meaningful
terms in the retrieval may have prominence compared to other terms.
Also, those terms will have linguistic specificity. From this assumption,
we predict the contribution of terms included in spontaneously spoken
queries using acoustic and linguistic features, and incorporate it
in the query likelihood model (QLM) which is a probabilistic retrieval
model. We verified the effectiveness of the proposed method through
experiments. Our proposed method was successful in improving retrieval
performance under various conditions.
</description>
    </item>
    
    <item>
        <title>Order-Preserving Abstractive Summarization for Spoken Content Based on Connectionist Temporal Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0862.PDF</link>
        <description>Connectionist temporal classification (CTC) is a powerful approach
for sequence-to-sequence learning, and has been popularly used in speech
recognition. The central ideas of CTC include adding a label  &amp;#8220;blank&amp;#8221;
during training. With this mechanism, CTC eliminates the need of segment
alignment, and hence has been applied to various sequence-to-sequence
learning problems. In this work, we applied CTC to abstractive summarization
for spoken content. The  &amp;#8220;blank&amp;#8221; in this case implies the
corresponding input data are less important or noisy; thus it can be
ignored. This approach was shown to outperform the existing methods
in term of ROUGE scores over Chinese Giga-word and MATBN corpora. This
approach also has the nice property that the ordering of words or characters
in the input documents can be better preserved in the generated summaries.
</description>
    </item>
    
    <item>
        <title>Automatic Alignment Between Classroom Lecture Utterances and Slide Components</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1752.PDF</link>
        <description>Multimodal alignment between classroom lecture utterances and lecture
slide components is one of the crucial problems to realize a multimodal
e-Learning application. This paper proposes the new method for the
automatic alignment, and formulates the alignment as the integer linear
programming (ILP) problem to maximize the score function which consists
of three factors: the similarity score between utterances and slide
components, the consistency of the explanation order, and the explanation
coverage of slide components. The experimental result on the Corpus
of Japanese classroom Lecture Contents (CJLC) shows that the automatic
alignment information acquired by the proposed method is effective
to improve the performance of the automatic extraction of important
utterances.
</description>
    </item>
    
    <item>
        <title>Compensating Gender Variability in Query-by-Example Search on Speech Using Voice Conversion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1183.PDF</link>
        <description>The huge amount of available spoken documents has raised the need for
tools to perform automatic searches within large audio databases. These
collections usually consist of documents with a great variability regarding
speaker, language or recording channel, among others. Reducing this
variability would boost the performance of query-by-example search
on speech systems, especially in zero-resource systems that use acoustic
features for audio representation. Hence, in this work, a technique
to compensate the variability caused by speaker gender is proposed.
Given a data collection composed of documents spoken by both male and
female voices, every time a spoken query has to be searched, an alternative
version of the query on its opposite gender is generated using voice
conversion. After that, the female version of the query is used to
search within documents spoken by females and vice versa. Experimental
validation of the proposed strategy shows an improvement of search
on speech performance caused by the reduction of gender variability.
</description>
    </item>
    
    <item>
        <title>Zero-Shot Learning Across Heterogeneous Overlapping Domains</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0516.PDF</link>
        <description>We present a zero-shot learning approach for text classification, predicting
which natural language understanding domain can handle a given utterance.
Our approach can predict domains at runtime that did not exist at training
time. We achieve this extensibility by learning to project utterances
and domains into the same embedding space while generating each domain-specific
embedding from a set of attributes that characterize the domain. Our
model is a neural network trained via ranking loss. We evaluate the
performance of this zero-shot approach on a subset of a virtual assistant&amp;#8217;s
third-party domains and show the effectiveness of the technique on
new domains not observed during training. We compare to generative
baselines and show that our approach requires less storage and performs
better on new domains.
</description>
    </item>
    
    <item>
        <title>Hierarchical Recurrent Neural Network for Story Segmentation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0392.PDF</link>
        <description>A broadcast news stream consists of a number of stories and each story
consists of several sentences. We capture this structure using a hierarchical
model based on a word-level Recurrent Neural Network (RNN) sentence
modeling layer and a sentence-level bidirectional Long Short-Term Memory
(LSTM) topic modeling layer. First, the word-level RNN layer extracts
a vector embedding the sentence information from the given transcribed
lexical tokens of each sentence. These sentence embedding vectors are
fed into a bidirectional LSTM that models the sentence and topic transitions.
A topic posterior for each sentence is estimated discriminatively and
a Hidden Markov model (HMM) follows to decode the story sequence and
identify story boundaries. Experiments on the topic detection and tracking
(TDT2) task indicate that the hierarchical RNN topic modeling achieves
the best story segmentation performance with a higher F1-measure compared
to conventional state-of-the-art methods. We also compare variations
of our model to infer the optimal structure for the story segmentation
task.
</description>
    </item>
    
    <item>
        <title>Evaluating Automatic Topic Segmentation as a Segment Retrieval Task</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Improving Speech Recognizers by Refining Broadcast Data with Inaccurate Subtitle Timestamps</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0650.PDF</link>
        <description>This paper proposes an automatic method to refine broadcast data collected
every week for efficient acoustic model training. For training acoustic
models, we use only audio signals, subtitle texts, and subtitle timestamps
accompanied by recorded broadcast programs. However, the subtitle timestamps
are often inaccurate due to inherent characteristics of closed captioning.
In the proposed method, we remove subtitle texts with low subtitle
quality index, concatenate adjacent subtitle texts into a merged subtitle
text, and correct the timestamp of the merged subtitle text by adding
a margin. Then, a speech recognizer is used to obtain a hypothesis
text from the speech segment corresponding to the merged subtitle text.
Finally, the refined speech segments to be used for acoustic model
training, are generated by selecting the subparts of the merged subtitle
text that matches the hypothesis text. It is shown that the acoustic
models trained by using refined broadcast data give significantly higher
speech recognition accuracy than those trained by using raw broadcast
data. Consequently, the proposed method can efficiently refine a large
amount of broadcast data with inaccurate timestamps taking about half
of the time, compared with the previous approaches.
</description>
    </item>
    
    <item>
        <title>A Relevance Score Estimation for Spoken Term Detection Based on RNN-Generated Pronunciation Embeddings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1087.PDF</link>
        <description>In this paper, we present a novel method for term score estimation.
The method is primarily designed for scoring the out-of-vocabulary
terms, however it could also estimate scores for in-vocabulary results.
The term score is computed as a cosine distance of two pronunciation
embeddings. The first one is generated from the grapheme representation
of the searched term, while the second one is computed from the recognized
phoneme confusion network. The embeddings are generated by specifically
trained recurrent neural network built on the idea of Siamese neural
networks. The RNN is trained from recognition results on word- and
phone-level in an unsupervised fashion without need of any hand-labeled
data. The method is evaluated on the MALACH data in two languages,
English and Czech. The results are compared with two baseline methods
for OOV term detection.
</description>
    </item>
    
    <item>
        <title>Predicting Automatic Speech Recognition Performance Over Communication Channels from Instrumental Speech Quality and Intelligibility Scores</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0036.PDF</link>
        <description>The performance of automatic speech recognition based on coded-decoded
speech heavily depends on the quality of the transmitted signals, determined
by channel impairments. This paper examines relationships between speech
recognition performance and measurements of speech quality and intelligibility
over transmission channels. Different to previous studies, the effects
of super-wideband transmissions are analyzed and compared to those
of wideband and narrowband channels. Furthermore, intelligibility scores,
gathered by conducting a listening test based on logatomes, are also
considered for the prediction of automatic speech recognition results.
The modern instrumental measurement techniques POLQA and POLQA-based
intelligibility have been respectively applied to estimate the quality
and the intelligibility of transmitted speech. Based on our results,
polynomial models are proposed that permit the prediction of speech
recognition accuracy from the subjective and instrumental measures,
involving a number of channel distortions in the three bandwidths.
This approach can save the costs of performing automatic speech recognition
experiments and can be seen as a first step towards a useful tool for
communication channel designers. 
</description>
    </item>
    
    <item>
        <title>Speech Intelligibility in Cars: The Effect of Speaking Style, Noise and Listener Age</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0105.PDF</link>
        <description>Intelligibility of speech in noise becomes lower as the listeners age
increases, even when no apparent hearing impairment is present. The
losses are, however, different depending on the nature of the noise
and the characteristics of the voice. In this paper we investigate
the effect that age, noise type and speaking style have on the intelligibility
of speech reproduced by car loudspeakers. Using a binaural mannequin
we recorded a variety of voices and speaking styles played from the
audio system of a car while driving in different conditions. We used
this material to create a listening test where participants were asked
to transcribe what they could hear and recruited groups of young and
older adults to take part in it. We found that intelligibility scores
of older participants were lower for the competing speaker and background
music conditions. Results also indicate that clear and Lombard speech
was more intelligible than plain speech for both age groups. A mixed
effect model revealed that the largest effect was the noise condition,
followed by sentence type, speaking style, voice, age group and pure
tone average.
</description>
    </item>
    
    <item>
        <title>Predicting Speech Intelligibility Using a Gammachirp Envelope Distortion Index Based on the Signal-to-Distortion Ratio</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0170.PDF</link>
        <description>A new intelligibility prediction measure, called &amp;#8220;Gammachirp
Envelope Distortion Index (GEDI)&amp;#8221; is proposed for the evaluation
of speech enhancement algorithms. This model calculates the signal-to-distortion
ratio (SDR) in envelope responses SDRenv derived from the gammachirp
filterbank outputs of clean and enhanced speech, and is an extension
of the speech based envelope power spectrum model (sEPSM) to improve
prediction and usability. An evaluation was performed by comparing
human subjective results and model predictions for the speech intelligibility
of noise-reduced sounds processed by spectral subtraction and a recent
Wiener filtering technique. The proposed GEDI predicted the subjective
results of the Wiener filtering better than those predicted by the
original sEPSM and well-known conventional measures, i.e., STOI, CSII,
and HASPI.
</description>
    </item>
    
    <item>
        <title>Intelligibilities of Mandarin Chinese Sentences with Spectral &amp;#8220;Holes&amp;#8221;</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0281.PDF</link>
        <description>The speech intelligibility of Mandarin Chinese sentences of various
spectral regions, regarding band-stop conditions (one or two &amp;#8220;holes&amp;#8221;
in the spectrum), was investigated through subjective listening tests.
Results demonstrated significant effects on Mandarin Chinese sentence
intelligibilities when a single or a pair of spectral holes was introduced.
Meanwhile, it revealed the importance of the first and second formant
(F1, F2) frequencies for the comprehension of Mandarin sentences. More
importantly, the first formant frequencies played a more primary role
rather than those of the second formants. Sentence intelligibilities
declined evidently with the lacking of F1 frequencies, but the effect
became small when the spectrum holes covered more than 50% of F1 frequencies,
and F2 frequencies came into a major play in the intelligibility of
Mandarin sentence.
</description>
    </item>
    
    <item>
        <title>The Effect of Situation-Specific Non-Speech Acoustic Cues on the Intelligibility of Speech in Noise</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0500.PDF</link>
        <description>In everyday life, speech is often accompanied by a situation-specific
acoustic cue; a hungry bark as you ask  &amp;#8216;Has anyone fed the dog?&amp;#8217;.
This paper investigates the effect such cues have on speech intelligibility
in noise and evaluates their interaction with the established effect
of situation-specific semantic cues. This work is motivated by the
introduction of new object-based broadcast formats, which have the
potential to optimise intelligibility by controlling the level of individual
broadcast audio elements, at point of service. Results of this study
show that situation-specific acoustic cues alone can improve word recognition
in multi-talker babble by 69.5%, a similar amount to semantic cues.
The combination of both semantic and acoustic cues provide further
improvement of 106.0% compared with no cues, and 18.7% compared with
semantic cues only. Interestingly, whilst increasing subjective intelligibility
of the target word, the presence of acoustic cues degraded the objective
intelligibility of the speech-based semantic cues by 47.0% (equivalent
to reducing the speech level by 4.5 dB). This paper discusses the interactions
between the two types of cues and the implications that these results
have for assessing and improving the intelligibility of broadcast speech.
</description>
    </item>
    
    <item>
        <title>On the Use of Band Importance Weighting in the Short-Time Objective Intelligibility Measure</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1043.PDF</link>
        <description>Speech intelligibility prediction methods are popular tools within
the speech processing community for objective evaluation of speech
intelligibility of e.g. enhanced speech. The Short-Time Objective Intelligibility
(STOI) measure has become highly used due to its simplicity and high
prediction accuracy. In this paper we investigate the use of Band Importance
Functions (BIFs) in the STOI measure, i.e. of unequally weighting the
contribution of speech information from each frequency band. We do
so by fitting BIFs to several datasets of measured intelligibility,
and cross evaluating the prediction performance. Our findings indicate
that it is possible to improve prediction performance in specific situations.
However, it has not been possible to find BIFs which systematically
improve prediction performance beyond the data used for fitting. In
other words, we find no evidence that the performance of the STOI measure
can be improved considerably by extending it with a non-uniform BIF.
</description>
    </item>
    
    <item>
        <title>Listening in the Dips: Comparing Relevant Features for Speech Recognition in Humans and Machines</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1168.PDF</link>
        <description>In recent years, automatic speech recognition (ASR) systems gradually
decreased (and for some tasks closed) the gap between human and automatic
speech recognition. However, it is unclear if similar performance implies
humans and ASR systems to rely on similar signal cues. In the current
study, ASR and HSR are compared using speech material from a matrix
sentence test mixed with either a stationary speech-shaped noise (SSN)
or amplitude-modulated SSN. Recognition performance of HSR and ASR
is measured in term of the speech recognition threshold (SRT), i.e.,
the signal-to-noise ratio with 50% recognition rate and by comparing
psychometric functions. ASR results are obtained with matched-trained
DNN-based systems that use FBank features as input and compared to
results obtained from eight normal-hearing listeners and two established
models of speech intelligibility. For both maskers, HSR and ASR achieve
similar SRTs with an average deviation of only 0.4 dB. A relevance
propagation algorithm is applied to identify features relevant for
ASR. The analysis shows that relevant features coincide either with
spectral peaks of the speech signal or with dips of the noise masker,
indicating that similar cues are important in HSR and ASR.
</description>
    </item>
    
    <item>
        <title>Mental Representation of Japanese Mora; Focusing on its Intrinsic Duration</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1720.PDF</link>
        <description>Japanese is one of the typical languages in which vowel quantity plays
a key role. In Japanese, a phonological structure called &amp;#8220;mora&amp;#8221;
is a fundamental rhythmic unit, and theoretically, each mora is supposed
to have a similar duration (isochronicity). The rhythm of a native
language has great importance on spoken language processing, including
second language speaking; therefore, in order to get a clear picture
of bottom-up speech processing, it is crucial to discern how morae
are mentally represented. Various studies have been conducted to understand
the nature of speech processing as a cognitive construct; however,
most of this research was conducted with the target stimuli embedded
in words or carrier sentences to clarify on specifically the relative
duration of morae. In this study, two reaction-time experiments were
conducted to investigate whether morae are mentally represented and
how long the duration is. The isolated vowels /i/, /e/, /a/, /o/, /u/,
and syllable /tan/ were chosen as target stimuli, and the first morae
were digitally manipulated into 15 durations with 20 ms variations
in length, from 150 ms to 330 ms. The results revealed the existence
of a durational threshold between one and two morae, ranging around
250 ms.
</description>
    </item>
    
    <item>
        <title>Temporal Dynamics of Lateral Channel Formation in /l/: 3D EMA Data from Australian English</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0765.PDF</link>
        <description>This study investigated the dynamics of lateral channel formation of
/l/ in Australian-accented English (AusE) using 3D electromagnetic
articulography (EMA). Coils were placed on the tongue both mid-sagitally
and para-sagitally. We varied the vowel preceding /l/ between /&amp;#618;/
and /&amp;#230;/, e.g.,  filbert vs.  talbot, and the syllable position
of /l/, e.g., /&amp;#39;t&amp;#230;l.b&amp;#x259;t/ vs. /&amp;#39;t&amp;#230;b.l&amp;#x259;t/.
The articulatory analyses of lateral /l/ show that: (1) the mid-sagittal
delay (from the tongue tip gesture to the tongue middle/tongue back
gesture) changes across different syllable positions and vowel contexts;
(2) the para-sagittal lateralization duration remains the same across
syllable positions and vowel contexts; (3) the lateral formation reaches
its peak earlier than the mid-sagittal gesture peak; (4) the magnitude
of tongue asymmetrical lateralization is greater than the magnitude
of tongue curvature in the coronal plane. We discuss these results
in light of the temporal dynamics of lateral channel formation. We
interpret our results as evidence that the formation of the lateral
channel is the primary goal of /l/ production.
</description>
    </item>
    
    <item>
        <title>Vowel and Consonant Sequences in three Bavarian Dialects of Austria</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Acoustic Cues to the Singleton-Geminate Contrast: The Case of Libyan Arabic Sonorants</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1609.PDF</link>
        <description>This study examines the acoustic correlates of the singleton and geminate
consonants in Tripolitanian Libyan Arabic (TLA). Several measurements
were obtained including target segment duration, preceding vowel duration,
RMS amplitude for the singleton and geminate consonants, and F1, F2
and F3 for the target consonants. The results confirm that the primary
acoustic correlate that distinguishes singletons from geminates in
TLA is duration regardless of sound type with the ratio of C to CC
being 1 to 2.42. The duration of the preceding vowels is suggestive
and may be considered as another cue to the distinction between them.
There was no evidence of differences in RMS amplitude between singleton
and geminate consonants of any type. F1, F2 and F3 frequencies are
found to show similar patterns for singleton and geminate consonants
for all sound types, suggesting no gestural effects of gemination in
TLA. Preliminary results from the phonetic cues investigated here suggest
that the acoustic distinction between singleton and geminate consonants
in TLA is dependent mainly on durational correlates.
</description>
    </item>
    
    <item>
        <title>Mel-Cepstral Distortion of German Vowels in Different Information Density Contexts</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0838.PDF</link>
        <description>This study investigated whether German vowels differ significantly
from each other in mel-cepstral distortion (MCD) when they stand in
different information density (ID) contexts. We hypothesized that vowels
in the same ID contexts are more similar to each other than vowels
that stand in different ID conditions. Read speech material from PhonDat2
of 16 German natives (m = 10, f = 6) was analyzed. Bi-phone and word
language models were calculated based on DeWaC. To account for additional
variability in the data, prosodic factors, as well as corpus-specific
frequency values were also entered into the statistical models. Results
showed that vowels in different ID conditions were significantly different
in their MCD values. Unigram word probability and corpus-specific word
frequency showed the expected effect on vowel similarity with a hierarchy
between non-contrasting and contrasting conditions. However, these
did not form a homogeneous group since there were group-internal significant
differences. The largest distance can be found between vowels produced
at fast speech rate, and between unstressed vowels.
</description>
    </item>
    
    <item>
        <title>Effect of Formant and F0 Discontinuity on Perceived Vowel Duration: Impacts for Concatenative Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1161.PDF</link>
        <description>Unit selection systems of speech synthesis offer good overall quality,
but this may be countervailed by a sporadic and unpredictable occurrence
of audible artifacts, such as discontinuities in F0 and the spectrum.
Informal observations suggested that such breaks may have an effect
on perceived vowel duration. This study therefore investigates the
effect of F0 and formant discontinuities on the perceived duration
of vowels in Czech synthetic speech. Ten manipulations of F0, F1 and
F2 were performed on target vowels in short synthesized phrases creating
abrupt breaks in the contours at the midpoint of the vowels. Listeners
decided in a 2AFC task in which phrase the last syllable was longer.
The results showed that despite identical duration of the compared
stimuli, vowels which were manipulated in the second part towards centralized
values (i.e., less peripheral) were systematically considered to be
shorter by the listeners than stimuli without such discontinuities,
and vice versa. However, the influence seems to be distinct from an
overall formant change (without a discontinuity) since a control stimulus
in which the manipulation was performed within the entire vowel was
not perceived as significantly shorter or longer. No effect of F0 manipulations
was observed.
</description>
    </item>
    
    <item>
        <title>An Ultrasound Study of Alveolar and Retroflex Consonants in Arrernte: Stressed and Unstressed Syllables</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0578.PDF</link>
        <description>This study presents ultrasound data from six female speakers of the
Central Australian language Arrernte. We focus on the apical stop contrast,
alveolar /t/ versus retroflex /&amp;#x288;/, which may be considered phonemically
marginal. We compare these sounds in stressed and unstressed position.
Consistent with previous results on this apical contrast, we show that
there are minimal differences between the retroflex and the alveolar
at stop offset; however, at stop onset, the retroflex has a higher
front portion of the tongue, and often a more forward posterior portion
of the tongue. This difference between the alveolar and the retroflex
is particularly remarked in unstressed prosodic context. This result
confirms our previous EPG and EMA results from two of the speakers
in the present study, which showed that the most prototypical retroflex
consonant occurs in the unstressed prosodic position.
</description>
    </item>
    
    <item>
        <title>Reshaping the Transformed LF Model: Generating the Glottal Source from the Waveshape Parameter  R<SUB>d</SUB></title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Kinematic Signatures of Prosody in Lombard Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0722.PDF</link>
        <description>Human spoken interactions are embodied and situated. Better understanding
of the restrictions and affordances this embodiment and situational
awareness has on human speech informs the quest for more natural models
of human-machine spoken interactions. Here we examine the articulatory
realization of communicative meanings expressed through f0 falling
and rising prosodic boundaries in quiet and noisy conditions. Our data
show that 1) the effect of environmental noise is more robustly present
in the post-boundary than the pre-boundary movements, 2) f0 falls and
rises are only weakly differentiated in supra-laryngeal articulation
and differ minimally in their response to noise, 3) individual speakers
find different solutions for achieving the communicative goals, and
4) lip movements are affected by noise and boundary type more than
the tongue movements.
</description>
    </item>
    
    <item>
        <title>What do Finnish and Central Bavarian Have in Common? Towards an Acoustically Based Quantity Typology</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1285.PDF</link>
        <description>The aim of this study was to investigate vowel and consonant quantity
in Finnish, a typical quantity language, and to set up a reference
corpus for a large-scale project studying the diachronic development
of quantity contrasts in German varieties. Although German is not considered
a quantity language, both tense and lax vowels and voiced and voiceless
stops are differentiated by vowel and closure duration, respectively.
The role of these cues, however, has undergone different diachronic
changes in various German varieties. To understand the conditions for
such prosodic changes, the present study investigates the stability
of quantity relations in an undisputed quantity language. To this end,
recordings of words differing in vowel and stop length were obtained
from seven older and six younger L1 Finnish speakers, both in a normal
and a loud voice. We then measured vowel and stop duration and calculated
the vowel to vowel-plus-consonant ratio (a measure known to differentiate
German VC sequences) as well as the geminate-to-singleton ratio. Results
show stability across age groups but variability across speech styles.
Moreover, VC ratios were similar for Finnish and Bavarian German speakers.
We discuss our findings against the background of a typology of vowel
and consonant quantity.
</description>
    </item>
    
    <item>
        <title>Locating Burst Onsets Using SFF Envelope and Phase Information</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1027.PDF</link>
        <description>Bursts are produced by closing the oral tract at a place of articulation
and suddenly releasing the acoustic energy built-up behind the closure
in the tract. The release of energy is an impulse-like behavior, and
it is followed by a short duration of frication. The burst release
is short and mostly weak in nature (compared to sonorant sounds), thus
making it difficult to detect its presence in continuous speech. This
paper attempts to identify burst onsets based on parameters derived
from single frequency filtering (SFF) analysis of speech signals. The
SFF envelope and phase information give good spectral and temporal
resolutions of certain features of the signal. Signal reconstructed
from the SFF phase information is shown to be useful in locating burst
onsets. Entropy and spectral distance parameters from the SFF spectral
envelopes are used to refine the burst onset candidate set. The identified
burst onset locations are compared with manual annotations in the TIMIT
database.
</description>
    </item>
    
    <item>
        <title>A Preliminary Phonetic Investigation of Alphabetic Words in Mandarin Chinese</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0876.PDF</link>
        <description>Chinese words written partly or fully in roman letters have gained
popularity in Mandarin Chinese in the last few decades and an appendix
of such Mandarin Alphabetical Words (MAWs) is included in the authoritative
dictionary of Standard Mandarin. However, no transcription of MAWs
has been provided because it is not clear whether we should keep the
original English pronunciation or transcribe MAWs with Mandarin Pinyin
system. This study aims to investigate the phonetic adaptation of several
most frequent MAWs extracted from the corpus. We recruited eight students
from Shanghai, 18 students from Shandong Province, and one student
from the USA. All the subjects were asked to read both 24 Chinese sentences
embedding the MAWs and all 26 letters of the English alphabet. The
results showed that Letters  A O N T were predominantly pronounced
in Tone 1;  H was often produced with vowel epenthesis after the final
consonant; and  B was usually produced in Tone 2 by Shanghai speakers
and in Tone 4 by Shandong speakers. We conclude that the phonetic adaptation
of MAWs is influenced by the dialects of the speakers, tones of other
Chinese characters in the MAWs, as well as individual preferences.
</description>
    </item>
    
    <item>
        <title>A Quantitative Measure of the Impact of Coarticulation on Phone Discriminability</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1306.PDF</link>
        <description>Acoustic realizations of a given phonetic segment are typically affected
by coarticulation with the preceding and following phonetic context.
While coarticulation has been extensively studied using descriptive
phonetic measurements, little is known about the functional impact
of coarticulation for speech processing. Here, we use DTW-based similarity
defined on raw acoustic features and ABX scores to derive a measure
of the effect of coarticulation on phonetic discriminability. This
measure does not rely on defining segment-specific phonetic cues (formants,
duration, etc.) and can be applied systematically and automatically
to any segment in large scale corpora. We illustrate our method using
stimuli in English and Japanese. We confirm some expected trends, i.e.,
stronger anticipatory than perseveratory coarticulation and stronger
coarticulation for lax/short vowels than for tense/long vowels. We
then quantify for the first time the impact of coarticulation across
different segment types (like vowels and consonants). We discuss how
our metric and its possible extensions can help addressing current
challenges in the systematic study of coarticulation.
</description>
    </item>
    
    <item>
        <title>Sinusoidal Partials Tracking for Singing Analysis Using the Heuristic of the Minimal Frequency and Magnitude Difference</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0017.PDF</link>
        <description>We present a simple heuristic-based Sinusoidal Partial Tracking (PT)
algorithm for singing analysis. Our PT algorithm uses a heuristic of
minimal frequency and magnitude difference to track sinusoidal partials
in the popular music. An Ideal Binary Mask (IBM), which is created
from the ground truth of the singing voice and the music accompaniment,
is used to identify the sound source of the partials. In this justifiable
way, we are able to assess the quality of the partials identified from
the PT algorithm. Using the iKala dataset along with the IBM and BSS
Eval 3.0 as a new method of quantifying the partials quality, the comparative
results show that our PT algorithm can achieve 0.8746 &amp;#126; 1.7029
dB GNSDR gain, compared to two common benchmarks, namely the MQ algorithm
and the SMS-PT algorithm. Thus, our PT algorithm can be considered
as a new benchmark of the PT algorithm used in singing analysis.
</description>
    </item>
    
    <item>
        <title>Audio Scene Classification with Deep Recurrent Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0101.PDF</link>
        <description>We introduce in this work an efficient approach for audio scene classification
using deep recurrent neural networks. An audio scene is firstly transformed
into a sequence of high-level label tree embedding feature vectors.
The vector sequence is then divided into multiple subsequences on which
a deep GRU-based recurrent neural network is trained for sequence-to-label
classification. The global predicted label for the entire sequence
is finally obtained via aggregation of subsequence classification outputs.
We will show that our approach obtains an F1-score of 97.7% on the
LITIS Rouen dataset, which is the largest dataset publicly available
for the task. Compared to the best previously reported result on the
dataset, our approach is able to reduce the relative classification
error by 35.3%.
</description>
    </item>
    
    <item>
        <title>Automatic Time-Frequency Analysis of Echolocation Signals Using the Matched Gaussian Multitaper Spectrogram</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0119.PDF</link>
        <description>High-resolution time-frequency (TF) images of multi-component signals
are of great interest for visualization, feature extraction and estimation.
The matched Gaussian multitaper spectrogram has been proposed to optimally
resolve multi-component transient functions of Gaussian shape. Hermite
functions are used as multitapers and the weights of the different
spectrogram functions are optimized. For a fixed number of multitapers,
the optimization gives the approximate Wigner distribution of the Gaussian
shaped function. Increasing the number of multitapers gives a better
approximation, i.e. a better resolution, but the cross-terms also become
more prominent for close TF components. In this submission, we evaluate
a number of different concentration measures to automatically estimate
the number of multitapers resulting in the optimal spectrogram for
TF images of dolphin echolocation signals. The measures are evaluated
for different multi-component signals and noise levels and a suggestion
of an automatic procedure for optimal TF analysis is given. The results
are compared to other well known TF estimation algorithms and examples
of real data measurements of echolocation signals from a beluga whale
( Delphinapterus leucas) are presented.
</description>
    </item>
    
    <item>
        <title>Classification-Based Detection of Glottal Closure Instants from Speech Signals</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0213.PDF</link>
        <description>In this paper a classification-based method for the automatic detection
of glottal closure instants (GCIs) from the speech signal is proposed.
Peaks in the speech waveforms are taken as candidates for GCI placements.
A classification framework is used to train a classification model
and to classify whether or not a peak corresponds to the GCI. We show
that the detection accuracy in terms of F1 score is 97.27%. In addition,
despite using the speech signal only, the proposed method behaves comparably
to a method utilizing the glottal signal. The method is also compared
with three existing GCI detection algorithms on publicly available
databases.
</description>
    </item>
    
    <item>
        <title>A Domain Knowledge-Assisted Nonlinear Model for Head-Related Transfer Functions Based on Bottleneck Deep Neural Network</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0222.PDF</link>
        <description>Many methods have been proposed for modeling head-related transfer
functions (HRTFs) and yield a good performance level in terms of log-spectral
distortion (LSD). However, most of them utilize linear weighting to
reconstruct or interpolate HRTFs, but not consider the inherent nonlinearity
relationship between the basis function and HRTFs. Motivated by this,
a domain knowledge-assisted nonlinear modeling method is proposed based
on bottleneck features. Domain knowledge is used in two aspects. One
is to generate the input features derived from the solution to sound
wave propagation equation at the physical level, and the other is to
design the loss function for model training based on the knowledge
of objective evaluation criterion, i.e., LSD. Furthermore, with utilizing
the strong representation ability of the bottleneck features, the nonlinear
model has the potential to achieve a more accurate mapping. The objective
and subjective experimental results show that the proposed method gains
less LSD when compared with linear model, and the interpolated HRTFs
can generate a similar perception to those of the database.
</description>
    </item>
    
    <item>
        <title>Laryngeal Articulation During Trumpet Performance: An Exploratory Study</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0315.PDF</link>
        <description>Music teacher&amp;#8217;s reports suggest that the respiratory function
and laryngeal control in wind instruments, stimulate muscular tension
of the involved anatomical structure. However, the physiology and acoustics
of the larynx during trumpet playing has seldom been studied. Therefore,
the current paper describes the laryngeal articulation during trumpet
performance with biomedical signals and auditory perception. The activation
of laryngeal musculature of six professional trumpeters when playing
a standard musical passage was analysed using audio, electroglottography
(EGG), oxygen saturation and heart rate signals. Two University trumpet
teachers listened to the audio recordings, to evaluate the participants&amp;#8217;
laryngeal effort (answers on a 100 mm Visual-Analogue-Scale (VAS):
0 &amp;#8220;no perceived effort&amp;#8221;; 100 &amp;#8220;extreme effort&amp;#8221;).
Correlations between parameters extracted from the EGG data and the
perception of the audio stimuli by the teachers were explored. Two
hundred and fifty laryngeal articulations, where raising of the larynx
and muscular effort were observed, were annotated and analysed. No
correlation between the EGG data and the auditory evaluation was observed.
However, both teachers perceived the laryngeal effort (VAS mean scores
= 61&amp;#177;14). Our findings show that EGG and auditory perception data
can provide new insights into laryngeal articulation and breathing
control that are key to low muscular tension.
</description>
    </item>
    
    <item>
        <title>Matrix of Polynomials Model Based Polynomial Dictionary Learning Method for Acoustic Impulse Response Modeling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0395.PDF</link>
        <description>We study the problem of dictionary learning for signals that can be
represented as polynomials or polynomial matrices, such as convolutive
signals with time delays or acoustic impulse responses. Recently, we
developed a method for polynomial dictionary learning based on the
fact that a polynomial matrix can be expressed as a polynomial with
matrix coefficients, where the coefficient of the polynomial at each
time lag is a scalar matrix. However, a polynomial matrix can be also
equally represented as a matrix with polynomial elements. In this paper,
we develop an alternative method for learning a polynomial dictionary
and a sparse representation method for polynomial signal reconstruction
based on this model. The proposed methods can be used directly to operate
on the polynomial matrix without having to access its coefficients
matrices. We demonstrate the performance of the proposed method for
acoustic impulse response modeling.
</description>
    </item>
    
    <item>
        <title>Acoustic Scene Classification Using a CNN-SuperVector System Trained with Auditory and Spectrogram Image Features</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0431.PDF</link>
        <description>Enabling smart devices to infer about the environment using audio signals
has been one of the several long-standing challenges in machine listening.
The availability of public-domain datasets, e.g., Detection and Classification
of Acoustic Scenes and Events (DCASE) 2016, enabled researchers to
compare various algorithms on standard predefined tasks. Most of the
current best performing individual acoustic scene classification systems
utilize different spectrogram image based features with a Convolutional
Neural Network (CNN) architecture. In this study, we first analyze
the performance of a state-of-the-art CNN system for different auditory
image and spectrogram features, including Mel-scaled, logarithmically
scaled, linearly scaled filterbank spectrograms, and Stabilized Auditory
Image (SAI) features. Next, we benchmark an MFCC based Gaussian Mixture
Model (GMM) SuperVector (SV) system for acoustic scene classification.
Finally, we utilize the activations from the final layer of the CNN
to form a SuperVector (SV) and use them as feature vectors for a Probabilistic
Linear Discriminative Analysis (PLDA) classifier. Experimental evaluation
on the DCASE 2016 database demonstrates the effectiveness of the proposed
CNN-SV approach compared to conventional CNNs with a fully connected
softmax output layer. Score fusion of individual systems provides up
to 7% relative improvement in overall accuracy compared to the CNN
baseline system.
</description>
    </item>
    
    <item>
        <title>An Environmental Feature Representation for Robust Speech Recognition and for Environment Identification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0485.PDF</link>
        <description>In this paper we investigate environment feature representations, which
we refer to as e-vectors, that can be used for environment adaption
in automatic speech recognition (ASR), and for environment identification.
Inspired by the fact that i-vectors in the total variability space
capture both speaker and channel environment variability, our proposed
e-vectors are extracted from i-vectors. Two extraction methods are
proposed: one is via linear discriminant analysis (LDA) projection,
and the other via a bottleneck deep neural network (BN-DNN). Our evaluations
show that by augmenting DNN-HMM ASR systems with the proposed e-vectors
for environment adaptation, ASR performance is significantly improved.
We also demonstrate that the proposed e-vector yields promising results
on environment identification.
</description>
    </item>
    
    <item>
        <title>Attention and Localization Based on a Deep Convolutional Recurrent Model for Weakly Supervised Audio Tagging</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0486.PDF</link>
        <description>Audio tagging aims to perform multi-label classification on audio chunks
and it is a newly proposed task in the Detection and Classification
of Acoustic Scenes and Events 2016 (DCASE 2016) challenge. This task
encourages research efforts to better analyze and understand the content
of the huge amounts of audio data on the web. The difficulty in audio
tagging is that it only has a chunk-level label without a frame-level
label. This paper presents a weakly supervised method to not only predict
the tags but also indicate the temporal locations of the occurred acoustic
events. The attention scheme is found to be effective in identifying
the important frames while ignoring the unrelated frames. The proposed
framework is a deep convolutional recurrent model with two auxiliary
modules: an attention module and a localization module. The proposed
algorithm was evaluated on the Task 4 of DCASE 2016 challenge. State-of-the-art
performance was achieved on the evaluation set with equal error rate
(EER) reduced from 0.13 to 0.11, compared with the convolutional recurrent
baseline system.
</description>
    </item>
    
    <item>
        <title>An Audio Based Piano Performance Evaluation Method Using Deep Neural Network Based Acoustic Modeling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0866.PDF</link>
        <description>In this paper, we propose an annotated piano performance evaluation
dataset with 185 audio pieces and a method to evaluate the performance
of piano beginners based on their audio recordings. The proposed framework
includes three parts: piano key posterior probability extraction, Dynamic
Time Warping (DTW) based matching and performance score regression.
First, a deep neural network model is trained to extract 88 dimensional
piano key features from Constant-Q Transform (CQT) spectrum. The proposed
acoustic model shows high robustness to the recording environments.
Second, we employ the DTW algorithm on the high-level piano key feature
sequences to align the input with the template. Upon the alignment,
we extract multiple global matching features that could reflect the
similarity between the input and the template. Finally, we apply linear
regression upon these matching features with the scores annotated by
expertise in training data to estimate performance scores for test
audio. Experimental results show that our automatic evaluation method
achieves 2.64 average absolute score error in score range from 0 to
100, and 0.73 average correlation coefficient on our in-house collected
YCU-MPPE-II dataset.
</description>
    </item>
    
    <item>
        <title>Music Tempo Estimation Using Sub-Band Synchrony</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1000.PDF</link>
        <description>Tempo estimation aims at estimating the pace of a musical piece measured
in beats per minute. This paper presents a new tempo estimation method
that utilizes coherent energy changes across multiple frequency sub-bands
to identify the onsets. A new measure, called the sub-band synchrony,
is proposed to detect and quantify the coherent amplitude changes across
multiple sub-bands. Given a musical piece, our method first detects
the onsets using the sub-band synchrony measure. The periodicity of
the resulting onset curve, measured using the autocorrelation function,
is used to estimate the tempo value. The performance of the sub-band
synchrony based tempo estimation method is evaluated on two music databases.
Experimental results indicate a reasonable improvement in performance
when compared to conventional methods of tempo estimation.
</description>
    </item>
    
    <item>
        <title>A Transfer Learning Based Feature Extractor for Polyphonic Sound Event Detection Using Connectionist Temporal Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>A Note Based Query By Humming System Using Convolutional Neural Network</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Unsupervised Filterbank Learning Using Convolutional Restricted Boltzmann Machine for Environmental Sound Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0831.PDF</link>
        <description>In this paper, we propose to use Convolutional Restricted Boltzmann
Machine (ConvRBM) to learn filterbank from the raw audio signals. ConvRBM
is a generative model trained in an unsupervised way to model the audio
signals of arbitrary lengths. ConvRBM is trained using annealed dropout
technique and parameters are optimized using Adam optimization. The
subband filters of ConvRBM learned from the ESC-50 database resemble
Fourier basis in the mid-frequency range while some of the low-frequency
subband filters resemble Gammatone basis. The auditory-like filterbank
scale is nonlinear w.r.t. the center frequencies of the subband filters
and follows the standard auditory scales. We have used our proposed
model as a front-end for the Environmental Sound Classification (ESC)
task with supervised Convolutional Neural Network (CNN) as a back-end.
Using CNN classifier, the ConvRBM filterbank (ConvRBM-BANK) and its
score-level fusion with the Mel filterbank energies (FBEs) gave an
absolute improvement of 10.65%, and 18.70% in the classification accuracy,
respectively, over FBEs alone on the ESC-50 database. This shows that
the proposed ConvRBM filterbank also contains highly complementary
information over the Mel filterbank, which is helpful in the ESC task.
</description>
    </item>
    
    <item>
        <title>Novel Shifted Real Spectrum for Exact Signal Reconstruction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1422.PDF</link>
        <description>Retrieval of the phase of a signal is one of the major problems in
signal processing. For an exact signal reconstruction, both magnitude,
and phase spectrum of the signal is required. In many speech-based
applications, only the magnitude spectrum is processed and the phase
is ignored, which leads to degradation in the performance. Here, we
propose a novel technique that enables the reconstruction of the speech
signal from magnitude spectrum only. We consider the even-odd part
decomposition of a causal sequence and process only on the real part
of the DTFT of the signal. We propose the shifting of the real part
of DTFT of the sequence to make it non-negative. By adding a constant
of sufficient value to the real part of the DTFT, the exact signal
reconstruction is possible from the magnitude or power spectrum alone.
Moreover, we have compared our proposed approach with recently proposed
phase retrieval method from magnitude spectrum of the Causal Delta
Dominant (CDD) signal. We found that the method of phase retrieval
from CDD signal and proposed method are identical under certain approximation.
However, proposed method involves the less computational cost for the
exact processing of the signal.
</description>
    </item>
    
    <item>
        <title>Manual and Automatic Transcriptions in Dementia Detection from Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>An Affect Prediction Approach Through Depression Severity Parameter Incorporation in Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0120.PDF</link>
        <description>Humans use emotional expressions to communicate their internal affective
states. These behavioral expressions are often multi-modal (e.g. facial
expression, voice and gestures) and researchers have proposed several
schemes to predict the latent affective states based on these expressions.
The relationship between the latent affective states and their expression
is hypothesized to be affected by several factors; depression disorder
being one of them. Despite a wide interest in affect prediction, and
several studies linking the effect of depression on affective expressions,
only a limited number of affect prediction models account for the depression
severity. In this work, we present a novel scheme that incorporates
depression severity as a parameter in Deep Neural Networks (DNNs).
In order to predict affective dimensions for an individual at hand,
our scheme alters the DNN activation function based on the subject&amp;#8217;s
depression severity. We perform experiments on affect prediction in
two different sessions of the Audio-Visual Depressive language Corpus,
which involves patients with varying degree of depression. Our results
show improvements in arousal and valence prediction on both the sessions
using the proposed DNN modeling. We also present analysis of the impact
of such an alteration in DNNs during training and testing.
</description>
    </item>
    
    <item>
        <title>Cross-Database Models for the Classification of Dysarthria Presence</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0216.PDF</link>
        <description>Dysarthria is a motor speech disorder that impacts verbal articulation
and co-ordination, resulting in slow, slurred and imprecise speech.
Automated classification of dysarthria subtypes and severities could
provide a useful clinical tool in assessing the onset and progress
in treatment. This study represents a pilot project to train models
to detect the presence of dysarthria in continuous speech. Subsets
of the Universal Access Research Dataset (UA-Speech) and the Atlanta
Motor Speech Disorders Corpus (AMSDC) database were utilized in a cross-database
training strategy (training on UA-Speech / testing on AMSDC) to distinguish
speech with and without dysarthria. In addition to traditional spectral
and prosodic features, the current study also includes features based
on the Teager Energy Operator (TEO) and the glottal waveform. Baseline
results on the UA-Speech dataset maximize word- and participant-level
accuracies at 75.3% and 92.9% using prosodic features. However, the
cross-training of UA-Speech tested on the AMSDC maximize word- and
participant-level accuracies at 71.3% and 90% based on a TEO feature.
The results of this pilot study reinforce consideration of dysarthria
subtypes in cross-dataset training as well as highlight additional
features that may be sensitive to the presence of dysarthria in continuous
speech.
</description>
    </item>
    
    <item>
        <title>Acoustic Evaluation of Nasality in Cerebellar Syndromes</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0381.PDF</link>
        <description>Although previous studies have reported the occurrence of velopharyngeal
incompetence connected with ataxic dysarthria, there is a lack of evidence
related to nasality assessment in cerebellar disorders. This is partly
due to the limited reliability of challenging analyses and partly due
to nasality being a less pronounced manifestation of ataxic dysarthria.
Therefore, we employed 1/3-octave spectra analysis as an objective
measurement of nasality disturbances. We analyzed 20 subjects with
multiple system atrophy (MSA), 13 subjects with cerebellar ataxia (CA),
20 subjects with multiple sclerosis (MS) and 20 healthy (HC) speakers.
Although we did not detect the presence of hypernasality, our results
showed increased nasality fluctuation in 65% of MSA, 43% of CA and
30% of MS subjects compared to 15% of HC speakers, suggesting inconsistent
velopharyngeal motor control. Furthermore, we found a statistically
significant difference between MSA and HC participants (p&amp;#60;0.001),
and significant correlation between the natural history cerebellar
subscore and neuroprotection in Parkinson plus syndromes &amp;#8212; Parkinson
plus scale and nasality fluctuations in MSA (r=0.51, p&amp;#60;0.05). In
conclusion, acoustic analysis showed an increased presence of abnormal
nasality fluctuations in all ataxic groups and revealed that nasality
fluctuation is associated with distortion of cerebellar functions.
</description>
    </item>
    
    <item>
        <title>Emotional Speech of Mentally and Physically Disabled Individuals: Introducing the EmotAsS Database and First Findings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0409.PDF</link>
        <description>The automatic recognition of emotion from speech is a mature research
field with a large number of publicly available corpora. However, to
the best of the authors knowledge, none of these datasets consist solely
of emotional speech samples from individuals with mental, neurological
and/or physical disabilities. Yet, such individuals could benefit from
speech-based assistive technologies to enhance their communication
with their environment and to manage their daily work process. With
the aim of advancing these technologies, we fill this void in emotional
speech resources by introducing the EmotAsS (Emotional Sensitivity
Assistance System for People with Disabilities) corpus consisting of
spontaneous emotional German speech data recorded from 17 mentally,
neurologically and/or physically disabled participants in their daily
work environment, resulting in just under 11 hours of total speech
time and featuring approximately 12.7 k utterances after segmentation.
Transcription was performed and labelling was carried out in seven
emotional categories, as well as for the intelligibility of the speaker.
We present a set of baseline results, based on using standard acoustic
and linguistic features, for arousal and valence emotion recognition.
</description>
    </item>
    
    <item>
        <title>Phonological Markers of Oxytocin and MDMA Ingestion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0621.PDF</link>
        <description>Speech data has the potential to become a powerful tool to provide
quantitative information about emotion beyond that achieved by subjective
assessments. Based on this concept, we investigate the use of speech
to identify effects in subjects under the influence of two different
drugs: Oxytocin (OT) and 3,4-methylenedioxymethamphetamine (MDMA),
also known as ecstasy. We extract a set of informative phonological
features that can characterize emotion. Then, we perform classification
to detect if the subject is under the influence of a drug. Our best
results show low error rates of 13% and 17% for the subject classification
of OT and MDMA vs. placebo, respectively. We also analyze the performance
of the features to differentiate the two levels of MDMA doses, obtaining
an error rate of 19%. The results indicate that subtle emotional changes
can be detected in the context of drug use.
</description>
    </item>
    
    <item>
        <title>An Avatar-Based System for Identifying Individuals Likely to Develop Dementia</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0690.PDF</link>
        <description>This paper presents work on developing an automatic dementia screening
test based on patients&amp;#8217; ability to interact and communicate &amp;#8212;
a highly cognitively demanding process where early signs of dementia
can often be detected. Such a test would help general practitioners,
with no specialist knowledge, make better diagnostic decisions as current
tests lack specificity and sensitivity. We investigate the feasibility
of basing the test on conversations between a &amp;#8216;talking head&amp;#8217;
(avatar) and a patient and we present a system for analysing such conversations
for signs of dementia in the patient&amp;#8217;s speech and language. Previously
we proposed a semi-automatic system that transcribed conversations
between patients and neurologists and extracted conversation analysis
style features in order to differentiate between patients with progressive
neurodegenerative dementia (ND) and functional memory disorders (FMD).
Determining who talks when in the conversations was performed manually.
In this study, we investigate a fully automatic system including speaker
diarisation, and the use of additional acoustic and lexical features.
Initial results from a pilot study are presented which shows that the
avatar conversations can successfully classify ND/FMD with around 91%
accuracy, which is in line with previous results for conversations
that were led by a neurologist.
</description>
    </item>
    
    <item>
        <title>Cross-Domain Classification of Drowsiness in Speech: The Case of Alcohol Intoxication and Sleep Deprivation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1015.PDF</link>
        <description>In this work, we study the drowsy state of a speaker, induced by alcohol
intoxication or sleep deprivation. In particular, we investigate the
coherence between the two pivotal causes of drowsiness, as featured
in the Intoxication and Sleepiness tasks of the INTERSPEECH Speaker
State Challenge. In this way, we aim to exploit the interrelations
between these different, yet highly correlated speaker states, which
need to be reliably recognised in safety and security critical environments.
To this end, we perform cross-domain classification of alcohol intoxication
and sleepiness, thus leveraging the acoustic similarities of these
speech phenomena for transfer learning. Further, we conducted in-depth
feature analysis to quantitatively assess the task relatedness and
to determine the most relevant features for both tasks. To test our
methods in realistic contexts, we use the Alcohol Language Corpus and
the Sleepy Language Corpus containing in total 60 hours of genuine
intoxicated and sleepy speech. In the result, cross-domain classification
combined with feature selection yields up to 60.3% unweighted average
recall, which is significantly above-chance (50%) and highly notable
given the mismatch in the training and validation data. Finally, we
show that an effective, general drowsiness classifier can be obtained
by aggregating the training data from both domains.
</description>
    </item>
    
    <item>
        <title>Depression Detection Using Automatic Transcriptions of De-Identified Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1201.PDF</link>
        <description>Depression is a mood disorder that is usually addressed by outpatient
treatments in order to favour patient&amp;#8217;s inclusion in society.
This leads to a need for novel automatic tools exploiting speech processing
approaches that can help to monitor the emotional state of patients
via telephone or the Internet. However, the transmission, processing
and subsequent storage of such sensitive data raises several privacy
concerns. Speech de-identification can be used to protect the patients&amp;#8217;
identity. Nevertheless, these techniques modify the speech signal,
eventually affecting the performance of depression detection approaches
based on either speech characteristics or automatic transcriptions.
This paper presents a study on the influence of speech de-identification
when using transcription-based approaches for depression detection.
To this effect, a system based on the global vectors method for natural
language processing is proposed. In contrast to previous works, two
main sources of nuisance have been considered: the de-identification
process itself and the transcription errors introduced by the automatic
recognition of the patients&amp;#8217; speech. Experimental validation
on the DAIC-WOZ corpus reveals very promising results, obtaining only
a slight performance degradation with respect to the use of manual
transcriptions.
</description>
    </item>
    
    <item>
        <title>An N-Gram Based Approach to the Automatic Diagnosis of Alzheimer&amp;#8217;s Disease from Spoken Language</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1572.PDF</link>
        <description>Alzheimer&amp;#8217;s disease (AD) is the most common cause of dementia
and affects wide parts of the elderly population. Since there exists
no cure for this illness, it is of particular interest to develop reliable
and easy-to-use diagnostic methods to alleviate its effects. Speech
can be a useful indicator to reach this goal. We propose a purely statistical
approach towards the automatic diagnosis of AD which is solely based
on n-gram models with subsequent evaluation of the perplexity and does
not incorporate any further linguistic features. Hence, it works independently
of a concrete language. We evaluate our approach on the DementiaBank
which contains spontaneous speech of test subjects describing a picture.
Using the Equal-Error-Rate as classification threshold, we achieve
an accuracy of 77.1%. In addition to that, we studied the correlation
between the calculated perplexities and the Mini-Mental State Examination
(MMSE) scores of the test subjects. While there is little correlation
for the healthy control group, a higher correlation could be found
when considering the demented speakers. This makes it reasonable to
conclude that our approach reveals some of the cognitive limitations
of AD patients and can help to better diagnose the disease based on
speech.
</description>
    </item>
    
    <item>
        <title>Exploiting Intra-Annotator Rating Consistency Through Copeland&amp;#8217;s Method for Estimation of Ground Truth Labels in Couples&amp;#8217; Therapy</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1599.PDF</link>
        <description>Behavioral and mental health research and its clinical applications
widely rely on quantifying human behavioral expressions. This often
requires human-derived behavioral annotations, which tend to be noisy,
especially when the psychological objects of interest are latent and
subjective in nature. This paper focuses on exploiting multiple human
annotations toward improving reliability of the ensemble decision,
by creating a ranking of the evaluated objects. To create this ranking,
we employ an adapted version of Copeland&amp;#8217;s counting method, which
results in robust inter-annotator rankings and agreement. We use a
simple mapping between the ranked objects and the scale of evaluation,
which preserves the original distribution of ratings, based on maximum
likelihood estimation. We apply the algorithm to ratings that lack
a ground truth. Therefore, we assess our algorithm in two ways: (1)
by corrupting the annotations with different distributions of noise,
and computing the inter-annotator agreement between the ensemble estimates
derived from the original and corrupted data using Krippendorff&amp;#8217;s
&amp;#945;; and (2) by replacing one annotator at a time with the ensemble
estimate. Our results suggest that the proposed method provides a robust
alternative that suffers less from individual annotator preferences/biases
and scale misuse.
</description>
    </item>
    
    <item>
        <title>Rhythmic Characteristics of Parkinsonian Speech: A Study on Mandarin and Polish</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0850.PDF</link>
        <description>Previous studies on Italian speech showed that the percentage of vocalic
portion in the utterance (%V) and the duration of the interval between
two consecutive vowel onset points (VtoV) were larger for parkinsonian
(PD) than for healthy controls (HC). Especially, the values of %V were
distinctly separated between PD and HC. The present study aimed to
further test the finding on Mandarin and Polish. Twenty-five Mandarin
speakers (13 PD and 12 HC matched on age) and thirty-one Polish speakers
(18 PD and 13 HC matched on age) read aloud a passage of story. The
recorded speeches were segmented into vocalic and consonantal intervals,
and then %V and VtoV were calculated. For both languages, VtoV overlapped
between HC and PD. For Polish, %V was distinctly higher in PD than
in HC, while for Mandarin there was no significant difference. It suggests
that %V could be used for automatic diagnosis of PD for Italian and
Polish, but not for Mandarin. The effectiveness of the rhythmic metric
appears to be language-dependent, varying with the rhythmic typology
of the language.
</description>
    </item>
    
    <item>
        <title>Trisyllabic Tone 3 Sandhi Patterns in Mandarin Produced by Cantonese Speakers</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0291.PDF</link>
        <description>The third tone sandhi in Mandarin is a well-studied rule, where a Tone
3 followed by another Tone 3 is changed as a rising tone, similar to
Tone 2. This Tone 3 sandhi rule is straightforward in disyllabic words,
which is phonetically driven for the ease of production. In three or
more than three syllables with Tone 3, however, the Tone 3 sandhi application
is more complicated and involves both the prosodic and morph-syntactic
domains, which makes it difficult for L2 learners. This study aims
to understand how L2 learners with another tone language experience
could master the Mandarin Tone 3 sandhi rule. Specifically, the study
investigates the production of Tone 3 sandhi in trisyllabic Mandarin
words by Cantonese speakers. In the current study, 30 Cantonese speakers
were requested to produce 15 trisyllabic words (&amp;#8220;1+[2+3]&amp;#8221;
and &amp;#8220;[1+2]+3&amp;#8221; sandhi patterns) and 5 hexasyllabic sentences
with Tone 3 in sequences. The analyses of results center on three major
types of error patterns: overgeneralization, under application, and
combination. The findings are discussed with regard to the phono-syntactic
interactions of Tone 3 sandhi at the lexical and phrasal levels as
well as the influence of the Cantonese tonal system.
</description>
    </item>
    
    <item>
        <title>Intonation of Contrastive Topic in Estonian</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0840.PDF</link>
        <description>Contrastive topic is an information structural category that is usually
associated with a specific intonation, which tends to be similar across
languages (a rising pitch accent). The aim of the present study is
to examine whether this also true of Estonian. Three potential prosodic
correlates of contrastive topics are examined: marking with a particular
pitch accent type, an emphatic realization of the pitch accent, and
a following prosodic boundary. With respect to pitch accent types,
it is found that only two subjects out of eight distinguish sentences
with a contrastive topic from other types of information structure;
the contour bears resemblance to contrastive topic intonation in other
languages (consisting of an H* accent on the contrastive topic and
an HL* accent on the focus), but is not restricted to sentences with
contrastive topics. A more consistent correlate turns out to be an
emphatic realization of the pitch accent carried by the contrastive
topic constituent. No evidence is found of a tendency to produce contrastive
topics as separate prosodic phrases.
</description>
    </item>
    
    <item>
        <title>Reanalyze Fundamental Frequency Peak Delay in Mandarin</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1235.PDF</link>
        <description>In Mandarin, Fundamental Frequency (F0) peak delay has been reported
to occur frequently in the rising (R) tone or high (H) tone succeeding
by a low (L) tone. Its occurrence was ascribed to articulatory constraints
within a conflicting tonal context: a high offset target followed by
a low onset target. To further examine the underlying mechanism of
the phenomenon, the current study tests the possibility that valley
delay, as opposed to peak delay, may occur in an L+H tonal context;
and peak or valley delay may also occur within a compatible tonal context
where adjacent tonal values are identical or similar. An experiment
was done on Annotated Speech Corpus of Chinese Discourse to investigate
the frequency of occurrence and amount of peak and valley delay. The
results indicated that: F0 peak and valley delay frequently occurred
in both conflicting and compatible tonal contexts; the phenomenon was
found extensively in R tone and F (falling) tone, but barely in H tone
and L tone. The findings suggest that while peak or valley delay is
partially due to articulatory constraints in certain tonal contexts,
the speakers&amp;#8217; active effort-distribution strategy based on economical
principle is also behind the phenomenon.
</description>
    </item>
    
    <item>
        <title>How Does the Absence of Shared Knowledge Between Interlocutors Affect the Production of French Prosodic Forms?</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1430.PDF</link>
        <description>We examine the hypothesis that modelling the addressee in spoken interaction
affects the production of prosodic forms by the speaker. This question
was tested in an interactive paradigm that enabled us to measure prosodic
variations at two levels: the global/acoustic level and the phonological
one. We used a semi-spontaneous task in which French speakers gave
instructions to addressees about where to place a cross between different
objects (e.g.,  Tu mets la croix entre la souris bordeau et la maison
bordeau; &amp;#8216;You put the cross between the red mouse and the red
house&amp;#8217;). Each trial was composed of two noun-adjective fragments
and the target was the second fragment. We manipulated (i) whether
the two interlocutors shared or didn&amp;#8217;t share the same objects
and (ii) the informational status of targets to obtain variations in
abstract prosodic phrasing. We found that the absence of shared knowledge
between interlocutors affected the speaker&amp;#8217;s production of prosodic
forms at the global/acoustic level (i.e., pitch range and speech rate)
but not at the phonological one (i.e., prosodic phrasing). These results
are consistent with a mechanism in which global prosodic variations
are influenced by audience design because they reflect the way that
speakers help addressees to understand speech.
</description>
    </item>
    
    <item>
        <title>Three Dimensions of Sentence Prosody and Their (Non-)Interactions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1500.PDF</link>
        <description>Prosody simultaneously encodes different kinds of information, including
the type of speech act of an utterance (e.g., falling declarative vs.
rising interrogative intonational tunes), the location of semantic
focus (via prosodic prominence), and syntactic constituent structure
(via prosodic phrasing). The syntactic/ semantic functional dimensions
(speech act, focus, constituency) are orthogonal to each other, but
to which extent their prosodic correlates (tune, prominence, phrasing)
are remains controversial. This paper takes a &amp;#8216;bottom up&amp;#8217;
approach to test for interactions, and reports evidence that contrary
to many current theories of sentence intonation, the cues to the three
dimensions are often orthogonal where interactions are predicted.
</description>
    </item>
    
    <item>
        <title>Using Prosody to Classify Discourse Relations</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0710.PDF</link>
        <description>This work aims to explore the correlation between the discourse structure
of a spoken monologue and its prosody by predicting discourse relations
from different prosodic attributes. For this purpose, a corpus of semi-spontaneous
monologues in English has been automatically annotated according to
the Rhetorical Structure Theory, which models coherence in text via
rhetorical relations. From corresponding audio files, prosodic features
such as pitch, intensity, and speech rate have been extracted from
different contexts of a relation. Supervised classification tasks using
Support Vector Machines have been performed to find relationships between
prosodic features and rhetorical relations. Preliminary results show
that intensity combined with other features extracted from intra- and
intersegmental environments is the feature with the highest predictability
for a discourse relation. The prediction of rhetorical relations from
prosodic features and their combinations is straightforwardly applicable
to several tasks such as speech understanding or generation. Moreover,
the knowledge of how rhetorical relations should be marked in terms
of prosody will serve as a basis to improve speech synthesis applications
and make voices sound more natural and expressive.
</description>
    </item>
    
    <item>
        <title>Canonical Correlation Analysis and Prediction of Perceived Rhythmic Prominences and Pitch Tones in Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1585.PDF</link>
        <description>Speech prosody encodes information about language and communicative
intent as well as speaker identity and state. Consequently, a host
of speech technologies could benefit from increased understanding of
prosodic phenomena and corresponding acoustics. A recently developed
comprehensive prosodic transcription system called RaP (Rhythm-and-Pitch)
annotates both perceived rhythmic prominences and pitch tones in speech.
Using RaP-annotated speech corpora, the present work analyzes relationships
between perceived prosodic events and acoustic features including syllable
duration and novel measures of intensity and fundamental frequency.
Canonical Correlation Analysis (CCA) reveals two dominant prosodic
dimensions relating the acoustic features and RaP annotations. The
first captures perceived prosodic emphasis of syllables indicated by
strong metrical beats and significant pitch variability (i.e. presence
of either high or low pitch tones). Acoustically, this dimension is
described most by syllable duration followed by the mean intensity
and fundamental frequency measures. The second CCA dimension then primarily
discriminates pitch tone level (high versus low), indicated mainly
by the mean fundamental frequency measure. Finally, within a leave-one-out
cross-validation framework, RaP prosodic events are well-predicted
from acoustic features (AUC between 0.78 and 0.84). Future work will
exploit automated RaP labelling in contexts ranging from language learning
to neurological disorder recognition.
</description>
    </item>
    
    <item>
        <title>Evaluation of Spectral Tilt Measures for Sentence Prominence Under Different Noise Conditions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1237.PDF</link>
        <description>Spectral tilt has been suggested to be a correlate of prominence in
speech, although several studies have not replicated this empirically.
This may be partially due to the lack of a standard method for tilt
estimation from speech, rendering interpretations and comparisons between
studies difficult. In addition, little is known about the performance
of tilt estimators for prominence detection in the presence of noise.
In this work, we investigate and compare several standard tilt measures
on quantifying prominence in spoken Dutch and under different levels
of additive noise. We also compare these measures with other acoustic
correlates of prominence, namely, energy, F0, and duration. Our results
provide further empirical support for the finding that tilt is a systematic
correlate of prominence, at least in Dutch, even though energy, F0,
and duration appear still to be more robust features for the task.
In addition, our results show that there are notable differences between
different tilt estimators in their ability to discriminate prominent
words from non-prominent ones in different levels of noise.
</description>
    </item>
    
    <item>
        <title>Creaky Voice as a Function of Tonal Categories and Prosodic Boundaries</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1578.PDF</link>
        <description>This study looks into the distribution of creaky voice in Mandarin
in continuous speech. A creaky voice detector was used to automatically
detect the appearance of creaky voice in a large-scale Mandarin corpus
(Sinica COSPRO corpus). As the prosodic information has been annotated
in the corpus, we were able to look at the distribution of creaky voice
as a function of the interaction between tone and prosodic structures.
As expected, among the five tonal categories (four lexical tones and
one neutral tone), creaky voice is most likely to occur with Tone 3
and the neutral tone, followed by Tone 2 and Tone 4. Prosodic boundaries
also play important roles, as the likelihood of creak increases when
the prosodic boundaries are larger, regardless of the tonal categories.
It is also confirmed that the pitch range for the occurrence of creaky
voice is 110 Hz for male speakers and 170 Hz for female speakers, consistent
with previous small-scale studies. Finally, male speakers have a higher
overall rate of creaky voice than female speakers. Altogether, this
study validates the hypotheses from previous studies, and provides
a better understanding of voice-source variation in different prosodic
conditions.
</description>
    </item>
    
    <item>
        <title>The Acoustics of Word Stress in Czech as a Function of Speaking Style</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>What You See is What You Get Prosodically Less &#8212; Visibility Shapes Prosodic Prominence Production in Spontaneous Interaction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Focus Acoustics in Mandarin Nominals</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1167.PDF</link>
        <description>In addition to deciding what to say, interlocutors have to decide how
to say it. One of the important tasks of linguists is then to model
how differences in acoustic patterns influence the interpretation of
a sentence. In light of previous studies on how prosodic structure
convey discourse-level of information in a sentence, this study makes
use of a speech production experiment to investigate how expressions
related to different information packaging, such as information focus,
corrective focus, and old information, are prosodically realized within
a complex nominal. Special attention was paid to the sequence of &amp;#8220;numeral-classifier-noun&amp;#8221;
in Mandarin, which consists of closely related sub-syntactic units
internally, and provides a phonetically controlled environment comparable
to previous phonetic studies on focus prominence at the sentential
level. The result shows that a multi-dimensional strategy is used in
focus-marking, and that focus prosody is sensitive to the size of focus
domain and is observable in various lexical tonal environments in Mandarin.
</description>
    </item>
    
    <item>
        <title>Exploring Multidimensionality: Acoustic and Articulatory Correlates of Swedish Word Accents</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>The Perception of English Intonation Patterns by German L2 Speakers of English</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1279.PDF</link>
        <description>Previous research suggests that intonation is a particularly challenging
aspect of L2 speech learning. While most research focuses on speech
production, we widen the focus and study the perception of intonation
by L2 learners. We investigate whether advanced German learners of
English have knowledge of the appropriate English intonation patterns
in a narrative context with different sentence types (e.g. statements,
questions). The results of a tonal pattern selection task indicate
that learners (n=20) performed similar to British English controls
(n=25) for some sentence types (e.g. statements, yes/no-questions),
but performed significantly worse than the control group in the case
of open and closed tag questions and the expression of sarcasm. The
results can be explained by the fact that tag questions are the only
sentence type investigated that does not exist in the learners&amp;#8217;
L1, and sarcasm is not represented syntactically. This suggests that
L1 influence can partly account for why some intonation patterns are
more challenging than others, and that contextualized knowledge of
the intonation patterns of the target language rather than knowledge
of intonation patterns in isolation is crucial for the successful L2
learning of intonation.
</description>
    </item>
    
    <item>
        <title>The Perception of Emotions in Noisified Nonsense Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0104.PDF</link>
        <description>Noise pollution is part of our daily life, affecting millions of people,
particularly those living in urban environments. Noise alters our perception
and decreases our ability to understand others. Considering this, speech
perception in background noise has been extensively studied, showing
that especially white noise can damage listener perception. However,
the perception of emotions in noisified speech has not been explored
with as much depth. In the present study, we use artificial background
noise conditions, by applying noise to a subset of the GEMEP corpus
(emotions expressed in nonsense speech). Noises were at varying intensities
and &amp;#8216;colours&amp;#8217;; white, pink, and brownian. The categorical
and dimensional perceptual test was completed by 26 listeners. The
results indicate that background noise conditions influence the perception
of emotion in speech &amp;#8212; pink noise most, brownian least. Worsened
perception invokes higher confusion, especially with sadness, an emotion
with less pronounced prosodic characteristics. Yet, all this does not
lead to a break-down of the &amp;#8216;cognitive-emotional space&amp;#8217;
in a Non-metric MultiDimensional Scaling representation. The gender
of speakers and the cultural background of listeners do not seem to
play a role.
</description>
    </item>
    
    <item>
        <title>Attention Networks for Modeling Behaviors in Addiction Counseling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0218.PDF</link>
        <description>In psychotherapy interactions there are several desirable and undesirable
behaviors that give insight into the efficacy of the counselor and
the progress of the client. It is important to be able to identify
when these target behaviors occur and what aspects of the interaction
signal their occurrence. Manual observation and annotation of these
behaviors is costly and time intensive. In this paper, we use long
short term memory networks equipped with an attention mechanism to
process transcripts of addiction counseling sessions and predict prominent
counselor and client behaviors. We demonstrate that this approach gives
competitive performance while also providing additional interpretability.
</description>
    </item>
    
    <item>
        <title>Computational Analysis of Acoustic Descriptors in Psychotic Patients</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0466.PDF</link>
        <description>Various forms of psychotic disorders, including schizophrenia, can
influence how we speak. Therefore, clinicians assess speech and language
behaviors of their patients. While it is difficult for humans to quantify
speech behaviors precisely, acoustic descriptors, such as tenseness
of voice and speech rate, can be quantified automatically. In this
work, we identify previously unstudied acoustic descriptors related
to the severity of psychotic symptoms within a clinical population
(N=29). Our dataset consists of semi-structured interviews between
patients and clinicians. Psychotic disorders are often characterized
by two groups of symptoms: negative and positive. While negative symptoms
are also prevalent in disorders such as depression, positive symptoms
in psychotic disorders have rarely been studied from an acoustic and
computational perspective. Our experiments show relationships between
psychotic symptoms and acoustic descriptors related to voice quality
consistency, variation of speech rate and volume, vowel space, and
a parameter of glottal flow. Further, we show that certain acoustic
descriptors can track a patient&amp;#8217;s state from admission to discharge.
Finally, we demonstrate that measures from the Brief Psychiatric Rating
Scale (BPRS) can be estimated with acoustic descriptors.
</description>
    </item>
    
    <item>
        <title>Modeling Perceivers Neural-Responses Using Lobe-Dependent Convolutional Neural Network to Improve Speech Emotion Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0562.PDF</link>
        <description>Developing automatic emotion recognition by modeling expressive behaviors
is becoming crucial in enabling the next generation design of human-machine
interface. Also, with the availability of functional magnetic resonance
imaging (fMRI), researchers have also conducted studies into quantitative
understanding of vocal emotion perception mechanism. In this work,
our aim is two folds: 1) investigating whether the neural-responses
can be used to automatically decode the emotion labels of vocal stimuli,
and 2) combining acoustic and fMRI features to improve the speech emotion
recognition accuracies. We introduce a novel framework of lobe-dependent
convolutional neural network (LD-CNN) to provide better modeling of
perceivers neural-responses on vocal emotion. Furthermore, by fusing
LD-CNN with acoustic features, we demonstrate an overall 63.17% accuracies
in a four-class emotion recognition task (9.89% and 14.42% relative
improvement compared to the acoustic-only and the fMRI-only features).
Our analysis further shows that temporal lobe possess the most information
in decoding emotion labels; the fMRI and the acoustic information are
complementary to each other, where neural-responses and acoustic features
are better at discriminating along the valence and activation dimensions,
respectively.
</description>
    </item>
    
    <item>
        <title>Implementing Gender-Dependent Vowel-Level Analysis for Boosting Speech-Based Depression Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0887.PDF</link>
        <description>Whilst studies on emotion recognition show that gender-dependent analysis
can improve emotion classification performance, the potential differences
in the manifestation of depression between male and female speech have
yet to be fully explored. This paper presents a qualitative analysis
of phonetically aligned acoustic features to highlight differences
in the manifestation of depression. Gender-dependent analysis with
phonetically aligned gender-dependent features are used for speech-based
depression recognition. The presented experimental study reveals gender
differences in the effect of depression on vowel-level features. Considering
the experimental study, we also show that a small set of knowledge-driven
gender-dependent vowel-level features can outperform state-of-the-art
turn-level acoustic features when performing a binary depressed speech
recognition task. A combination of these preselected gender-dependent
vowel-level features with turn-level standardised openSMILE features
results in additional improvement for depression recognition.
</description>
    </item>
    
    <item>
        <title>Bilingual Word Embeddings for Cross-Lingual Personality Recognition Using Convolutional Neural Nets</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1379.PDF</link>
        <description>We propose a multilingual personality classifier that uses text data
from social media and Youtube Vlog transcriptions, and maps them into
Big Five personality traits using a Convolutional Neural Network (CNN).
We first train unsupervised bilingual word embeddings from an English-Chinese
parallel corpus, and use these trained word representations as input
to our CNN. This enables our model to yield relatively high cross-lingual
and multilingual performance on Chinese texts, after training on the
English dataset for example. We also train monolingual Chinese embeddings
from a large Chinese text corpus and then train our CNN model on a
Chinese dataset consisting of conversational dialogue labeled with
personality. We achieve an average F-score of 66.1 in our multilingual
task compared to 63.3 F-score in cross-lingual, and 63.2 F-score in
the monolingual performance. 
</description>
    </item>
    
    <item>
        <title>Emotion Category Mapping to Emotional Space by Cross-Corpus Emotion Labeling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0994.PDF</link>
        <description>The psychological classification of emotion has two main approaches.
One is emotion category, in which emotions are classified into discrete
and fundamental groups; the other is emotion dimension, in which emotions
are characterized by multiple continuous scales. The cognitive classification
of emotion by humans perceived from speech is not sufficiently established.
Although there have been several studies on such classification, they
did not discuss it deeply. Moreover, the relationship between emotion
category and emotion dimension perceived from speech is not well studied.
Aiming to establish common emotion labels for emotional speech, this
study elucidated the relationship between the emotion category and
the emotion dimension perceived by speech by conducting an experiment
of cross-corpus emotion labeling with two different Japanese dialogue
corpora (Online Gaming Voice Chat Corpus with Emotional Label (OGVC)
and Utsunomiya University Spoken Dialogue Database for Paralinguistic
Information Studies (UUDB)). A likelihood ratio test was conducted
to assess the independency of one emotion category from the others
in three-dimensional emotional space. This experiment revealed that
many emotion categories exhibited independency from the other emotion
categories. Only the neutral states did not exhibit independency from
the three emotions of sadness, disgust, and surprise.
</description>
    </item>
    
    <item>
        <title>Big Five vs. Prosodic Features as Cues to Detect Abnormality in SSPNET-Personality Corpus</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1194.PDF</link>
        <description>This paper presents an attempt to evaluate three different sets of
features extracted from prosodic descriptors and Big Five traits for
building an anomaly detector. The Big Five model enables to capture
personality information. Big Five traits are extracted from a manual
annotation while Prosodic features are extracted directly from the
speech signal. Two different anomaly detection methods are evaluated:
Gaussian Mixture Model (GMM) and One-Class SVM (OC-SVM), each one combined
with a threshold classification to decide the &amp;#8220;normality&amp;#8221;
of a sample. The different combinations of models and feature sets
are evaluated on the SSPNET-Personality corpus which has already been
used in several experiments, including a previous work on separating
two types of personality profiles in a supervised way. In this work,
we propose the above mentioned unsupervised or semi-supervised methods,
and discuss their performance, to detect particular audio-clips produced
by a speaker with an abnormal personality. Results show that using
automatically extracted prosodic features competes with the Big Five
traits. The overall detection performance achieved by the best model
is around 0.8 (F1-measure).
</description>
    </item>
    
    <item>
        <title>Speech Rate Comparison When Talking to a System and Talking to a Human: A Study from a Speech-to-Speech, Machine Translation Mediated Map Task</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1584.PDF</link>
        <description>This study focuses on the adaptation of subjects in Human-to-Human
(H2H) communication in spontaneous dialogues in two different settings.
The speech rate of sixteen dialogues from the HCRC Map Task corpus
have been analyzed as direct H2H communication, while fifteen dialogues
from the ILMT-s2s corpus have been analyzed as a Speech-to-Speech Machine
Translation (S2S-MT) mediated H2H communication comparison. The analysis
shows that while the mean speech rate of the subjects in the two task
oriented corpora differ, in both corpora the role of the subject causes
a significant difference in the speech rate with the Information Giver
using a slower speech rate than the Information Follower. Also the
different settings of the dialogue recordings (with or without eye
contact in the HCRC corpus and with or without live video streaming
in the ILMT-s2s corpus) only show a negligible difference in the speech
rate. However, the gender of the subjects have provided an interesting
difference with the female subjects of the ILMT-s2s corpus using a
slower speech rate than the male subjects, gender does not show any
difference in the HCRC corpus. This indicates that the difference is
not from performing the map task, but a result of their adaptation
strategy to the S2S-MT system.
</description>
    </item>
    
    <item>
        <title>Approaching Human Performance in Behavior Estimation in Couples Therapy Using Deep Sentence Embeddings</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1621.PDF</link>
        <description>Identifying complex behavior in human interactions for observational
studies often involves the tedious process of transcribing and annotating
large amounts of data. While there is significant work towards accurate
transcription in Automatic Speech Recognition, automatic Natural Language
Understanding of high-level human behaviors from the transcribed text
is still at an early stage of development. In this paper we present
a novel approach for modeling human behavior using sentence embeddings
and propose an automatic behavior annotation framework. We explore
unsupervised methods of extracting semantic information, using  seq2seq
models, into deep sentence embeddings and demonstrate that these embeddings
capture behaviorally meaningful information. Our proposed framework
utilizes LSTM Recurrent Neural Networks to estimate behavior trajectories
from these sentence embeddings. Finally, we employ fusion to compare
our high-resolution behavioral trajectories with the coarse, session-level
behavioral ratings of human annotators in Couples Therapy. Our experiments
show that behavior annotation using this framework achieves better
results than prior methods and approaches or exceeds human performance
in terms of annotator agreement.
</description>
    </item>
    
    <item>
        <title>Complexity in Speech and its Relation to Emotional Bond in Therapist-Patient Interactions During Suicide Risk Assessment Interviews</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1641.PDF</link>
        <description>In this paper, we analyze a 53-hour speech corpus of interactions of
soldiers who had recently attempted suicide or had strong suicidal
ideation conversing with their therapists. In particular, we study
the complexity in therapist-patient speech as a marker of their emotional
bond. Emotional bond is the extent to which the patient feels understood
by and connected to the therapist. First, we extract speech features
from audio recordings of their interactions. Then, we consider the
nonlinear time series representation of those features and compute
complexity measures based on the Lyapunov coefficient and correlation
dimension. For the majority of the subjects, we observe that speech
complexity in therapist-patient pairs is higher for the interview sessions,
when compared to that of the rest of their interactions (intervention
and post-interview follow-up). This indicates that entrainment (adapting
to each other&amp;#8217;s speech) between the patient and the therapist
is lower during the interview than regular interactions. This observation
is consistent with prior studies in clinical psychology, considering
that assessment interviews typically involve the therapist asking routine
questions to enquire about the patient&amp;#8217;s suicidal thoughts and
feelings. In addition, we find that complexity is negatively correlated
with the patient&amp;#8217;s perceived emotional bond with the therapist.
</description>
    </item>
    
    <item>
        <title>An Investigation of Emotion Dynamics and Kalman Filtering for Speech-Based Emotion Prediction</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1707.PDF</link>
        <description>Despite recent interest in continuous prediction of dimensional emotions,
the dynamical aspect of emotions has received less attention in automated
systems. This paper investigates how emotion change can be effectively
incorporated to improve continuous prediction of arousal and valence
from speech. Significant correlations were found between emotion ratings
and their dynamics during investigations on the RECOLA database, and
here we examine how to best exploit them using a Kalman filter. In
particular, we investigate the correlation between predicted arousal
and valence dynamics with arousal and valence ground truth; the Kalman
filter internal delay for estimating the state transition matrix; the
use of emotion dynamics as a measurement input to a Kalman filter;
and how multiple probabilistic Kalman filter outputs can be effectively
fused. Evaluation results show that correct dynamics estimation and
internal delay settings allow up to 5% and 58% relative improvement
in arousal and valence prediction respectively over existing Kalman
filter implementations. Fusion based on probabilistic Kalman filter
outputs yields further gains.
</description>
    </item>
    
    <item>
        <title>Zero-Shot Learning for Natural Language Understanding Using Domain-Independent Sequential Structure and Question Types</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0638.PDF</link>
        <description>Natural language understanding (NLU) is an important module of spoken
dialogue systems. One of the difficulties when it comes to adapting
NLU to new domains is the high cost of constructing new training data
for each domain. To reduce this cost, we propose a zero-shot learning
of NLU that takes into account the sequential structures of sentences
together with general question types across different domains. Experimental
results show that our methods achieve higher accuracy than baseline
methods in two completely different domains (insurance and sightseeing).
</description>
    </item>
    
    <item>
        <title>Parallel Hierarchical Attention Networks with Shared Memory Reader for Multi-Stream Conversational Document Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0269.PDF</link>
        <description>This paper describes a novel classification method for multi-stream
conversational documents. Documents of contact center dialogues or
meetings are often composed of multiple source documents that are transcriptions
of the recordings of each speaker&amp;#8217;s channel. To enhance the classification
performance of such multi-stream conversational documents, three main
advances over the previous method are introduced. The first is a parallel
hierarchical attention network (PHAN) for multi-stream conversational
document modeling. PHAN can precisely capture word and sentence structures
of individual source documents and efficiently integrate them. The
second is a shared memory reader that can yield a shared attention
mechanism. The shared memory reader highlights common important information
in a conversation. Our experiments on a call category classification
in contact center dialogues show that PHAN together with the shared
memory reader outperforms the single document modeling method and previous
multi-stream document modeling method.
</description>
    </item>
    
    <item>
        <title>Internal Memory Gate for Recurrent Neural Networks with Application to Spoken Language Understanding</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0357.PDF</link>
        <description>Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN) require
4 gates to learn short- and long-term dependencies for a given sequence
of basic elements. Recently, &amp;#8220;Gated Recurrent Unit&amp;#8221; (GRU)
has been introduced and requires fewer gates than LSTM (reset and update
gates), to code short- and long-term dependencies and reaches equivalent
performances to LSTM, with less processing time during the learning.
The &amp;#8220;Leaky integration Unit&amp;#8221; (LU) is a GRU with a single
gate (update) that codes mostly long-term dependencies quicker than
LSTM or GRU (small number of operations for learning). This paper proposes
a novel RNN that takes advantage of LSTM, GRU (short- and long-term
dependencies) and the LU (fast learning) called &amp;#8220;Internal Memory
Gate&amp;#8221; (IMG). The effectiveness and the robustness of the proposed
IMG-RNN is evaluated during a classification task of a small corpus
of spoken dialogues from the DECODA project that allows us to evaluate
the capability of each RNN to code short-term dependencies. The experiments
show that IMG-RNNs reach better accuracies with a gain of 0.4 points
compared to LSTM- and GRU-RNNs and 0.7 points compared to the LU-RNN.
Moreover, IMG-RNN requires less processing time than GRU or LSTM with
a gain of 19% and 50% respectively.
</description>
    </item>
    
    <item>
        <title>Character-Based Embedding Models and Reranking Strategies for Understanding Natural Language Meal Descriptions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0422.PDF</link>
        <description>Character-based embedding models provide robustness for handling misspellings
and typos in natural language. In this paper, we explore convolutional
neural network based embedding models for handling out-of-vocabulary
words in a meal description food ranking task. We demonstrate that
character-based models combined with a standard word-based model improves
the top-5 recall of USDA database food items from 26.3% to 30.3% on
a test set of all USDA foods with typos simulated in 10% of the data.
We also propose a new reranking strategy for predicting the top USDA
food matches given a meal description, which significantly outperforms
our prior method of n-best decoding with a finite state transducer,
improving the top-5 recall on the all USDA foods task from 20.7% to
63.8%.
</description>
    </item>
    
    <item>
        <title>Quaternion Denoising Encoder-Decoder for Theme Identification of Telephone Conversations</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1029.PDF</link>
        <description>In the last decades, encoder-decoders or autoencoders (AE) have received
a great interest from researchers due to their capability to construct
robust representations of documents in a low dimensional subspace.
Nonetheless, autoencoders reveal little in way of spoken document internal
structure by only considering words or topics contained in the document
as an isolate basic element, and tend to overfit with small corpus
of documents. Therefore, Quaternion Multi-layer Perceptrons (QMLP)
have been introduced to capture such internal latent dependencies,
whereas denoising autoencoders (DAE) are composed with different stochastic
noises to better process small set of documents. This paper presents
a novel autoencoder based on both hitherto-proposed DAE (to manage
small corpus) and the QMLP (to consider internal latent structures)
called &amp;#8220;Quaternion denoising encoder-decoder&amp;#8221; (QDAE). Moreover,
the paper defines an original angular Gaussian noise adapted to the
specificity of hyper-complex algebra. The experiments, conduced on
a theme identification task of spoken dialogues from the DECODA framework,
show that the QDAE obtains the promising gains of 3% and 1.5% compared
to the standard real valued denoising autoencoder and the QMLP respectively.
</description>
    </item>
    
    <item>
        <title>ASR Error Management for Improving Spoken Language Understanding</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1178.PDF</link>
        <description>This paper addresses the problem of automatic speech recognition (ASR)
error detection and their use for improving spoken language understanding
(SLU) systems. In this study, the SLU task consists in automatically
extracting, from ASR transcriptions, semantic concepts and concept/values
pairs in a  e.g touristic information system. An approach is proposed
for enriching the set of semantic labels with error specific labels
and by using a recently proposed neural approach based on word embeddings
to compute well calibrated ASR confidence measures. Experimental results
are reported showing that it is possible to decrease significantly
the Concept/Value Error Rate with a state of the art system, outperforming
previously published results performance on the same experimental data.
It also shown that combining an SLU approach based on conditional random
fields with a neural encoder/decoder attention based architecture,
it is possible to effectively identifying confidence islands and uncertain
semantic output segments useful for deciding appropriate error handling
actions by the dialogue manager strategy.
</description>
    </item>
    
    <item>
        <title>Jointly Trained Sequential Labeling and Classification by Sparse Attention Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1321.PDF</link>
        <description>Sentence-level classification and sequential labeling are two fundamental
tasks in language understanding. While these two tasks are usually
modeled separately, in reality, they are often correlated, for example
in intent classification and slot filling, or in topic classification
and named-entity recognition. In order to utilize the potential benefits
from their correlations, we propose a jointly trained model for learning
the two tasks simultaneously via Long Short-Term Memory (LSTM) networks.
This model predicts the sentence-level category and the word-level
label sequence from the stepwise output hidden representations of LSTM.
We also introduce a novel mechanism of &amp;#8220;sparse attention&amp;#8221;
to weigh words differently based on their semantic relevance to sentence-level
classification. The proposed method outperforms baseline models on
ATIS and TREC datasets.
</description>
    </item>
    
    <item>
        <title>To Plan or not to Plan? Discourse Planning in Slot-Value Informed Sequence to Sequence Models for Language Generation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1525.PDF</link>
        <description>Natural language generation for task-oriented dialogue systems aims
to effectively realize system dialogue actions. All natural language
generators (NLGs) must realize grammatical, natural and appropriate
output, but in addition, generators for task-oriented dialogue must
faithfully perform a specific dialogue act that conveys specific semantic
information, as dictated by the dialogue policy of the system dialogue
manager. Most previous work on deep learning methods for task-oriented
NLG assumes that generation output can be an utterance skeleton. Utterances
are delexicalized, with variable names for slots, which are then replaced
with actual values as part of post-processing. However, the value of
slots do, in fact, influence the lexical selection in the surrounding
context as well as the overall sentence plan. To model this effect,
we investigate sequence-to-sequence (seq2seq) models in which slot
values are included as part of the input sequence and the output surface
form. Furthermore, we study whether a separate sentence planning module
that decides on grouping of slot value mentions as input to the seq2seq
model results in more natural sentences than a seq2seq model that aims
to jointly learn the plan and the surface realization.
</description>
    </item>
    
    <item>
        <title>Online Adaptation of an Attention-Based Neural Network for Natural Language Generation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0921.PDF</link>
        <description>Following some recent propositions to handle natural language generation
in spoken dialog systems with long short-term memory recurrent neural
network models [1] we first investigate a variant thereof with the
objective of a better integration of the attention subnetwork. Then
our main objective is to propose and evaluate a framework to adapt
the NLG module online through direct interactions with the users. When
doing so the basic way is to ask the user to utter an alternative sentence
to express a particular dialog act. But then the system has to decide
between using an automatic transcription or to ask for a manual transcription.
To do so a reinforcement learning approach based on an adversarial
bandit scheme is retained. We show that by defining appropriately the
rewards as a linear combination of expected payoffs and costs of acquiring
the new data provided by the user, a system design can balance between
improving the system&amp;#8217;s performance towards a better match with
the user&amp;#8217;s preferences and the burden associated with it.
</description>
    </item>
    
    <item>
        <title>Spanish Sign Language Recognition with Different Topology Hidden Markov Models</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0275.PDF</link>
        <description>Natural language recognition techniques can be applied not only to
speech signals, but to other signals that represent natural language
units (e.g., words and sentences). This is the case of sign language
recognition, which is usually employed by deaf people to communicate.
The use of recognition techniques may allow this language users to
communicate more independently with non-signal users. Several works
have been done for different variants of sign languages, but in most
cases their vocabulary is quite limited and they only recognise gestures
corresponding to isolated words. In this work, we propose gesture recognisers
which make use of typical Continuous Density Hidden Markov Model. They
solve not only the isolated word problem, but also the recognition
of basic sentences using the Spanish Sign Language with a higher vocabulary
than in other approximations. Different topologies and Gaussian mixtures
are studied. Results show that our proposal provides promising results
that are the first step to obtain a general automatic recognition of
Spanish Sign Language.
</description>
    </item>
    
    <item>
        <title>OpenMM: An Open-Source Multimodal Feature Extraction Tool</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1382.PDF</link>
        <description>The primary use of speech is in face-to-face interactions and situational
context and human behavior therefore intrinsically shape and affect
communication. In order to usefully model situational awareness, machines
must have access to the same streams of information humans have access
to. In other words, we need to provide machines with features that
represent each communicative modality: face and gesture, voice and
speech, and language. This paper presents OpenMM: an open-source multimodal
feature extraction tool. We build upon existing open-source repositories
to present the first publicly available tool for multimodal feature
extraction. The tool provides a pipeline for researchers to easily
extract visual and acoustic features. In addition, the tool also performs
automatic speech recognition (ASR) and then uses the transcripts to
extract linguistic features. We evaluate the OpenMM&amp;#8217;s multimodal
feature set on deception, depression and sentiment classification tasks
and show its performance is very promising. This tool provides researchers
with a simple way of extracting multimodal features and consequently
a richer and more robust feature representation for machine learning
tasks.
</description>
    </item>
    
    <item>
        <title>Speaker Dependency Analysis, Audiovisual Fusion Cues and a Multimodal BLSTM for Conversational Engagement Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1496.PDF</link>
        <description>Conversational engagement is a multimodal phenomenon and an essential
cue to assess both human-human and human-robot communication. Speaker-dependent
and speaker-independent scenarios were addressed in our engagement
study. Handcrafted audio-visual features were used. Fixed window sizes
for feature fusion method were analysed. Novel dynamic window size
selection and multimodal bi-directional long short term memory (Multimodal
BLSTM) approaches were proposed and evaluated for engagement level
recognition.
</description>
    </item>
    
    <item>
        <title>Voice Conversion from Unaligned Corpora Using Variational Autoencoding Wasserstein Generative Adversarial Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0063.PDF</link>
        <description>Building a voice conversion (VC) system from non-parallel speech corpora
is challenging but highly valuable in real application scenarios. In
most situations, the source and the target speakers do not repeat the
same texts or they may even speak different languages. In this case,
one possible, although indirect, solution is to build a generative
model for speech. Generative models focus on explaining the observations
with latent variables instead of learning a pairwise transformation
function, thereby bypassing the requirement of speech frame alignment.
In this paper, we propose a non-parallel VC framework with a variational
autoencoding Wasserstein generative adversarial network (VAW-GAN) that
explicitly considers a VC objective when building the speech model.
Experimental results corroborate the capability of our framework for
building a VC system from unaligned data, and demonstrate improved
conversion quality.
</description>
    </item>
    
    <item>
        <title>CAB: An Energy-Based Speaker Clustering Model for Rapid Adaptation in Non-Parallel Voice Conversion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0133.PDF</link>
        <description>In this paper, a new energy-based probabilistic model, called CAB (Cluster
Adaptive restricted Boltzmann machine), is proposed for voice conversion
(VC) that does not require parallel data during the training and requires
only a small amount of speech data during the adaptation. Most of the
existing VC methods require parallel data for training. Recently, VC
methods that do not require parallel data (called non-parallel VCs)
have been also proposed and are attracting much attention because they
do not require prepared or recorded parallel speech data, unlike conventional
approaches. The proposed CAB model is aimed at statistical non-parallel
VC based on cluster adaptive training (CAT). This extends the VC method
used in our previous model, ARBM (adaptive restricted Boltzmann machine).
The ARBM approach assumes that any speech signals can be decomposed
into speaker-invariant phonetic information and speaker-identity information
using the ARBM adaptation matrices of each speaker. VC is achieved
by switching the source speaker&amp;#8217;s identity into those of the
target speaker while retaining the phonetic information obtained by
decomposition of the source speaker&amp;#8217;s speech. In contrast, CAB
speaker identities are represented as cluster vectors that determine
the adaptation matrices. As the number of clusters is generally smaller
than the number of speakers, the number of model parameters can be
reduced compared to ARBM, which enables rapid adaptation of a new speaker.
Our experimental results show that the proposed method especially performed
better than the ARBM approach, particularly in adaptation.
</description>
    </item>
    
    <item>
        <title>Phoneme-Discriminative Features for Dysarthric Speech Conversion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0664.PDF</link>
        <description>We present in this paper a Voice Conversion (VC) method for a person
with dysarthria resulting from athetoid cerebral palsy. VC is being
widely researched in the field of speech processing because of increased
interest in using such processing in applications such as personalized
Text-To-Speech systems. A Gaussian Mixture Model (GMM)-based VC method
has been widely researched and Partial Least Square (PLS)-based VC
has been proposed to prevent the over-fitting problems associated with
the GMM-based VC method. In this paper, we present phoneme-discriminative
features, which are associated with PLS-based VC. Conventional VC methods
do not consider the phonetic structure of spectral features although
phonetic structures are important for speech analysis. Especially for
dysarthric speech, their phonetic structures are difficult to discriminate
and discriminative learning will improve the conversion accuracy. This
paper employs discriminative manifold learning. Spectral features are
projected into a subspace in which a near point with the same phoneme
label is close to another and a near point with a different phoneme
label is apart. Our proposed method was evaluated on dysarthric speaker
conversion task which converts dysarthric voice into non-dysarthric
speech.
</description>
    </item>
    
    <item>
        <title>Denoising Recurrent Neural Network for Deep Bidirectional LSTM Based Voice Conversion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0694.PDF</link>
        <description>The paper studies the post processing in deep bidirectional Long Short-Term
Memory (DBLSTM) based voice conversion, where the statistical parameters
are optimized to generate speech that exhibits similar properties to
target speech. However, there always exists residual error between
converted speech and target one. We reformulate the residual error
problem as speech restoration, which aims to recover the target speech
samples from the converted ones. Specifically, we propose a denoising
recurrent neural network (DeRNN) by introducing regularization during
training to shape the distribution of the converted data in latent
space. We compare the proposed approach with global variance (GV),
modulation spectrum (MS) and recurrent neural network (RNN) based postfilters,
which serve a similar purpose. The subjective test results show that
the proposed approach significantly outperforms these conventional
approaches in terms of quality and similarity.
</description>
    </item>
    
    <item>
        <title>Speaker Dependent Approach for Enhancing a Glossectomy Patient&amp;#8217;s Speech via GMM-Based Voice Conversion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0841.PDF</link>
        <description>In this paper, using GMM-based voice conversion algorithm, we propose
to generate speaker-dependent mapping functions to improve the intelligibility
of speech uttered by patients with a wide glossectomy. The speaker-dependent
approach enables to generate the mapping functions that reconstruct
missing spectrum features of speech uttered by a patient without having
influences of a speaker&amp;#8217;s factor. The proposed idea is simple,
i.e., to collect speech uttered by a patient before and after the glossectomy,
but in practice it is hard to ask patients to utter speech just for
developing algorithms. To confirm the performance of the proposed approach,
in this paper, in order to simulate glossectomy patients, we fabricated
an intraoral appliance which covers lower dental arch and tongue surface
to restrain tongue movements. In terms of the Mel-frequency cepstrum
(MFC) distance, by applying the voice conversion, the distances were
reduced by 25% and 42% for speaker-dependent case and speaker-independent
case, respectively. In terms of phoneme intelligibility, dictation
tests revealed that speech reconstructed by speaker-dependent approach
almost always showed better performance than the original speech uttered
by simulated patients, while speaker-independent approach did not.
</description>
    </item>
    
    <item>
        <title>Generative Adversarial Network-Based Postfilter for STFT Spectrograms</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0962.PDF</link>
        <description>We propose a learning-based postfilter to reconstruct the high-fidelity
spectral texture in short-term Fourier transform (STFT) spectrograms.
In speech-processing systems, such as speech synthesis, conversion,
enhancement, separation, and coding, STFT spectrograms have been widely
used as key acoustic representations. In these tasks, we normally need
to precisely generate or predict the representations from inputs; however,
generated spectra typically lack the fine structures that are close
to those of the true data. To overcome these limitations and reconstruct
spectra having finer structures, we propose a generative adversarial
network (GAN)-based postfilter that is implicitly optimized to match
the true feature distribution in adversarial learning. The challenge
with this postfilter is that a GAN cannot be easily trained for very
high-dimensional data such as STFT spectra. We take a simple divide-and-concatenate
strategy. Namely, we first divide the spectrograms into multiple frequency
bands with overlap, reconstruct the individual bands using the GAN-based
postfilter trained for each band, and finally connect the bands with
overlap. We tested our proposed postfilter on a deep neural network-based
text-to-speech task and confirmed that it was able to reduce the gap
between synthesized and target spectra, even in the high-dimensional
STFT domain.
</description>
    </item>
    
    <item>
        <title>Generative Adversarial Network-Based Glottal Waveform Model for Statistical Parametric Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1288.PDF</link>
        <description>Recent studies have shown that text-to-speech synthesis quality can
be improved by using glottal vocoding. This refers to vocoders that
parameterize speech into two parts, the glottal excitation and vocal
tract, that occur in the human speech production apparatus. Current
glottal vocoders generate the glottal excitation waveform by using
deep neural networks (DNNs). However, the squared error-based training
of the present glottal excitation models is limited to generating conditional
average waveforms, which fails to capture the stochastic variation
of the waveforms. As a result, shaped noise is added as post-processing.
In this study, we propose a new method for predicting glottal waveforms
by generative adversarial networks (GANs). GANs are generative models
that aim to embed the data distribution in a latent space, enabling
generation of new instances very similar to the original by randomly
sampling the latent distribution. The glottal pulses generated by GANs
show a stochastic component similar to natural glottal pulses. In our
experiments, we compare synthetic speech generated using glottal waveforms
produced by both DNNs and GANs. The results show that the newly proposed
GANs achieve synthesis quality comparable to that of widely-used DNNs,
without using an additive noise component.
</description>
    </item>
    
    <item>
        <title>Emotional Voice Conversion with Adaptive Scales F0 Based on Wavelet Transform Using Limited Amount of Emotional Data</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0984.PDF</link>
        <description>Deep learning techniques have been successfully applied to speech processing.
Typically, neural networks (NNs) are very effective in processing nonlinear
features, such as mel cepstral coefficients (MCC), which represent
the spectrum features in voice conversion (VC) tasks. Despite these
successes, the approach is restricted to problems with moderate dimension
and sufficient data. Thus, in emotional VC tasks, it is hard to deal
with a simple representation of fundamental frequency (F0), which is
the most important feature in emotional voice representation, Another
problem is that there are insufficient emotional data for training.
To deal with these two problems, in this paper, we propose the adaptive
scales continuous wavelet transform (AS-CWT) method to systematically
capture the F0 features of different temporal scales, which can represent
different prosodic levels ranging from micro-prosody to sentence levels.
Meanwhile, we also use the pre-trained conversion functions obtained
from other emotional datasets to synthesize new emotional data as additional
training samples for target emotional voice conversion. Experimental
results indicate that our proposed method achieves the best performance
in both objective and subjective evaluations.
</description>
    </item>
    
    <item>
        <title>Speaker Adaptation in DNN-Based Speech Synthesis Using d-Vectors</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1038.PDF</link>
        <description>The paper presents a mechanism to perform speaker adaptation in speech
synthesis based on deep neural networks (DNNs). The mechanism extracts
speaker identification vectors, so-called  d-vectors, from the training
speakers and uses them jointly with the linguistic features to train
a multi-speaker DNN-based text-to-speech synthesizer (DNN-TTS). The
 d-vectors are derived by applying principal component analysis (PCA)
on the bottle-neck features of a speaker classifier network. At the
adaptation stage, three variants are explored: (1)  d-vectors calculated
using data from the target speaker, or (2)  d-vectors calculated as
a weighted sum of  d-vectors from training speakers, or (3)  d-vectors
calculated as an average of the above two approaches. The proposed
method of unsupervised adaptation using the  d-vector is compared with
the commonly used  i-vector based approach for speaker adaptation.
Listening tests show that: (1) for speech quality, the  d-vector based
approach is significantly preferred over the  i-vector based approach.
All the  d-vector variants perform similar for speech quality; (2)
for speaker similarity, both  d-vector and  i-vector based adaptation
were found to perform similar, except a small significant preference
for the  d-vector calculated as an average over the  i-vector.
</description>
    </item>
    
    <item>
        <title>Spectro-Temporal Modelling with Time-Frequency LSTM and Structured Output Layer for Voice Conversion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1122.PDF</link>
        <description>From speech, speaker identity can be mostly characterized by the spectro-temporal
structures of spectrum. Although recent researches have demonstrated
the effectiveness of employing long short-term memory (LSTM) recurrent
neural network (RNN) in voice conversion, traditional LSTM-RNN based
approaches usually focus on temporal evolutions of speech features
only. In this paper, we improve the conventional LSTM-RNN method for
voice conversion by employing the two-dimensional time-frequency LSTM
(TFLSTM) to model spectro-temporal warping along both time and frequency
axes. A multi-task learned structured output layer (SOL) is afterward
adopted to capture the dependencies between spectral and pitch parameters
for further improvement, where spectral parameter targets are conditioned
upon pitch parameters prediction. Experimental results show the proposed
approach outperforms conventional systems in speech quality and speaker
similarity.
</description>
    </item>
    
    <item>
        <title>Segment Level Voice Conversion with Recurrent Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1538.PDF</link>
        <description>Voice conversion techniques aim to modify a subject&amp;#8217;s voice characteristics
in order to mimic the one&amp;#8217;s of another person. Due to the difference
in utterance length between source and target speaker, state of the
art voice conversion systems often rely on a frame alignment pre-processing
step. This step aligns the entire utterances with algorithms such as
dynamic time warping (DTW) that introduce errors, hindering system
performance. In this paper we present a new technique that avoids the
alignment of entire utterances at frame level, while keeping the local
context during training. For this purpose, we combine an RNN model
with the use of phoneme or syllable-level information, obtained from
a speech recognition system. This system segments the utterances into
segments which then can be grouped into overlapping windows, providing
the needed context for the model to learn the temporal dependencies.
We show that with this approach, notable improvements can be attained
over a state of the art RNN voice conversion system on the CMU ARCTIC
database. It is also worth noting that with this technique it is possible
to halve the training data size and still outperform the baseline.
</description>
    </item>
    
    <item>
        <title>Creating a Voice for  MiRo, the World&amp;#8217;s First Commercial Biomimetic Robot</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2022.PDF</link>
        <description>This paper introduces  MiRo &amp;#8212; the world&amp;#8217;s first commercial
biomimetic robot &amp;#8212; and describes how its vocal system was designed
using a real-time parametric general-purpose mammalian vocal synthesiser
tailored to the specific physical characteristics of the robot.  MiRo&amp;#8217;s
capabilities will be demonstrated live during the hands-on interactive
&amp;#8216;Show &amp;amp; Tell&amp;#8217; session at INTERSPEECH-2017.
</description>
    </item>
    
    <item>
        <title>A Thematicity-Based Prosody Enrichment Tool for CTS</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2023.PDF</link>
        <description>This paper presents a demonstration of a stochastic prosody tool for
enrichment of synthesized speech using SSML prosody tags applied over
hierarchical thematicity spans in the context of a CTS application.
The motivation for using hierarchical thematicity is exemplified, together
with the capabilities of the module to generate a variety of SSML prosody
tags within a controlled range of values depending on the input thematicity
label.
</description>
    </item>
    
    <item>
        <title>WebSubDub &amp;#8212; Experimental System for Creating High-Quality Alternative Audio Track for TV Broadcasting</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2024.PDF</link>
        <description>This paper deals with a presentation of an experimental system (called
 WebSubDub) for creating a high-quality alternative audio track for
TV broadcasting. The system is used to create subtitles for TV shows
in such a format which allows to automatically generate an alternative
audio track with multiple voices employing a specially adapted TTS
system. This alternative audio track is intended for televiewers with
slight hearing impairments, i.e. for a group of televiewers who encounter
issues when perceiving the original audio track &amp;#8212; especially
dialogues with background music, background noise or emotional speech.
The system was developed in cooperation with Czech television, the
public service broadcaster in the Czech Republic.
</description>
    </item>
    
    <item>
        <title>Voice Conservation and TTS System for People Facing Total Laryngectomy</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2026.PDF</link>
        <description>The presented paper is focused on the building of personalized text-to-speech
(TTS) synthesis for people who are losing their voices due to fatal
diseases. The special conditions of this issue make the process different
from preparing professional synthetic voices for commercial TTS systems
and make it also more difficult. The whole process is described in
this paper and the first results of the personalized voice building
are presented here as well.
</description>
    </item>
    
    <item>
        <title>TBT (Toolkit to Build TTS): A High Performance Framework to Build Multiple Language HTS Voice</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2042.PDF</link>
        <description>With the development of high quality TTS systems, application area
of synthetic speech is increasing rapidly. Beyond the communication
aids for the visually impaired and vocally handicap, TTS voices are
being used in various educational, telecommunication and multimedia
applications. All around the world people are trying to build TTS voice
for their regional languages. TTS voice building requires a number
of steps to follow and involves use of multiple tools, which makes
it time consuming, tedious and perplexing to a user. This paper describes
a Toolkit developed for HMM-based TTS voice building that makes the
process much easier and handy. The toolkit uses all required tools,
viz. HTS, Festival, Festvox, Hybrid Segmentation Tool, etc. and handles
each and every step starting from phone set creation, then prompt generation,
hybrid segmentation, F0 range finding, voice building, and finally
putting the built voice into Synthesis framework. Wherever possible
it does parallel processing to reduce time. It saves manual effort
and time to a large extent and enable a person to build TTS voice very
easily. This toolkit is made available under Open Source license.
</description>
    </item>
    
    <item>
        <title>SIAK &amp;#8212; A Game for Foreign Language Pronunciation Learning</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2046.PDF</link>
        <description>We introduce a digital game for children&amp;#8217;s foreign-language learning
that uses automatic speech recognition (ASR) for evaluating children&amp;#8217;s
utterances. Our first prototype focuses on the learning of English
words and their pronunciation. The game connects to a network server,
which handles the recognition and pronunciation grading of children&amp;#8217;s
foreign-language speech. The server is reusable for different applications.
Given suitable acoustic models, it can be used for grading pronunciations
in any language.
</description>
    </item>
    
    <item>
        <title>Integrating the Talkamatic Dialogue Manager with Alexa</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2029.PDF</link>
        <description>This paper describes the integration of Amazon Alexa with the Talkamatic
Dialogue Manager (TDM), and shows how flexible dialogue skills and
rapid prototyping of dialogue apps can be brought to the Alexa platform.
</description>
    </item>
    
    <item>
        <title>A Robust Medical Speech-to-Speech/Speech-to-Sign Phraselator</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2031.PDF</link>
        <description>We present BabelDr, a web-enabled spoken-input phraselator for medical
domains, which has been developed at Geneva University in a collaboration
between a human language technology group and a group at the University
hospital. The current production version of the system translates French
into Arabic, using exclusively rule-based methods, and has performed
credibly in simulated triaging tests with standardised patients. We
also present an experimental version which combines large-vocabulary
recognition with the main rule-based recogniser; offline tests on unseen
data suggest that the new architecture adds robustness while more than
halving the 2-best semantic error rate. The experimental version translates
from spoken English into spoken French and also two sign languages.
</description>
    </item>
    
    <item>
        <title>Towards an Autarkic Embedded Cognitive User Interface</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2041.PDF</link>
        <description>With this paper we present an overview of an autarkic embedded cognitive
user interface. It is realized in form of an integrated device able
to communicate with the user over speech &amp;amp; gesture recognition,
speech synthesis and a touch display. Semantic processing and cognitive
behaviour control support intuitive interaction and help controlling
arbitrary electronic devices. To ensure user privacy and to operate
autonomously of network access all information processing is done on
the device.
</description>
    </item>
    
    <item>
        <title>Nora the Empathetic Psychologist</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2050.PDF</link>
        <description>Nora is a new dialog system that mimics a conversation with a psychologist
by screening for stress, anxiety, and depression. She understands,
empathizes, and adapts to users using emotional intelligence modules
trained via statistical modelling such as Convolutional Neural Networks.
These modules also enable her to personalize the content of each conversation.
</description>
    </item>
    
    <item>
        <title>Modifying Amazon&amp;#8217;s Alexa ASR Grammar and Lexicon &amp;#8212; A Case Study</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2057.PDF</link>
        <description>In this proof-of-concept study we build a tool that modifies the grammar
and the dictionary of an Automatic Speech Recognition (ASR) engine.
We evaluated our tool using Amazon&amp;#8217;s Alexa ASR engine. The experiments
show that with our grammar and dictionary modification algorithms in
the military domain, the accuracy of the modified ASR went up significantly
&amp;#8212; from 20/100 correct to 80/100 correct.
</description>
    </item>
    
    <item>
        <title>Re-Inventing Speech &#8212; The Biological Way</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>The INTERSPEECH 2017 Computational Paralinguistics Challenge: Addressee, Cold &amp;amp; Snoring</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0043.PDF</link>
        <description>The INTERSPEECH 2017 Computational Paralinguistics Challenge addresses
three different problems for the first time in research competition
under well-defined conditions: In the  Addressee sub-challenge, it
has to be determined whether speech produced by an adult is directed
towards another adult or towards a child; in the  Cold sub-challenge,
speech under cold has to be told apart from &amp;#8216;healthy&amp;#8217; speech;
and in the  Snoring sub-challenge, four different types of snoring
have to be classified. In this paper, we describe these sub-challenges,
their conditions, and the baseline feature extraction and classifiers,
which include data-learnt feature representations by end-to-end learning
with convolutional and recurrent neural networks, and bag-of-audio-words
for the first time in the challenge series.
</description>
    </item>
    
    <item>
        <title>It Sounds Like You Have a Cold! Testing Voice Features for the Interspeech 2017 Computational Paralinguistics Cold Challenge</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1261.PDF</link>
        <description>This paper describes an evaluation of four different voice feature
sets for detecting symptoms of the common cold in speech as part of
the Interspeech 2017 Computational Paralinguistics Challenge. The challenge
corpus consists of 630 speakers in three partitions, of which approximately
one third had a &amp;#8220;severe&amp;#8221; cold at the time of recording.
Success on the task is measured in terms of unweighted average recall
of cold/not-cold classification from short extracts of the recordings.
In this paper we review previous voice features used for studying changes
in health and devise four basic types of features for evaluation: voice
quality features, vowel spectra features, modulation spectra features,
and spectral distribution features. The evaluation shows that each
feature set provides some useful information to the task, with features
from the modulation spectrogram being most effective. Feature-level
fusion of the feature sets shows small performance improvements on
the development test set. We discuss the results in terms of the most
suitable features for detecting symptoms of cold and address issues
arising from the design of the challenge.
</description>
    </item>
    
    <item>
        <title>End-to-End Deep Learning Framework for Speech Paralinguistics Detection Based on Perception Aware Spectrum</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1445.PDF</link>
        <description>In this paper, we propose an end-to-end deep learning framework to
detect speech paralinguistics using perception aware spectrum as input.
Existing studies show that speech under cold has distinct variations
of energy distribution on low frequency components compared with the
speech under &amp;#8216;healthy&amp;#8217; condition. This motivates us to
use perception aware spectrum as the input to an end-to-end learning
framework with small scale dataset. In this work, we try both Constant
Q Transform (CQT) spectrum and Gammatone spectrum in different end-to-end
deep learning networks, where both spectrums are able to closely mimic
the human speech perception and transform it into 2D images. Experimental
results show the effectiveness of the proposed perception aware spectrum
with end-to-end deep learning approach on Interspeech 2017 Computational
Paralinguistics  Cold sub-Challenge. The final fusion result of our
proposed method is 8% better than that of the provided baseline in
terms of UAR.
</description>
    </item>
    
    <item>
        <title>Infected Phonemes: How a Cold Impairs Speech on a Phonetic Level</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1066.PDF</link>
        <description>The realization of language through vocal sounds involves a complex
interplay between the lungs, the vocal cords, and a series of resonant
chambers (e.g. mouth and nasal cavities). Due to their connection to
the outside world, these body parts are popular spots for viruses and
bacteria to enter the human organism. Affected people may suffer from
an upper respiratory tract infection (URTIC) and consequently their
voice often sounds breathy, raspy or sniffly. In this paper, we investigate
the audible effects of a cold on a phonetic level. Results on a German
corpus show that the articulation of consonants is more impaired than
that of vowels. Surprisingly, nasal sounds do not follow this trend
in our experiments. We finally try to predict a speaker&amp;#8217;s health
condition by fusing decisions we derive from single phonemes. The presented
work is part of the INTERSPEECH 2017 Computational Paralinguistics
Challenge.
</description>
    </item>
    
    <item>
        <title>Phoneme State Posteriorgram Features for Speech Based Automatic Classification of Speakers in Cold and Healthy Condition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1550.PDF</link>
        <description>We consider the problem of automatically detecting if a speaker is
suffering from common cold from his/her speech. When a speaker has
symptoms of cold, his/her voice quality changes compared to the normal
one. We hypothesize that such a change in voice quality could be reflected
in lower likelihoods from a model built using normal speech. In order
to capture this, we compute a 120-dimensional posteriorgram feature
in each frame using Gaussian mixture model from 120 states of 40 three-states
phonetic hidden Markov models trained on approximately 16.4 hours of
normal English speech. Finally, a fixed 5160-dimensional phoneme state
posteriorgram (PSP) feature vector for each utterance is obtained by
computing statistics from the posteriorgram feature trajectory. Experiments
on the 2017-Cold sub-challenge data show that when the decisions from
bag-of-audio-words (BoAW) and end-to-end (e2e) are combined with those
from PSP features with unweighted majority rule, the UAR on the development
set becomes 69% which is 2.9% (absolute) better than the best of the
UARs obtained by the baseline schemes. When the decisions from ComParE,
BoAW and PSP features are combined with simple majority rule, it results
in a UAR of 68.52% on the test set.
</description>
    </item>
    
    <item>
        <title>An Integrated Solution for Snoring Sound Classification Using Bhattacharyya Distance Based GMM Supervectors with SVM, Feature Selection with Random Forest and Spectrogram with CNN</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1794.PDF</link>
        <description>Snoring is caused by the narrowing of the upper airway and it is excited
by different locations within the upper airways. This irregularity
could lead to the presence of Obstructive Sleep Apnea Syndrome (OSAS).
Diagnosis of OSAS could therefore be made by snoring sound analysis.
This paper proposes the novel method to automatically classify snoring
sounds by their excitation locations for ComParE2017 challenge. We
propose 3 sub-systems for classification. In the first system, we propose
to integrate Bhattacharyya distance based Gaussian Mixture Model (GMM)
supervectors to a set of static features provided by ComParE2017 challenge.
The Bhattacharyya distance based GMM supervectors characterize the
spectral dissimilarity measure among snore sounds excited by different
locations. And, we employ Support Vector Machine (SVM) for classification.
In the second system, we perform feature selection on static features
provided by the challenge and conduct classification using Random Forest.
In the third system, we extract spectrogram from audio and employ Convolutional
Neural Network (CNN) for snore sound classification. Then, we fuse
3 sub-systems to produce final classification results. The experimental
results show that the proposed system performs better than the challenge
baseline.
</description>
    </item>
    
    <item>
        <title>Acoustic Analysis of Detailed Three-Dimensional Shape of the Human Nasal Cavity and Paranasal Sinuses</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0107.PDF</link>
        <description>The nasal and paranasal cavities have a labyrinthine shape and their
acoustic properties affect speech sounds. In this study, we explored
the transfer function of the nasal and paranasal cavities, as well
as the contribution of each paranasal cavity, using acoustical and
numerical methods. A physical model of the nasal and paranasal cavities
was formed using data from a high-resolution 3D X-ray CT and a 3D printer.
The data was acquired from a female subject during silent nasal breathing.
The transfer function of the physical model was then measured by introducing
a white noise signal at the glottis and measuring its acoustic response
at a point 20 mm away from the nostrils. We also calculated the transfer
function of the 3D model using a finite-difference time-domain or FDTD
method. The results showed that the gross shape and the frequency of
peaks and dips of the measured and calculated transfer functions were
similar, suggesting that both methods used in this study were reliable.
The results of FDTD simulations evaluating the paranasal sinuses individually
suggested that they contribute not only to spectral dips but also to
peaks, which is contrary to the traditional theories regarding the
production of speech sounds.
</description>
    </item>
    
    <item>
        <title>A Semi-Polar Grid Strategy for the Three-Dimensional Finite Element Simulation of Vowel-Vowel Sequences</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0448.PDF</link>
        <description>Three-dimensional computational acoustic models need very detailed
3D vocal tract geometries to generate high quality sounds. Static geometries
can be obtained from Magnetic Resonance Imaging (MRI), but it is not
currently possible to capture dynamic MRI-based geometries with sufficient
spatial and time resolution. One possible solution consists in interpolating
between static geometries, but this is a complex task. We instead propose
herein to use a semi-polar grid to extract 2D cross-sections from the
static 3D geometries, and then interpolate them to obtain the vocal
tract dynamics. Other approaches such as the adaptive grid have also
been explored. In this method, cross-sections are defined perpendicular
to the vocal tract midline, as typically done in 1D to obtain the vocal
tract area functions. However, intersections between adjacent cross-sections
may occur during the interpolation process, especially when the vocal
tract midline quickly changes its orientation. In contrast, the semi-polar
grid prevents these intersections because the plane orientations are
fixed over time. Finite element simulations of static vowels are first
conducted, showing that 3D acoustic wave propagation is not significantly
altered when the semi-polar grid is used instead of the adaptive grid.
The vowel-vowel sequence [&amp;#593;i] is finally simulated to demonstrate
the method.
</description>
    </item>
    
    <item>
        <title>A Fast Robust 1D Flow Model for a Self-Oscillating Coupled 2D FEM Vocal Fold Simulation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0844.PDF</link>
        <description>A balance between the simplicity and speed of lumped-element vocal
fold models and the completeness and complexity of continuum-models
is required to achieve fast high-quality articulatory speech synthesis.
We develop and implement a novel self-oscillating vocal-fold model,
composed of a 1D unsteady fluid model loosely coupled with a 2D FEM
structural model. The flow model is capable of robustly handling irregular
geometries, different boundary conditions, closure of the glottis and
unsteady flow states. A method for a fast decoupled solution of the
flow equations that does not require the computation of the Jacobian
is provided. The model is coupled with a 2D real-time finite-difference
wave-solver for simulating vocal tract acoustics and a 1D wave-reflection
analog representation of the trachea. The simulation results are shown
to agree with existing data in literature, and give realistic pressure-velocity
distributions, glottal width and glottal flow values. In addition,
the model is more than an order of magnitude faster to run than comparable
2D Navier-Stokes fluid solvers, while better capturing transitional
flow than simple Bernoulli-based flow models. The vocal fold model
provides an alternative to simple lumped-element models for faster
higher-quality articulatory speech synthesis.
</description>
    </item>
    
    <item>
        <title>Waveform Patterns in Pitch Glides Near a Vocal Tract Resonance</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0875.PDF</link>
        <description>A time-domain model of vowel production is used to simulate fundamental
frequency glides over the first vocal tract resonance. A vocal tract
geometry extracted from MRI data of a female speaker pronouncing [i]
is used. The model contains direct feedback from the acoustic loads
to vocal fold tissues and the inertial effect of the full air column
on the glottal flow. The simulations reveal that a perturbation pattern
in the fundamental frequency, namely, a jump and locking to the vocal
tract resonance, is accompanied by a specific pattern of glottal waveform
changes.
</description>
    </item>
    
    <item>
        <title>A Unified Numerical Simulation of Vowel Production That Comprises Phonation and the Emitted Sound</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Synthesis of VV Utterances from Muscle Activation to Sound with a 3D Model</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1614.PDF</link>
        <description>We propose a method to automatically generate deformable 3D vocal tract
geometries from the surrounding structures in a biomechanical model.
This allows us to couple 3D biomechanics and acoustics simulations.
The basis of the simulations is muscle activation trajectories in the
biomechanical model, which move the articulators to the desired articulatory
positions. The muscle activation trajectories for a vowel-vowel utterance
are here defined through interpolation between the determined activations
of the start and end vowel. The resulting articulatory trajectories
of flesh points on the tongue surface and jaw are similar to corresponding
trajectories measured using Electromagnetic Articulography, hence corroborating
the validity of interpolating muscle activation. At each time step
in the articulatory transition, a 3D vocal tract tube is created through
a cavity extraction method based on first slicing the geometry of the
articulators with a semi-polar grid to extract the vocal tract contour
in each plane and then reconstructing the vocal tract through a smoothed
3D mesh-generation using the extracted contours. A finite element method
applied to these changing 3D geometries simulates the acoustic wave
propagation. We present the resulting acoustic pressure changes on
the vocal tract boundary and the formant transitions for the utterance
[&amp;#593;i].
</description>
    </item>
    
    <item>
        <title>A Dual Source-Filter Model of Snore Audio for Snorer Group Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1211.PDF</link>
        <description>Snoring is a common symptom of serious chronic disease known as obstructive
sleep apnea (OSA). Knowledge about the location of obstruction site
(V&amp;#8212;Velum, O&amp;#8212;Oropharyngeal lateral walls, T&amp;#8212;Tongue,
E&amp;#8212;Epiglottis) in the upper airways is necessary for proper surgical
treatment. In this paper we propose a dual source-filter model similar
to the source-filter model of speech to approximate the generation
process of snore audio. The first filter models the vocal tract from
lungs to the point of obstruction with white noise excitation from
the lungs. The second filter models the vocal tract from the obstruction
point to the lips/nose with impulse train excitation which represents
vibrations at the point of obstruction. The filter coefficients are
estimated using the closed and open phases of the snore beat cycle.
VOTE classification is done by using SVM classifier and filter coefficients
as features. The classification experiments are performed on the development
set (283 snore audios) of the MUNICH-PASSAU SNORE SOUND CORPUS (MPSSC).We
obtain an unweighted average recall (UAR) of 49.58%, which is higher
than the INTERSPEECH-2017 snoring sub-challenge baseline technique
by &amp;#126;3% (absolute).
</description>
    </item>
    
    <item>
        <title>An &amp;#8216;End-to-Evolution&amp;#8217; Hybrid Approach for Snore Sound Classification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0173.PDF</link>
        <description>Whilst snoring itself is usually not harmful to a person&amp;#8217;s health,
it can be an indication of Obstructive Sleep Apnoea (OSA), a serious
sleep-related disorder. As a result, studies into using snoring as
acoustic based marker of OSA are gaining in popularity. Motivated by
this, the INTERSPEECH 2017 ComParE Snoring sub-challenge requires classification
from which areas in the upper airways different snoring sounds originate.
This paper explores a hybrid approach combining evolutionary feature
selection based on competitive swarm optimisation and deep convolutional
neural networks (CNN). Feature selection is applied to novel deep spectrum
features extracted directly from spectrograms using pre-trained image
classification CNN. Key results presented demonstrate that our hybrid
approach can substantially increase the performance of a linear support
vector machine on a set of low-level features extracted from the Snoring
sub-challenge data. Even without subset selection, the deep spectrum
features are sufficient to outperform the challenge baseline, and competitive
swarm optimisation further improves system performance. In comparison
to the challenge baseline, unweighted average recall is increased from
40.6% to 57.6% on the development partition, and from 58.5% to 66.5%
on the test partition, using 2246 of the 4096 deep spectrum features.
</description>
    </item>
    
    <item>
        <title>Snore Sound Classification Using Image-Based Deep Spectrum Features</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0434.PDF</link>
        <description>In this paper, we propose a method for automatically detecting various
types of snore sounds using image classification convolutional neural
network (CNN) descriptors extracted from audio file spectrograms. The
descriptors, denoted as deep spectrum features, are derived from forwarding
spectrograms through very deep task-independent pre-trained CNNs. Specifically,
activations of fully connected layers from two common image classification
CNNs, AlexNet and VGG19, are used as feature vectors. Moreover, we
investigate the impact of differing spectrogram colour maps and two
CNN architectures on the performance of the system. Results presented
indicate that deep spectrum features extracted from the activations
of the second fully connected layer of AlexNet using a viridis colour
map are well suited to the task. This feature space, when combined
with a support vector classifier, outperforms the more conventional
knowledge-based features of 6 373 acoustic functionals used in the
INTERSPEECH ComParE 2017 Snoring sub-challenge baseline system. In
comparison to the baseline, unweighted average recall is increased
from 40.6% to 44.8% on the development partition, and from 58.5% to
67.0% on the test partition.
</description>
    </item>
    
    <item>
        <title>Exploring Fusion Methods and Feature Space for the Classification of Paralinguistic Information</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1378.PDF</link>
        <description>This paper introduces the different systems developed by Aholab Signal
Processing Laboratory for The INTERSPEECH 2017 Computational Paralinguistics
Challenge, which includes three different subtasks: Addressee, Cold
and Snoring classification. Several classification strategies and features
related with the spectrum, prosody and phase have been tested separately
and further combined by using different fusion techniques, such as
early fusion by means of multi-feature vectors, late fusion of the
standalone classifier scores and label fusion via weighted voting.
The obtained results show that the applied fusion methods improve the
performance of the standalone detectors and provide systems capable
of outperforming the baseline systems in terms of UAR.
</description>
    </item>
    
    <item>
        <title>DNN-Based Feature Extraction and Classifier Combination for Child-Directed Speech, Cold and Snoring Identification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0905.PDF</link>
        <description>In this study we deal with the three sub-challenges of the Interspeech
ComParE Challenge 2017, where the goal is to identify child-directed
speech, speakers having a cold, and different types of snoring sounds.
For the first two sub-challenges we propose a simple, two-step feature
extraction and classification scheme: first we perform frame-level
classification via Deep Neural Networks (DNNs), and then we extract
utterance-level features from the DNN outputs. By utilizing these features
for classification, we were able to match the performance of the standard
paralinguistic approach (which involves extracting thousands of features,
many of them being completely irrelevant to the actual task). As for
the Snoring Sub-Challenge, we divided the recordings into segments,
and averaged out some frame-level features segment-wise, which were
then used for utterance-level classification. When combining the predictions
of the proposed approaches with those got by the standard paralinguistic
approach, we managed to outperform the baseline values of the Cold
and Snoring sub-challenges on the hidden test sets.
</description>
    </item>
    
    <item>
        <title>Introducing Weighted Kernel Classifiers for Handling Imbalanced Paralinguistic Corpora: Snoring, Addressee and Cold</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0653.PDF</link>
        <description>The field of paralinguistics is growing rapidly with a wide range of
applications that go beyond recognition of emotions, laughter and personality.
The research flourishes in multiple directions such as signal representation
and classification, addressing the issues of the domain. Apart from
the noise robustness, an important issue with real life data is the
imbalanced nature: some classes of states/traits are under-represented.
Combined with the high dimensionality of the feature vectors used in
the state-of-the-art analysis systems, this issue poses the threat
of over-fitting. While the kernel trick can be employed to handle the
dimensionality issue, regular classifiers inherently aim to minimize
the misclassification error and hence are biased towards the majority
class. A solution to this problem is over-sampling of the minority
class(es). However, this brings increased memory/computational costs,
while not bringing any new information to the classifier. In this work,
we propose a new weighting scheme on instances of the original dataset,
employing Weighted Kernel Extreme Learning Machine, and inspired from
that, introducing the Weighted Partial Least Squares Regression based
classifier. The proposed methods are applied on all three INTERSPEECH
ComParE 2017 challenge corpora, giving better or competitive results
compared to the challenge baselines.
</description>
    </item>
    
    <item>
        <title>Multitask Learning with Low-Level Auxiliary Tasks for Encoder-Decoder Based Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1118.PDF</link>
        <description>End-to-end training of deep learning-based models allows for implicit
learning of intermediate representations based on the final task loss.
However, the end-to-end approach ignores the useful domain knowledge
encoded in explicit intermediate-level supervision. We hypothesize
that using intermediate representations as auxiliary supervision at
lower levels of deep networks may be a good way of combining the advantages
of end-to-end training and more traditional pipeline approaches. We
present experiments on conversational speech recognition where we use
lower-level tasks, such as phoneme recognition, in a multitask training
approach with an encoder-decoder model for direct character transcription.
We compare multiple types of lower-level tasks and analyze the effects
of the auxiliary tasks. Our results on the Switchboard corpus show
that this approach improves recognition accuracy over a standard encoder-decoder
model on the Eval2000 test set.
</description>
    </item>
    
    <item>
        <title>Optimizing Expected Word Error Rate via Sampling for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0639.PDF</link>
        <description>State-level minimum Bayes risk (sMBR) training has become the de facto
standard for sequence-level training of speech recognition acoustic
models. It has an elegant formulation using the expectation semiring,
and gives large improvements in word error rate (WER) over models trained
solely using cross-entropy (CE) or connectionist temporal classification
(CTC). sMBR training optimizes the expected number of frames at which
the reference and hypothesized acoustic states differ. It may be preferable
to optimize the expected WER, but WER does not interact well with the
expectation semiring, and previous approaches based on computing expected
WER exactly involve expanding the lattices used during training. In
this paper we show how to perform optimization of the expected WER
by sampling paths from the lattices used during conventional sMBR training.
The gradient of the expected WER is itself an expectation, and so may
be approximated using Monte Carlo sampling. We show experimentally
that optimizing WER during acoustic model training gives 5% relative
improvement in WER over a well-tuned sMBR baseline on a 2-channel query
recognition task (Google Home).
</description>
    </item>
    
    <item>
        <title>Annealed f-Smoothing as a Mechanism to Speed up Neural Network Training</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0231.PDF</link>
        <description>In this paper, we describe a method to reduce the overall number of
neural network training steps, during both cross-entropy and sequence
training stages. This is achieved through the interpolation of frame-level
CE and sequence level SMBR criteria, during the sequence training stage.
This interpolation is known as f-smoothing and has previously been
just used to prevent overfitting during sequence training. However,
in this paper, we investigate its application to reduce the training
time. We explore different interpolation strategies to reduce the overall
training steps; and achieve a reduction of up to 25% with almost no
degradation in word error rate (WER). Finally, we explore the generalization
of f-smoothing to other tasks.
</description>
    </item>
    
    <item>
        <title>Non-Uniform MCE Training of Deep Long Short-Term Memory Recurrent Neural Networks for Keyword Spotting</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0583.PDF</link>
        <description>It has been shown in [1, 2] that improved performance can be achieved
by formulating the keyword spotting as a non-uniform error automatic
speech recognition problem. In this work, we discriminatively train
a deep bidirectional long short-term memory (BLSTM) &amp;#8212; hidden
Markov model (HMM) based acoustic model with non-uniform boosted minimum
classification error (BMCE) criterion which imposes more significant
error cost on the keywords than those on the non-keywords. By introducing
the BLSTM, the context information in both the past and the future
are stored and updated to predict the desired output and the long-term
dependencies within the speech signal are well captured. With non-uniform
BMCE objective, the BLSTM is trained so that the recognition errors
related to the keywords are remarkably reduced. The BLSTM is optimized
using back-propagation through time and stochastic gradient descent.
The keyword spotting system is implemented within weighted finite state
transducer framework. The proposed method achieves 5.49% and 7.37%
absolute figure-of-merit improvements respectively over the BLSTM and
the feedforward deep neural network baseline systems trained with cross-entropy
criterion for the keyword spotting task on Switchboard-1 Release 2
dataset.
</description>
    </item>
    
    <item>
        <title>Exploiting Eigenposteriors for Semi-Supervised Training of DNN Acoustic Models with Sequence Discrimination</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1784.PDF</link>
        <description>Deep neural network (DNN) acoustic models yield posterior probabilities
of senone classes. Recent studies support the existence of low-dimensional
subspaces underlying senone posteriors. Principal component analysis
(PCA) is applied to identify eigenposteriors and perform low-dimensional
projection of the training data posteriors. The resulted enhanced posteriors
are applied as soft targets for training better DNN acoustic model
under the student-teacher framework. The present work advances this
approach by studying incorporation of sequence discriminative training.
We demonstrate how to combine the gains from eigenposterior based enhancement
with sequence discrimination to improve ASR using semi-supervised training.
Evaluation on AMI meeting corpus yields nearly 4% absolute reduction
in word error rate (WER) compared to the baseline DNN trained with
cross entropy objective. In this context, eigenposterior enhancement
of the soft targets is crucial to enable additive improvement using
out-of-domain untranscribed data.
</description>
    </item>
    
    <item>
        <title>Discriminative Autoencoders for Acoustic Modeling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0221.PDF</link>
        <description>Speech data typically contain information irrelevant to automatic speech
recognition (ASR), such as speaker variability and channel/environmental
noise, lurking deep within acoustic features. Such unwanted information
is always mixed together to stunt the development of an ASR system.
In this paper, we propose a new framework based on autoencoders for
acoustic modeling in ASR. Unlike other variants of autoencoder neural
networks, our framework is able to isolate phonetic components from
a speech utterance by simultaneously taking two kinds of objectives
into consideration. The first one relates to the minimization of reconstruction
errors and benefits to learn most salient and useful properties of
the data. The second one functions in the middlemost code layer, where
the categorical distribution of the context-dependent phone states
is estimated for phoneme discrimination and the derivation of acoustic
scores, the proximity relationship among utterances spoken by the same
speaker are preserved, and the intra-utterance noise is modeled and
abstracted away. We describe the implementation of the discriminative
autoencoders for training tri-phone acoustic models and present TIMIT
phone recognition results, which demonstrate that our proposed method
outperforms the conventional DNN-based approach.
</description>
    </item>
    
    <item>
        <title>Speaker Diarization Using Convolutional Neural Network for Statistics Accumulation Refinement</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0051.PDF</link>
        <description>The aim of this paper is to investigate the benefit of information
from a speaker change detection system based on Convolutional Neural
Network (CNN) when applied to the process of accumulation of statistics
for an i-vector generation. The investigation is carried out on the
problem of diarization. In our system, the output of the CNN is a probability
value of a speaker change in a conversation for a given time segment.
According to this probability, we cut the conversation into short segments
that are then represented by the i-vector (to describe a speaker in
it). We propose a technique to utilize the information from the CNN
for the weighting of the acoustic data in a segment to refine the statistics
accumulation process. This technique enables us to represent the speaker
better in the final i-vector. The experiments on the English part of
the CallHome corpus show that our proposed refinement of the statistics
accumulation is beneficial with the relative improvement of Diarization
Error Rate almost by 16% when compared to the speaker diarization system
without statistics refinement.
</description>
    </item>
    
    <item>
        <title>Speaker2Vec: Unsupervised Learning and Adaptation of a Speaker Manifold Using Deep Neural Networks with an Evaluation on Speaker Segmentation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1650.PDF</link>
        <description>This paper presents a novel approach, we term  Speaker2Vec, to derive
a speaker-characteristics manifold learned in an unsupervised manner.
The proposed representation can be employed in different applications
such as diarization, speaker identification or, as in our evaluation
test case, speaker segmentation. Speaker2Vec exploits large amounts
of unlabeled training data and the assumption of short-term active-speaker
stationarity to derive a speaker embedding using Deep Neural Networks
(DNN). We assume that temporally-near speech segments belong to the
same speaker, and as such a joint representation connecting these nearby
segments can encode their common information. Thus, this bottleneck
representation will be capturing mainly speaker-specific information.
Such training can take place in a completely unsupervised manner. For
testing, our trained model generates the embeddings for the test audio,
and applies a simple distance metric to detect speaker-change points.
The paper also proposes a strategy for unsupervised adaptation of the
DNN models to the application domain. The proposed method outperforms
the state-of-the-art speaker segmentation algorithms and MFCC based
baseline methods on four evaluation datasets, while it allows for further
improvements by employing this embedding into supervised training methods.
</description>
    </item>
    
    <item>
        <title>A Triplet Ranking-Based Neural Network for Speaker Diarization and Linking</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0270.PDF</link>
        <description>This paper investigates a novel neural scoring method, based on conventional
 i-vectors, to perform speaker diarization and linking of large collections
of recordings. Using triplet loss for training, the network projects
 i-vectors in a space that better separates speakers in terms of cosine
similarity. Experiments are run on two French TV collections built
from REPERE [1] and ETAPE [2] campaigns corpora, the system being trained
on French Radio data. Results indicate that the proposed approach outperforms
conventional cosine and Probabilistic Linear Discriminant Analysis
scoring methods on both within- and cross-recording diarization tasks,
with a Diarization Error Rate reduction of 14% in average.
</description>
    </item>
    
    <item>
        <title>Estimating Speaker Clustering Quality Using Logistic Regression</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0492.PDF</link>
        <description>This paper focuses on estimating clustering validity by using logistic
regression. For many applications it might be important to estimate
the quality of the clustering, e.g. in case of speech segments&amp;#8217;
clustering, make a decision whether to use the clustered data for speaker
verification. In the case of short segments speakers clustering, the
common criteria for cluster validity are  average cluster purity (ACP),
 average speaker purity (ASP) and K &amp;#8212; the geometric mean between
the two measures. As in practice, true labels are not available for
evaluation, hence they have to be estimated from the clustering itself.
In this paper, mean-shift clustering with PLDA score is applied in
order to cluster short speaker segments represented as i-vectors. Different
statistical parameters are then estimated on the clustered data and
are used to train logistic regression to estimate ACP, ASP and K. It
was found that logistic regression can be a good predictor of the actual
ACP, ASP and K, and yields reasonable information regarding the clustering
quality.
</description>
    </item>
    
    <item>
        <title>Combining Speaker Turn Embedding and Incremental Structure Prediction for Low-Latency Speaker Diarization</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1067.PDF</link>
        <description>Real-time speaker diarization has many potential applications, including
public security, biometrics or forensics. It can also significantly
speed up the indexing of increasingly large multimedia archives. In
this paper, we address the issue of low-latency speaker diarization
that consists in continuously detecting new or reoccurring speakers
within an audio stream, and determining when each speaker is active
with a low latency ( e.g. every second). This is in contrast with most
existing approaches in speaker diarization that rely on multiple passes
over the complete audio recording. The proposed approach combines speaker
turn neural embeddings with an incremental structure prediction approach
inspired by state-of-the-art Natural Language Processing models for
Part-of-Speech tagging and dependency parsing. It can therefore leverage
both information describing the utterance and the inherent temporal
structure of interactions between speakers to learn, in supervised
framework, to identify speakers. Experiments on the Etape broadcast
news benchmark validate the approach.
</description>
    </item>
    
    <item>
        <title> pyannote.metrics: A Toolkit for Reproducible Evaluation, Diagnostic, and Error Analysis of Speaker Diarization Systems</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0411.PDF</link>
        <description> pyannote.metrics is an open-source Python library aimed at researchers
working in the wide area of speaker diarization. It provides a command
line interface (CLI) to improve reproducibility and comparison of speaker
diarization research results. Through its application programming interface
(API), a large set of evaluation metrics is available for diagnostic
purposes of all modules of typical speaker diarization pipelines (speech
activity detection, speaker change detection, clustering, and identification).
Finally, thanks to visualization capabilities, we show that it can
also be used for detailed error analysis purposes.  pyannote.metrics
can be downloaded from  http://pyannote.github.io.
</description>
    </item>
    
    <item>
        <title>A Rescoring Approach for Keyword Search Using Lattice Context Information</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1328.PDF</link>
        <description>In this paper we present a rescoring approach for keyword search (KWS)
based on neural networks (NN). This approach exploits only the lattice
context in a detected time interval instead of its corresponding audio.
The most informative arcs in lattice context are selected and represented
as a matrix, where words on arcs are represented in an embedding space
with respect to their pronunciations. Then convolutional neural networks
(CNNs) are employed to capture distinctive features from this matrix.
A rescoring model is trained to minimize term-weighted sigmoid cross
entropy so as to match the evaluation metric. Experiments on single-word
queries show that lattice context brings complementary gains over normalized
posterior scores. Performance on both in-vocabulary (IV) and out-of-vocabulary
(OOV) queries are improved by combining NN-based scores with standard
posterior scores.
</description>
    </item>
    
    <item>
        <title>The Kaldi OpenKWS System: Improving Low Resource Keyword Search</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0601.PDF</link>
        <description>The IARPA BABEL program has stimulated worldwide research in keyword
search technology for low resource languages, and the NIST OpenKWS
evaluations are the de facto benchmark test for such capabilities.
The 2016 OpenKWS evaluation featured Georgian speech, and had 10 participants
from across the world. This paper describes the Kaldi system developed
to assist IARPA in creating a competitive baseline against which participants
were evaluated, and to provide a truly open source system to all participants
to support their research. This system handily met the BABEL program
goals of 0.60 ATWV and 50% WER, achieving 0.70 ATWV and 38% WER with
a single ASR system, i.e.  without ASR system combination. All except
one OpenKWS participant used Kaldi components in their submissions,
typically in conjunction with system combination. This paper therefore
complements all other OpenKWS-based papers.
</description>
    </item>
    
    <item>
        <title>The STC Keyword Search System for OpenKWS 2016 Evaluation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Compressed Time Delay Neural Network for Small-Footprint Keyword Spotting</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0480.PDF</link>
        <description>In this paper we investigate a time delay neural network (TDNN) for
a keyword spotting task that requires low CPU, memory and latency.
The TDNN is trained with transfer learning and multi-task learning.
Temporal subsampling enabled by the time delay architecture reduces
computational complexity. We propose to apply singular value decomposition
(SVD) to further reduce TDNN complexity. This allows us to first train
a larger full-rank TDNN model which is not limited by CPU/memory constraints.
The larger TDNN usually achieves better performance. Afterwards, its
size can be compressed by SVD to meet the budget requirements. Hidden
Markov models (HMM) are used in conjunction with the networks to perform
keyword detection and performance is measured in terms of area under
the curve (AUC) for detection error tradeoff (DET) curves. Our experimental
results on a large in-house far-field corpus show that the full-rank
TDNN achieves a 19.7% DET AUC reduction compared to a similar-size
deep neural network (DNN) baseline. If we train a larger size full-rank
TDNN first and then reduce it via SVD to the comparable size of the
DNN, we obtain a 37.6% reduction in DET AUC compared to the DNN baseline.
</description>
    </item>
    
    <item>
        <title>Symbol Sequence Search from Telephone Conversation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0904.PDF</link>
        <description>We propose a method for searching for symbol sequences in conversations.
Symbol sequences can include phone numbers, credit card numbers, and
any kind of ticket (identification) numbers and are often communicated
in call center conversations. Automatic extraction of these from speech
is a key to many automatic speech recognition (ASR) applications such
as question answering and summarization. Compared with spoken term
detection (STD), symbol sequence searches have two additional problems.
First, the entire symbol sequence is typically not observed continuously
but in sub sequences, where customers or agents speak these sequences
in fragments, while the recipient repeats them to ensure they have
the correct sequence. Second, we have to distinguish between different
symbol sequences, for example, phone numbers versus ticket numbers
or customer identification numbers. To deal with these problems, we
propose to apply STD to symbol-sequence fragments and subsequently
use confidence scoring to obtain the entire symbol sequence. For the
confidence scoring, We propose a long short-term memory (LSTM) based
approach that inputs word before and after fragments. We also propose
to detect repetitions of fragments and use it for confidence scoring.
Our proposed method achieves a 0.87 F-measure, in an eight-digit customer
identification number search task, when operating at 20.3% WER.
</description>
    </item>
    
    <item>
        <title>Similarity Learning Based Query Modeling for Keyword Search</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1273.PDF</link>
        <description>In this paper, we propose a novel approach for query modeling using
neural networks for posteriorgram based keyword search (KWS). We aim
to help the conventional large vocabulary continuous speech recognition
(LVCSR) based KWS systems, especially on out-of-vocabulary (OOV) terms
by converting the task into a template matching problem, just like
the query-by-example retrieval tasks. For this, we use a dynamic time
warping (DTW) based similarity search on the speaker independent posteriorgram
space. In order to model the text queries as posteriorgrams, we propose
a non-symmetric Siamese neural network structure which both learns
a distance measure to be used in DTW and the frame representations
for this specific measure. We compare this new technique with similar
DTW based systems using other distance measures and query modeling
techniques. We also apply system fusion of the proposed system with
the LVCSR based baseline KWS system. We show that, the proposed system
works significantly better than other similar systems. Furthermore,
when combined with the LVSCR based baseline, the proposed system provides
up to 37.9% improvement on OOV terms and 9.8% improvement on all terms.
</description>
    </item>
    
    <item>
        <title>Deep Recurrent Neural Network Based Monaural Speech Separation Using Recurrent Temporal Restricted Boltzmann Machines</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0057.PDF</link>
        <description>This paper presents a single-channel speech separation method implemented
with a deep recurrent neural network (DRNN) using recurrent temporal
restricted Boltzmann machines (RTRBM). Although deep neural network
(DNN) based speech separation (denoising task) methods perform quite
well compared to the conventional statistical model based speech enhancement
techniques, in DNN-based methods, the temporal correlations across
speech frames are often ignored, resulting in loss of spectral detail
in the reconstructed output speech. In order to alleviate this issue,
one RTRBM is employed for modelling the acoustic features of input
(mixture) signal and two RTRBMs are trained for the two training targets
(source signals). Each RTRBM attempts to model the abstractions present
in the training data at each time step as well as the temporal dependencies
in the training data. The entire network (consisting of three RTRBMs
and one recurrent neural network) can be fine-tuned by the joint optimization
of the DRNN with an extra masking layer which enforces a reconstruction
constraint. The proposed method has been evaluated on the IEEE corpus
and TIMIT dataset for speech denoising task. Experimental results have
established that the proposed approach outperforms NMF and conventional
DNN and DRNN-based speech enhancement methods.
</description>
    </item>
    
    <item>
        <title>Improved Codebook-Based Speech Enhancement Based on MBE Model</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0109.PDF</link>
        <description>This paper provides an improved codebook-based speech enhancement method
using multi-band excitation (MBE) model. It aims to remove the noise
between the harmonics, which may exist in codebook-based enhanced speech.
In general, the proposed system is based on analysis-with-synthesis
(AwS) framework. During the analysis stage, acoustic features are extracted
including pitch, harmonic magnitude and voicing from noisy speech.
These parameters are obtained on the basis of the spectral magnitudes
obtained by codebook-based method. During the synthesis stage, different
synthesis strategies for voiced and unvoiced speech are employed. Besides,
this paper introduces speech presence probability to modify the codebook-based
Wiener filter so that more accurate acoustic parameters can be obtained.
The proposed system can eliminate noise not only between the harmonics,
but also in the silent segments, especially in low SNR noise environment.
Experiments show that, the performance of the proposed method is better
than traditional codebook-based method for different types of noise.
</description>
    </item>
    
    <item>
        <title>Improving Mask Learning Based Speech Enhancement System with Restoration Layers and Residual Connection</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0515.PDF</link>
        <description>For single-channel speech enhancement, mask learning based approach
through neural network has been shown to outperform the feature mapping
approach, and to be effective as a pre-processor for automatic speech
recognition. However, its assumption that the mixture and clean reference
must have the correspondent scale doesn&amp;#8217;t hold in data collected
from real world, and thus leads to significant performance degradation
on parallel recorded data. In this paper, we first extend the mask
learning based speech enhancement by integrating two types of restoration
layer to address the scale mismatch problem. We further propose a novel
residual learning based speech enhancement model via adding different
shortcut connections to a feature mapping network. We show such a structure
can benefit from both the mask learning and the feature mapping. We
evaluate the proposed speech enhancement models on CHiME 3 data. Without
retraining the acoustic model, the best bi-direction LSTM with residue
connections yields 24.90% relative WER reduction on real data and 34.57%
WER on simulated data.
</description>
    </item>
    
    <item>
        <title>Exploring Low-Dimensional Structures of Modulation Spectra for Robust Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0611.PDF</link>
        <description>Developments of noise robustness techniques are vital to the success
of automatic speech recognition (ASR) systems in face of varying sources
of environmental interference. Recent studies have shown that exploring
low-dimensional structures of speech features can yield good robustness.
Along this vein, research on low-rank representation (LRR), which considers
the intrinsic structures of speech features lying on some low dimensional
subspaces, has gained considerable interest from the ASR community.
When speech features are contaminated with various types of environmental
noise, its corresponding modulation spectra can be regarded as superpositions
of unstructured sparse noise over the inherent linguistic information.
As such, we in this paper endeavor to explore the low dimensional structures
of modulation spectra, in the hope to obtain more noise-robust speech
features. The main contribution is that we propose a novel use of the
LRR-based method to discover the subspace structures of modulation
spectra, thereby alleviating the negative effects of noise interference.
Furthermore, we also extensively compare our approach with several
well-practiced feature-based normalization methods. All experiments
were conducted and verified on the Aurora-4 database and task. The
empirical results show that the proposed LRR-based method can provide
significant word error reductions for a typical DNN-HMM hybrid ASR
system.
</description>
    </item>
    
    <item>
        <title>SEGAN: Speech Enhancement Generative Adversarial Network</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1428.PDF</link>
        <description>Current speech enhancement techniques operate on the spectral domain
and/or exploit some higher-level feature. The majority of them tackle
a limited number of noise conditions and rely on first-order statistics.
To circumvent these issues, deep networks are being increasingly used,
thanks to their ability to learn complex functions from large example
sets. In this work, we propose the use of generative adversarial networks
for speech enhancement. In contrast to current techniques, we operate
at the waveform level, training the model end-to-end, and incorporate
28 speakers and 40 different noise conditions into the same model,
such that model parameters are shared across them. We evaluate the
proposed model using an independent, unseen test set with two speakers
and 20 alternative noise conditions. The enhanced samples confirm the
viability of the proposed model, and both objective and subjective
evaluations confirm the effectiveness of it. With that, we open the
exploration of generative architectures for speech enhancement, which
may progressively incorporate further speech-centric design choices
to improve their performance.
</description>
    </item>
    
    <item>
        <title>Concatenative Resynthesis Using Twin Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1653.PDF</link>
        <description>Traditional noise reduction systems modify a noisy signal to make it
more like the original clean signal. For speech, these methods suffer
from two main problems: under-suppression of noise and over-suppression
of target speech. Instead, synthesizing clean speech based on the noisy
signal could produce outputs that are both noise-free and high quality.
Our previous work introduced such a system using concatenative synthesis,
but it required processing the clean speech at run time, which was
slow and not scalable. In order to make such a system scalable, we
propose here learning a similarity metric using two separate networks,
one network processing the clean segments offline and another processing
the noisy segments at run time. This system incorporates a ranking
loss to optimize for the retrieval of appropriate clean speech segments.
This model is compared against our original on the CHiME2-GRID corpus,
measuring ranking performance and subjective listening tests of resyntheses.
</description>
    </item>
    
    <item>
        <title>Combining Residual Networks with LSTMs for Lipreading</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0085.PDF</link>
        <description>We propose an end-to-end deep learning architecture for word-level
visual speech recognition. The system is a combination of spatiotemporal
convolutional, residual and bidirectional Long Short-Term Memory networks.
We train and evaluate it on the Lipreading In-The-Wild benchmark, a
challenging database of 500-size target-words consisting of 1.28sec
video excerpts from BBC TV broadcasts. The proposed network attains
word accuracy equal to 83.0%, yielding 6.8% absolute improvement over
the current state-of-the-art, without using information about word
boundaries during training or testing.
</description>
    </item>
    
    <item>
        <title>Improving Computer Lipreading via DNN Sequence Discriminative Training Techniques</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Improving Speaker-Independent Lipreading with Domain-Adversarial Training</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0421.PDF</link>
        <description>We present a  Lipreading system, i.e. a speech recognition system using
only visual features, which uses  domain-adversarial training for speaker
independence. Domain-adversarial training is integrated into the optimization
of a lipreader based on a stack of feedforward and LSTM (Long Short-Term
Memory) recurrent neural networks, yielding an end-to-end trainable
system which only requires a very small number of frames of  untranscribed
target data to substantially improve the recognition accuracy on the
target speaker. On pairs of different source and target speakers, we
achieve a relative accuracy improvement of around 40% with only 15
to 20 seconds of untranscribed target speech data. On multi-speaker
training setups, the accuracy improvements are smaller but still substantial.
</description>
    </item>
    
    <item>
        <title>Turbo Decoders for Audio-Visual Continuous Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0799.PDF</link>
        <description>Visual speech, i.e., video recordings of speakers&amp;#8217; mouths, plays
an important role in improving the robustness properties of automatic
speech recognition (ASR) against noise. Optimal fusion of audio and
video modalities is still one of the major challenges that attracts
significant interest in the realm of audio-visual ASR. Recently, turbo
decoders (TDs) have been successful in addressing the audio-visual
fusion problem. The idea of the TD framework is to iteratively exchange
some kind of soft information between the audio and video decoders
until convergence. The forward-backward algorithm (FBA) is mostly applied
to the decoding graphs to estimate this soft information. Applying
the FBA to the complex graphs that are usually used in large vocabulary
tasks may be computationally expensive. In this paper, I propose to
apply the forward-backward algorithm to a lattice of most likely state
sequences instead of using the entire decoding graph. Using lattices
allows for TD to be easily applied to large vocabulary tasks. The proposed
approach is evaluated using the newly released TCD-TIMIT corpus, where
a standard recipe for large vocabulary ASR is employed. The modified
TD performs significantly better than the feature and decision fusion
models in all clean and noisy test conditions.
</description>
    </item>
    
    <item>
        <title>DNN-Based Ultrasound-to-Speech Conversion for a Silent Speech Interface</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0939.PDF</link>
        <description>In this paper we present our initial results in articulatory-to-acoustic
conversion based on tongue movement recordings using Deep Neural Networks
(DNNs). Despite the fact that deep learning has revolutionized several
fields, so far only a few researchers have applied DNNs for this task.
Here, we compare various possible feature representation approaches
combined with DNN-based regression. As the input, we recorded synchronized
2D ultrasound images and speech signals. The task of the DNN was to
estimate Mel-Generalized Cepstrum-based Line Spectral Pair (MGC-LSP)
coefficients, which then served as input to a standard pulse-noise
vocoder for speech synthesis. As the raw ultrasound images have a relatively
high resolution, we experimented with various feature selection and
transformation approaches to reduce the size of the feature vectors.
The synthetic speech signals resulting from the various DNN configurations
were evaluated both using objective measures and a subjective listening
test. We found that the representation that used several neighboring
image frames in combination with a feature selection method was preferred
both by the subjects taking part in the listening experiments, and
in terms of the Normalized Mean Squared Error. Our results may be useful
for creating Silent Speech Interface applications in the future. 
</description>
    </item>
    
    <item>
        <title>Visually Grounded Learning of Keyword Prediction from Untranscribed Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0502.PDF</link>
        <description>During language acquisition, infants have the benefit of visual cues
to ground spoken language. Robots similarly have access to audio and
visual sensors. Recent work has shown that images and spoken captions
can be mapped into a meaningful common space, allowing images to be
retrieved using speech and vice versa. In this setting of images paired
with untranscribed spoken captions, we consider whether computer vision
systems can be used to obtain textual labels for the speech. Concretely,
we use an image-to-words multi-label visual classifier to tag images
with soft textual labels, and then train a neural network to map from
the speech to these soft targets. We show that the resulting speech
system is able to predict which words occur in an utterance &amp;#8212;
acting as a spoken bag-of-words classifier &amp;#8212; without seeing any
parallel speech and text. We find that the model often confuses semantically
related words, e.g. &amp;#8220;man&amp;#8221; and &amp;#8220;person&amp;#8221;, making
it even more effective as a  semantic keyword spotter.
</description>
    </item>
    
    <item>
        <title>Deep Neural Factorization for Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0892.PDF</link>
        <description>Conventional speech recognition system is constructed by unfolding
the spectral-temporal input matrices into one-way vectors and using
these vectors to estimate the affine parameters of neural network according
to the vector-based error back-propagation algorithm. System performance
is constrained because the contextual correlations in frequency and
time horizons are disregarded and the spectral and temporal factors
are excluded. This paper proposes a spectral-temporal factorized neural
network (STFNN) to tackle this weakness. The spectral-temporal structure
is preserved and factorized in hidden layers through two ways of factor
matrices which are trained by using the factorized error backpropagation.
Affine transformation in standard neural network is generalized to
the spectro-temporal factorization in STFNN. The structural features
or patterns are extracted and forwarded towards the softmax outputs.
A deep neural factorization is built by cascading a number of factorization
layers with fully-connected layers for speech recognition. An orthogonal
constraint is imposed in factor matrices for redundancy reduction.
Experimental results show the merit of integrating the factorized features
in deep feedforward and recurrent neural networks for speech recognition.
</description>
    </item>
    
    <item>
        <title>Semi-Supervised DNN Training with Word Selection for ASR</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Gaussian Prediction Based Attention for Online End-to-End Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0751.PDF</link>
        <description>Recently end-to-end speech recognition has obtained much attention.
One of the popular models to achieve end-to-end speech recognition
is attention based encoder-decoder model, which usually generating
output sequences iteratively by attending the whole representations
of the input sequences. However, predicting outputs until receiving
the whole input sequence is not practical for online or low time latency
speech recognition. In this paper, we present a simple but effective
attention mechanism which can make the encoder-decoder model generate
outputs without attending the entire input sequence and can apply to
online speech recognition. At each prediction step, the attention is
assumed to be a time-moving gaussian window with variable size and
can be predicted by using previous input and output information instead
of the content based computation on the whole input sequence. To further
improve the online performance of the model, we employ deep convolutional
neural networks as encoder. Experiments show that the gaussian prediction
based attention works well and under the help of deep convolutional
neural networks the online model achieves 19.5% phoneme error rate
in TIMIT ASR task.
</description>
    </item>
    
    <item>
        <title>Efficient Knowledge Distillation from an Ensemble of Teachers</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0614.PDF</link>
        <description>This paper describes the effectiveness of knowledge distillation using
teacher student training for building accurate and compact neural networks.
We show that with knowledge distillation, information from multiple
acoustic models like very deep VGG networks and Long Short-Term Memory
(LSTM) models can be used to train standard convolutional neural network
(CNN) acoustic models for a variety of systems requiring a quick turnaround.
We examine two strategies to leverage multiple teacher labels for training
student models. In the first technique, the weights of the student
model are updated by switching teacher labels at the minibatch level.
In the second method, student models are trained on multiple streams
of information from various teacher distributions via data augmentation.
We show that standard CNN acoustic models can achieve comparable recognition
accuracy with much smaller number of model parameters compared to teacher
VGG and LSTM acoustic models. Additionally we also investigate the
effectiveness of using broadband teacher labels as privileged knowledge
for training better narrowband acoustic models within this framework.
We show the benefit of this simple technique by training narrowband
student models with broadband teacher soft labels on the Aurora 4 task.
</description>
    </item>
    
    <item>
        <title>An Analysis of &amp;#8220;Attention&amp;#8221; in Sequence-to-Sequence Models</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0232.PDF</link>
        <description>In this paper, we conduct a detailed investigation of attention-based
models for automatic speech recognition (ASR). First, we explore different
types of attention, including &amp;#8220;online&amp;#8221; and &amp;#8220;full-sequence&amp;#8221;
attention. Second, we explore different subword units to see how much
of the end-to-end ASR process can reasonably be captured by an attention
model. In experimental evaluations, we find that although attention
is typically focused over a small region of the acoustics during each
step of next label prediction, &amp;#8220;full-sequence&amp;#8221; attention
outperforms &amp;#8220;online&amp;#8221; attention, although this gap can be
significantly reduced by increasing the length of the segments over
which attention is computed. Furthermore, we find that context-independent
phonemes are a reasonable sub-word unit for attention models. When
used in the second-pass to rescore N-best hypotheses, these models
provide over a 10% relative improvement in word error rate.
</description>
    </item>
    
    <item>
        <title>Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1566.PDF</link>
        <description>We present results that show it is possible to build a competitive,
greatly simplified, large vocabulary continuous speech recognition
system with whole words as acoustic units. We model the output vocabulary
of about 100,000 words directly using deep bi-directional LSTM RNNs
with CTC loss. The model is trained on 125,000 hours of semi-supervised
acoustic training data, which enables us to alleviate the data sparsity
problem for word models. We show that the CTC word models work very
well as an end-to-end all-neural speech recognition model without the
use of traditional context-dependent sub-word phone units that require
a pronunciation lexicon, and without any language model removing the
need to decode. We demonstrate that the CTC word models perform better
than a strong, more complex, state-of-the-art baseline with sub-word
units.
</description>
    </item>
    
    <item>
        <title>CNN-Based Joint Mapping of Short and Long Utterance i-Vectors for Speaker Verification Using Short Utterances</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0430.PDF</link>
        <description>Text-independent speaker recognition using short utterances is a highly
challenging task due to the large variation and content mismatch between
short utterances. I-vector and probabilistic linear discriminant analysis
(PLDA) based systems have become the standard in speaker verification
applications, but they are less effective with short utterances. To
address this issue, we propose a novel method, which trains a convolutional
neural network (CNN) model to map the i-vectors extracted from short
utterances to the corresponding long-utterance i-vectors. In order
to simultaneously learn the representation of the original short-utterance
i-vectors and fit the target long-version i-vectors, we jointly train
a supervised-regression model with an autoencoder using CNNs. The trained
CNN model is then used to generate the mapped version of short-utterance
i-vectors in the evaluation stage. We compare our proposed CNN-based
joint mapping method with a GMM-based joint modeling method under matched
and mismatched PLDA training conditions. Experimental results using
the NIST SRE 2008 dataset show that the proposed technique achieves
up to 30% relative improvement under duration mismatched PLDA-training
conditions and outperforms the GMM-based method. The improved systems
also perform better compared with the matched-length PLDA training
condition using short utterances.
</description>
    </item>
    
    <item>
        <title>Curriculum Learning Based Probabilistic Linear Discriminant Analysis for Noise Robust Speaker Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1199.PDF</link>
        <description>This study introduces a novel Curriculum Learning based Probabilistic
Linear Discriminant Analysis (CL-PLDA) algorithm for improving speaker
recognition in noisy conditions. CL-PLDA operates by initializing the
training EM algorithm with cleaner data ( easy examples), and successively
adds noisier data ( difficult examples) as the training progresses.
This curriculum learning based approach guides the parameters of CL-PLDA
to better local minima compared to regular PLDA. We test CL-PLDA on
speaker verification task of the severely noisy and degraded DARPA
RATS data, and show it to significantly outperform regular PLDA across
test-sets of varying duration.
</description>
    </item>
    
    <item>
        <title>i-Vector Transformation Using a Novel Discriminative Denoising Autoencoder for Noise-Robust Speaker Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0731.PDF</link>
        <description>This paper proposes i-vector transformations using neural networks
for achieving noise-robust speaker recognition. A novel discriminative
denoising autoencoder (DDAE) is employed on i-vectors to remove additive
noise effects. The DDAE is trained to denoise and classify noisy i-vectors
simultaneously, making it possible to add discriminability to the denoised
i-vectors. Speaker recognition experiments on the NIST SRE 2012 task
shows 32% better error performance as compared to a baseline system.
Also, our proposed method outperforms such conventional methods as
multi-condition training and a basic denoising autoencoder.
</description>
    </item>
    
    <item>
        <title>Unsupervised Discriminative Training of PLDA for Domain Adaptation in Speaker Verification</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0727.PDF</link>
        <description>This paper presents, for the first time, unsupervised discriminative
training of probabilistic linear discriminant analysis (unsupervised
DT-PLDA). While discriminative training avoids the problem of generative
training based on probabilistic model assumptions that often do not
agree with actual data, it has been difficult to apply it to unsupervised
scenarios because it can fit data with almost any labels. This paper
focuses on unsupervised training of DT-PLDA in the application of domain
adaptation in i-vector based speaker verification systems, using unlabeled
in-domain data. The proposed method makes it possible to conduct discriminative
training, i.e., estimation of model parameters and unknown labels,
by employing data statistics as a regularization term in addition to
the original objective function in DT-PLDA. An experiment on a NIST
Speaker Recognition Evaluation task shows that the proposed method
outperforms a conventional method using speaker clustering and performs
almost as well as supervised DT-PLDA.
</description>
    </item>
    
    <item>
        <title>Speaker Verification Under Adverse Conditions Using i-Vector Adaptation and Neural Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1240.PDF</link>
        <description>The main challenges introduced in the 2016 NIST speaker recognition
evaluation (SRE16) are domain mismatch between training and evaluation
data, duration variability in test recordings and unlabeled in-domain
training data. This paper outlines the systems developed at CRIM for
SRE16. To tackle the domain mismatch problem, we apply minimum divergence
training to adapt a conventional i-vector extractor to the task domain.
Specifically, we take an out-of-domain trained i-vector extractor as
an initialization and perform few iterations of minimum divergence
training on the unlabeled data provided. Next, we non-linearly transform
the adapted i-vectors by learning a speaker classifier neural network.
Speaker features extracted from this network have been shown to be
more robust than i-vectors under domain mismatch conditions with a
reduction in equal error rates of 2&amp;#8211;3% absolute. Finally, we
propose a new Beta-Bernoulli backend that models the features supplied
by the speaker classifier network. Our best single system is the speaker
classifier network - Beta-Bernoulli backend combination. Overall system
performance was very satisfactory for the fixed condition task. With
our submitted fused system we achieve an equal error rate of 9.89%.
</description>
    </item>
    
    <item>
        <title>Improving Robustness of Speaker Recognition to New Conditions Using Unlabeled Data</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0605.PDF</link>
        <description>Unsupervised techniques for the adaptation of speaker recognition are
important due to the problem of condition mismatch that is prevalent
when applying speaker recognition technology to new conditions and
the general scarcity of labeled &amp;#8216;in-domain&amp;#8217; data. In the
recent NIST 2016 Speaker Recognition Evaluation (SRE), symmetric score
normalization (S-norm) and calibration using unlabeled in-domain data
were shown to be beneficial. Because calibration requires speaker labels
for training, speaker-clustering techniques were used to generate pseudo-speakers
for learning calibration parameters in those cases where only unlabeled
in-domain data was available. These methods performed well in the SRE16.
It is unclear, however, whether those techniques generalize well to
other data sources. In this work, we benchmark these approaches on
several distinctly different databases, after we describe our SRI-CON-UAM
team system submission for the NIST 2016 SRE. Our analysis shows that
while the benefit of S-norm is also observed across other datasets,
applying speaker-clustered calibration provides considerably greater
benefit to the system in the context of new acoustic conditions.
</description>
    </item>
    
    <item>
        <title>CALYOU: A Comparable Spoken Algerian Corpus Harvested from YouTube</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1305.PDF</link>
        <description>This paper addresses the issue of comparability of comments extracted
from Youtube. The comments concern spoken Algerian that could be either
local Arabic, Modern Standard Arabic or French. This diversity of expression
gives rise to a huge number of problems concerning the data processing.
In this article, several methods of alignment will be proposed and
tested. The method which permits to best align is Word2Vec-based approach
that will be used iteratively. This recurrent call of Word2Vec allows
us improve significantly the results of comparability. In fact, a dictionary-based
approach leads to a Recall of 4, while our approach allows one to get
a Recall of 33 at rank 1. Thanks to this approach, we built from Youtube
CALYOU, a Comparable Corpus of the spoken Algerian.
</description>
    </item>
    
    <item>
        <title>PRAV: A Phonetically Rich Audio Visual Corpus</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0242.PDF</link>
        <description>This paper describes the acquisition of PRAV, a phonetically rich audio-visual
Corpus. The PRAV Corpus contains audio as well as visual recordings
of 2368 sentences from the TIMIT corpus each spoken by four subjects,
making it the largest audio-visual corpus in the literature in terms
of the number of sentences per subject. Visual features, comprising
the coordinates of points along the contour of the subjects lips, have
been extracted for the entire PRAV Corpus using the Active Appearance
Models (AAM) algorithm and have been made available along with the
audio and video recordings. The subjects being Indian makes PRAV an
ideal resource for audio-visual speech study with non-native English
speakers. Moreover, this paper describes how the large number of sentences
per subject makes the PRAV Corpus a significant dataset by highlighting
its utility in exploring a number of potential research problems including
visual speech synthesis and perception studies.
</description>
    </item>
    
    <item>
        <title>NTCD-TIMIT: A New Database and Baseline for Noise-Robust Audio-Visual Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0860.PDF</link>
        <description>Although audio-visual speech is well known to improve the robustness
properties of automatic speech recognition (ASR) systems against noise,
the realm of audio-visual ASR (AV-ASR) has not gathered the research
momentum it deserves. This is mainly due to the lack of audio-visual
corpora and the need to combine two fields of knowledge: ASR and computer
vision. This paper describes the NTCD-TIMIT database and baseline that
can overcome these two barriers and attract more research interest
to AV-ASR. The NTCD-TIMIT corpus has been created by adding six noise
types at a range of signal-to-noise ratios to the speech material of
the recently published TCD-TIMIT corpus. NTCD-TIMIT comprises visual
features that have been extracted from the TCD-TIMIT video recordings
using the visual front-end presented in this paper. The database contains
also Kaldi scripts for training and decoding audio-only, video-only,
and audio-visual ASR models. The baseline experiments and results obtained
using these scripts are detailed in this paper.
</description>
    </item>
    
    <item>
        <title>The Extended SPaRKy Restaurant Corpus: Designing a Corpus with Variable Information Density</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Automatic Construction of the Finnish Parliament Speech Corpus</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1115.PDF</link>
        <description>Automatic speech recognition (ASR) systems require large amounts of
transcribed speech data, for training state-of-the-art deep neural
network (DNN) acoustic models. Transcribed speech is a scarce and expensive
resource, and ASR systems are prone to underperform in domains where
there is not a lot of training data available. In this work, we open
up a vast and previously unused resource of transcribed speech for
Finnish, by retrieving and aligning all the recordings and meeting
transcripts from the web portal of the Parliament of Finland. Short
speech-text segment pairs are retrieved from the audio and text material,
by using the Levenshtein algorithm to align the first-pass ASR hypotheses
with the corresponding meeting transcripts. DNN acoustic models are
trained on the automatically constructed corpus, and performance is
compared to other models trained on a commercially available speech
corpus. Model performance is evaluated on Finnish parliament speech,
by dividing the testing set into seen and unseen speakers. Performance
is also evaluated on broadcast speech to test the general applicability
of the parliament speech corpus. We also study the use of meeting transcripts
in language model adaptation, to achieve additional gains in speech
recognition accuracy of Finnish parliament speech.
</description>
    </item>
    
    <item>
        <title>Building Audio-Visual Phonetically Annotated Arabic Corpus for Expressive Text to Speech</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>What is the Relevant Population? Considerations for the Computation of Likelihood Ratios in Forensic Voice Comparison</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1368.PDF</link>
        <description>In forensic voice comparison, it is essential to consider not only
the similarity between samples, but also the typicality of the evidence
in the relevant population. This is explicit within the likelihood
ratio (LR) framework. A significant issue, however, is the definition
of the  relevant population. This paper explores the complexity of
population selection for voice evidence. We evaluate the effects of
population specificity in terms of regional background on LR output
using combinations of the F1, F2, and F3 trajectories of the diphthong
/a&amp;#618;/. LRs were computed using development and reference data which
were regionally  matched (Standard Southern British English) and  mixed
(general British English) relative to the test data. These conditions
reflect the paradox that without knowing who the offender is, it is
not possible to know the population of which he is a member. Results
show that the more specific population produced stronger evidence and
better system validity than the more general definition. However, as
region-specific voice features (lower formants) were removed, the difference
in the output from the  matched and  mixed systems was reduced. This
shows that the effects of population selection are dependent on the
sociolinguistic constraints on the feature analysed.
</description>
    </item>
    
    <item>
        <title>Voice Disguise vs. Impersonation: Acoustic and Perceptual Measurements of Vocal Flexibility in Non Experts</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Schwa Realization in French: Using Automatic Speech Processing to Study Phonological and Socio-Linguistic Factors in Large Corpora</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0470.PDF</link>
        <description>The study investigates different factors influencing schwa realization
in French: phonological factors, speech style, gender, and socio-professional
status. Three large corpora, two of public journalistic speech (ESTER
and ETAPE) and one of casual speech (NCCFr) are used. The absence/presence
of schwa is automatically decided via forced alignment, which has a
successful performance rate of 95%. Only polysyllabic words including
a potential schwa in the word-initial syllable are studied in order
to control for variability in word structure and position. The effect
of the left context, grouped into classes of a word final vowel or
final consonant or a pause, is studied. Words preceded by a vowel (V#)
tend to favor schwa deletion. Interestingly, words preceded by a consonant
or a pause have similar behaviors: speakers tend to maintain schwa
in both contexts. As can be expected, the more casual the speech, the
more frequently schwa is dropped. Males tend to delete more schwas
than females, and journalists are more likely to delete schwa than
politicians. These results suggest that beyond phonology, other factors
such as gender, style and socio-professional status influence the realization
of schwa.
</description>
    </item>
    
    <item>
        <title>The Social Life of Setswana Ejectives</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0922.PDF</link>
        <description>This paper presents a first phonetic analysis of voiced, devoiced and
ejectivized stop sounds in Setswana taken from two different speech
databases. It is observed that rules governing the voicing/devoicing
processes depend on sociophonetic and ethnolinguistic factors. Speakers,
especially women, from the rural North West area of South Africa tend
to preserve the phonologically stronger devoiced (or even ejectivized)
forms, both in single standing plosives as well as in the post-nasal
context (NC&amp;#x325;). On the other hand, in the more industrialized
area of Gauteng, voiced forms of plosives prevail. The empirically
observed data is modelled with  KaMoso, a computational multi-agent
simulation framework. So far, this framework focused on open social
structures ( whole world networks) that facilitate language modernization
through exchange between different phonetic forms. The updated model
has been enriched with social/phonetic simulation scenarios in which
speech agents interact between each other in a so-called  parochial
setting, reflecting smaller, closed communities. Both configurations
correspond to the sociopolitical changes that have been taking place
in South Africa over the last decades, showing the differences in speech
between women and men from rural and industrialized areas of the country.
</description>
    </item>
    
    <item>
        <title>How Long is Too Long? How Pause Features After Requests Affect the Perceived Willingness of Affirmative Answers</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0050.PDF</link>
        <description>A perception experiment involving 28 German listeners is presented.
It investigates &amp;#8212; for sequences of request, pause, and affirmative
answer &amp;#8212; the effect of pause duration on the answerer&amp;#8217;s
perceived willingness to comply with the request. Replicating earlier
results on American English, perceived willingness was found to decrease
with increasing pause duration, particularly above a &amp;#8220;tolerance
threshold&amp;#8221; of 600 ms. Refining and qualifying this replicated
result, the perception experiment showed additional effects of speaking-rate
context and pause quality (silence vs. breathing vs. caf&amp;#233; noise)
on perceived willingness judgments. The overall results picture is
discussed with respect to the origin of the &amp;#8220;tolerance threshold&amp;#8221;,
the status of breathing in speech, and the function of pauses in communication.
</description>
    </item>
    
    <item>
        <title>Shadowing Synthesized Speech &amp;#8212; Segmental Analysis of Phonetic Convergence</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1433.PDF</link>
        <description>To shed light on the question whether humans converge phonetically
to synthesized speech, a shadowing experiment was conducted using three
different types of stimuli &amp;#8212; natural speaker, diphone synthesis,
and HMM synthesis. Three segment-level phonetic features of German
that are well-known to vary across native speakers were examined. The
first feature triggered convergence in roughly one third of the cases
for all stimulus types. The second feature showed generally a small
amount of convergence, which may be due to the nature of the feature
itself. Still the effect was strongest for the natural stimuli, followed
by the HMM stimuli and weakest for the diphone stimuli. The effect
of the third feature was clearly observable for the natural stimuli
and less pronounced in the synthetic stimuli. This is presumably a
result of the partly insufficient perceptibility of this target feature
in the synthetic stimuli and demonstrates the necessity of gaining
fine-grained control over the synthesis output, should it be intended
to implement capabilities of phonetic convergence on the segmental
level in spoken dialogue systems.
</description>
    </item>
    
    <item>
        <title>Occupancy Detection in Commercial and Residential Environments Using Audio Signal</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0524.PDF</link>
        <description>Occupancy detection, including presence detection and head count, as
one of the fast growing areas plays an important role in providing
safety, comfort and reducing energy consumption both in residential
and commercial setups. The focus of this study is proposing affordable
strategies to increase occupancy detection performance in realistic
scenarios using only audio signal collected from the environment. We
use approximately 100-hour of audio data in residential and commercial
environments to analyze and evaluate our setup. In this study, we take
advantage of developments in feature selection methods to choose the
most relevant audio features for the task. Attribute and error vs.
human activity analysis are also performed to gain a better understanding
of the environmental sounds and possible solutions to enhance the performance.
Experimental results confirm the effectiveness of audio sensor for
occupancy detection using a cost effective system with presence detection
accuracy of 96% and 99%, and the head count accuracy of 70% and 95%
for the residential and commercial setups, respectively.
</description>
    </item>
    
    <item>
        <title>Data Augmentation, Missing Feature Mask and Kernel Classification for Through-the-Wall Acoustic Surveillance</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0685.PDF</link>
        <description>This paper deals with sound event classification from poor quality
signals in the context of &amp;#8220;through-the-wall&amp;#8221; (TTW) surveillance.
The task is extremely challenging due to the high level of distortion
and attenuation caused by complex sound propagation and modulation
effect from signal acquisition. Another problem, facing in TTW surveillance,
is the lack of comprehensive training data as the recording is much
more complicated than conventional approaches using audio microphones.
To address that challenge, we employ a recurrent neural network, particularly
the Long Short-Term Memory (LSTM) encoder, to transform conventional
clean and noisy audio signals into TTW signals to augment additional
training data. Furthermore, a novel missing feature mask kernel classification
is developed to optimize the classification accuracy of TTW sound event
classification. Particularly, Wasserstein distance is calculated from
reliable intersection regions between pair-wise sound image representations
and embedded into a probabilistic distance Support Vector Machine (SVM)
kernel to optimize the TTW data separation. The proposed missing feature
mask kernel allows effective training with inhomogeneously distorted
data and the experimental results show promising results on TTW audio
recordings, outperforming several state-of-art methods.
</description>
    </item>
    
    <item>
        <title>Endpoint Detection Using Grid Long Short-Term Memory Networks for Streaming Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0284.PDF</link>
        <description>The task of endpointing is to determine when the user has finished
speaking. This is important for interactive speech applications such
as voice search and Google Home. In this paper, we propose a GLDNN-based
(grid long short-term memory deep neural network) endpointer model
and show that it provides significant improvements over a state-of-the-art
CLDNN (convolutional, long short-term memory, deep neural network)
model. Specifically, we replace the convolution layer in the CLDNN
with a grid LSTM layer that models both spectral and temporal variations
through recurrent connections. Results show that the GLDNN achieves
32% relative improvement in false alarm rate at a fixed false reject
rate of 2%, and reduces median latency by 11%. We also include detailed
experiments investigating why grid LSTMs offer better performance than
convolution layers. Analysis reveals that the recurrent connection
along the frequency axis is an important factor that greatly contributes
to the performance of grid LSTMs, especially in the presence of background
noise. Finally, we also show that multichannel input further increases
robustness to background speech. Overall, we achieve 16% (100 ms) endpointer
latency improvement relative to our previous best model on a Voice
Search Task.
</description>
    </item>
    
    <item>
        <title>Deep Learning Techniques in Tandem with Signal Processing Cues for Phonetic Segmentation for Text to Speech Synthesis in Indian Languages</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0666.PDF</link>
        <description>Automatic detection of phoneme boundaries is an important sub-task
in building speech processing applications, especially text-to-speech
synthesis (TTS) systems. The main drawback of the Gaussian mixture
model - hidden Markov model (GMM-HMM) based forced-alignment is that
the phoneme boundaries are not explicitly modeled. In an earlier work,
we had proposed the use of signal processing cues in tandem with GMM-HMM
based forced alignment for boundary correction for building Indian
language TTS systems. In this paper, we capitalise on the ability of
robust acoustic modeling techniques such as deep neural networks (DNN)
and convolutional deep neural networks (CNN) for acoustic modeling.
The GMM-HMM based forced alignment is replaced by DNN-HMM/CNN-HMM based
forced alignment. Signal processing cues are used to correct the segment
boundaries obtained using DNN-HMM/CNN-HMM segmentation. TTS systems
built using these boundaries show a relative improvement in synthesis
quality.
</description>
    </item>
    
    <item>
        <title>Gate Activation Signal Analysis for Gated Recurrent Neural Networks and its Correlation with Phoneme Boundaries</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0877.PDF</link>
        <description>In this paper we analyze the gate activation signals inside the gated
recurrent neural networks, and find the temporal structure of such
signals is highly correlated with the phoneme boundaries. This correlation
is further verified by a set of experiments for phoneme segmentation,
in which better results compared to standard approaches were obtained.
</description>
    </item>
    
    <item>
        <title>Speaker Change Detection in Broadcast TV Using Bidirectional Long Short-Term Memory Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0065.PDF</link>
        <description>Speaker change detection is an important step in a speaker diarization
system. It aims at finding speaker change points in the audio stream.
In this paper, it is treated as a sequence labeling task and addressed
by Bidirectional long short term memory networks (Bi-LSTM). The system
is trained and evaluated on the Broadcast TV subset from ETAPE database.
The result shows that the proposed model brings good improvement over
conventional methods based on BIC and Gaussian Divergence. For instance,
in comparison to Gaussian divergence, it produces speech turns that
are 19.5% longer on average, with the same level of purity.
</description>
    </item>
    
    <item>
        <title>Improved Automatic Speech Recognition Using Subband Temporal Envelope Features and Time-Delay Neural Network Denoising Autoencoder</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1096.PDF</link>
        <description>This paper investigates the use of perceptually-motivated subband temporal
envelope (STE) features and time-delay neural network (TDNN) denoising
autoencoder (DAE) to improve deep neural network (DNN)-based automatic
speech recognition (ASR). STEs are estimated by full-wave rectification
and low-pass filtering of band-passed speech using a Gammatone filter-bank.
TDNNs are used either as DAE or acoustic models. ASR experiments are
performed on Aurora-4 corpus. STE features provide 2.2% and 3.7% relative
word error rate (WER) reduction compared to conventional log-mel filter-bank
(FBANK) features when used in ASR systems using DNN and TDNN as acoustic
models, respectively. Features enhanced by TDNN DAE are better recognized
with ASR system using DNN acoustic models than using TDNN acoustic
models. Improved ASR performance is obtained when features enhanced
by TDNN DAE are used in ASR system using DNN acoustic models. In this
scenario, using STE features provides 9.8% relative WER reduction compared
to when using FBANK features. 
</description>
    </item>
    
    <item>
        <title>Factored Deep Convolutional Neural Networks for Noise Robust Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0225.PDF</link>
        <description>In this paper, we present a framework of a factored deep convolutional
neural network (CNN) learning for noise robust automatic speech recognition
(ASR). Deep CNN architecture, which has attracted great attention in
various research areas, has also been successfully applied to ASR.
However, to ensure noise robustness, since merely introducing deep
CNN architecture into the acoustic modeling of ASR is insufficient,
we introduce factored network architecture into deep CNN-based acoustic
modeling. The proposed factored deep CNN framework factors out feature
enhancement, delta parameter learning, and hidden Markov model state
classification into three specific network blocks. By assigning specific
roles to each block, the noise robustness of deep CNN-based acoustic
models can be improved. With various comparative evaluations, we reveal
that the proposed method successfully improves ASR accuracies in noise
environments.
</description>
    </item>
    
    <item>
        <title>Global SNR Estimation of Speech Signals for Unknown Noise Conditions Using Noise Adapted Non-Linear Regression</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0230.PDF</link>
        <description>The performance of speech technologies deteriorates in the presence
of noise. Additionally, we need these technologies to be able to operate
across a variety of noise levels and conditions. SNR estimation can
guide the design and operation of such technologies or can be used
as a pre-processing tool in database creation (e.g. identify/discard
noisy signals). We propose a new method to estimate the global SNR
of a speech signal when prior information about the noise that corrupts
the signal, and speech boundaries within the signal, are not available.
To achieve this goal, we train a neural network that performs non-linear
regression to estimate the SNR. We use energy ratios as features, as
well as ivectors to provide information about the noise that corrupts
the signal. We compare our method against others in the literature,
using the Mean Absolute Error (MAE) metric, and show that our method
outperforms them consistently.
</description>
    </item>
    
    <item>
        <title>Joint Training of Multi-Channel-Condition Dereverberation and Acoustic Modeling of Microphone Array Speech for Robust Distant Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0579.PDF</link>
        <description>We propose a novel data utilization strategy, called multi-channel-condition
learning, leveraging upon complementary information captured in microphone
array speech to jointly train dereverberation and acoustic deep neural
network (DNN) models for robust distant speech recognition. Experimental
results, with a single automatic speech recognition (ASR) system, on
the REVERB2014 simulated evaluation data show that, on 1-channel testing,
the baseline joint training scheme attains a word error rate (WER)
of 7.47%, reduced from 8.72% for separate training. The proposed multi-channel-condition
learning scheme has been experimented on different channel data combinations
and usage showing many interesting implications. Finally, training
on all 8-channel data and with DNN-based language model rescoring,
a state-of-the-art WER of 4.05% is achieved. We anticipate an even
lower WER when combining more top ASR systems.
</description>
    </item>
    
    <item>
        <title>Uncertainty Decoding with Adaptive Sampling for Noise Robust DNN-Based Acoustic Modeling</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0793.PDF</link>
        <description>Although deep neural network (DNN) based acoustic models have obtained
remarkable results, the automatic speech recognition (ASR) performance
still remains low in noise and reverberant conditions. To address this
issue, a speech enhancement front-end is often used before recognition
to reduce noise. However, the front-end cannot fully suppress noise
and often introduces artifacts that are limiting the ASR performance
improvement. Uncertainty decoding has been proposed to better interconnect
the speech enhancement front-end and ASR back-end and mitigate the
mismatch caused by residual noise and artifacts. By considering features
as distributions instead of point estimates, the uncertainty decoding
approach modifies the conventional decoding rules to account for the
uncertainty emanating from the speech enhancement. Although the concept
of uncertainty decoding has been investigated for DNN acoustic models
recently, finding efficient ways to incorporate distribution of the
enhanced features within a DNN acoustic model still requires further
investigations. In this paper, we propose to parameterize the distribution
of the enhanced feature and estimate the parameters by backpropagation
using an unsupervised adaptation scheme. We demonstrate the effectiveness
of the proposed approach on real audio data of the CHiME3 dataset.
</description>
    </item>
    
    <item>
        <title>Attention-Based LSTM with Multi-Task Learning for Distant Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0805.PDF</link>
        <description>Distant speech recognition is a highly challenging task due to background
noise, reverberation, and speech overlap. Recently, there has been
an increasing focus on attention mechanism. In this paper, we explore
the attention mechanism embedded within the long short-term memory
(LSTM) based acoustic model for large vocabulary distant speech recognition,
trained using speech recorded from a single distant microphone (SDM)
and multiple distant microphones (MDM). Furthermore, multi-task learning
architecture is incorporated to improve robustness in which the network
is trained to perform both a primary senone classification task and
a secondary feature enhancement task. Experiments were conducted on
the AMI meeting corpus. On average our model achieved 3.3% and 5.0%
relative improvements in word error rate (WER) over the LSTM baseline
model in the SDM and MDM cases, respectively. In addition, the model
provided between a 2&amp;#8211;4% absolute WER reduction compared to a
conventional pipeline of independent processing stage on the MDM task.
</description>
    </item>
    
    <item>
        <title>To Improve the Robustness of LSTM-RNN Acoustic Models Using Higher-Order Feedback from Multiple Histories</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1315.PDF</link>
        <description>This paper investigates a novel multiple-history long short-term memory
(MH-LSTM) RNN acoustic model to mitigate the robustness problem of
noisy outputs in the form of mis-labeled data and/or mis-alignments.
Conceptually, after an RNN is unfolded in time, the hidden units in
each layer are re-arranged into ordered sub-layers with a master sub-layer
on top and a set of auxiliary sub-layers below it. Only the master
sub-layer generates outputs for the next layer whereas the auxiliary
sub-layers run in parallel with the master sub-layer but with increasing
time lags. Each sub-layer also receives higher-order feedback from
a fixed number of sub-layers below it. As a result, each sub-layer
maintains a different history of the input speech, and the ensemble
of all the different histories lends itself to the model&amp;#8217;s robustness.
The higher-order connections not only provide shorter feedback paths
for error signals to propagate to the farther preceding hidden states
to better model the long-term memory, but also more feedback paths
to each model parameter and smooth its update during training. Phoneme
recognition results on both real TIMIT data as well as synthetic TIMIT
data with noisy labels or alignments show that the new model outperforms
the conventional LSTM RNN model.
</description>
    </item>
    
    <item>
        <title>End-to-End Speech Recognition with Auditory Attention for Multi-Microphone Distance Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1536.PDF</link>
        <description>End-to-End speech recognition is a recently proposed approach that
directly transcribes input speech to text using a single model. End-to-End
speech recognition methods including Connectionist Temporal Classification
and Attention-based Encoder Decoder Networks have been shown to obtain
state-of-the-art performance on a number of tasks and significantly
simplify the modeling, training and decoding procedures for speech
recognition. In this paper, we extend our prior work on End-to-End
speech recognition focusing on the effectiveness of these models in
far-field environments. Specifically, we propose introducing Auditory
Attention to integrate input from multiple microphones directly within
an End-to-End speech recognition model, leveraging the attention mechanism
to dynamically tune the model&amp;#8217;s attention to the most reliable
input sources. We evaluate our proposed model on the CHiME-4 task,
and show substantial improvement compared to a model optimized for
a single microphone input.
</description>
    </item>
    
    <item>
        <title>Robust Speech Recognition Based on Binaural Auditory Processing</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1665.PDF</link>
        <description>This paper discusses a combination of techniques for improving speech
recognition accuracy in the presence of reverberation and spatially-separated
interfering sound sources. Interaural Time Delay (ITD), observed as
a consequence of the difference in arrival times of a sound to the
two ears, is an important feature used by the human auditory system
to reliably localize and separate sound sources. In addition, the &amp;#8220;precedence
effect&amp;#8221; helps the auditory system differentiate between the direct
sound and its subsequent reflections in reverberant environments. This
paper uses a cross-correlation-based measure across the two channels
of a binaural signal to isolate the target source by rejecting portions
of the signal corresponding to larger ITDs. To overcome the effects
of reverberation, the steady-state components of speech are suppressed,
effectively boosting the onsets, so as to retain the direct sound and
suppress the reflections. Experimental results show a significant improvement
in recognition accuracy using both these techniques. Cross-correlation-based
processing and steady-state suppression are carried out separately,
and the order in which these techniques are applied produces differences
in the resulting recognition accuracy.
</description>
    </item>
    
    <item>
        <title>Adaptive Multichannel Dereverberation for Automatic Speech Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1791.PDF</link>
        <description>Reverberation is known to degrade the performance of automatic speech
recognition (ASR) systems dramatically in far-field conditions. Adopting
the weighted prediction error (WPE) approach, we formulate an online
dereverberation algorithm for a multi-microphone array. The key contributions
of this paper are: (a) we demonstrate that dereverberation using WPE
improves performance even when the acoustic models are trained using
multi-style training (MTR) with noisy, reverberated speech; (b) we
show that the gains from WPE are preserved even in large and diverse
real-world data sets; (c) we propose an adaptive version for online
multichannel ASR tasks which gives similar gains as the non-causal
version; and (d) while the algorithm can just be applied for evaluation,
we show that also including dereverberation during training gives increased
performance gains. We also report how different parameter settings
of the dereverberation algorithm impacts the ASR performance.
</description>
    </item>
    
    <item>
        <title>The Effects of Real and Placebo Alcohol on Deaffrication</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1579.PDF</link>
        <description>The more alcohol a person has consumed, the more mispronunciations
occur. This study investigates how deaffrication surfaces in Bernese
Swiss German when speakers are moderately intoxicated (0.05&amp;#8211;0.08%
Vol.), whether these effects can be hidden, and whether a placebo effect
interacting with mispronunciation occurs. Five participants reading
a text were recorded as follows. In stage I, they read the text before
and after drinking placebo alcohol, and finally again after being told
to enunciate very clearly. 3&amp;#8211;7 days later, the same experiment
was repeated with real alcohol. The recordings were then analysed with
 Praat. Despite interspeaker variation, the following generalisations
can be made. The most deaffrication occurs in the C_C context both
when speakers are sober and inebriated; affricates in _#, V_C, and
V_V position encounter more deaffrication in the alcohol stage; and
/&amp;#x361;t&amp;#643;/ and &amp;#x361;kx are deaffricated more when the speaker
is intoxicated, with /&amp;#x361;t&amp;#643;/ being the most susceptible to
mispronunciation. Moreover, when alcohol is consumed, more deaffrication
occurs, which cannot consciously be controlled. Furthermore, a statistically
significant difference between the pre- and the post-placebo-drinking
experiment could be found, which implies that a placebo effect takes
place. Nevertheless, the effects of real alcohol are considerably stronger.
</description>
    </item>
    
    <item>
        <title>Polyglot and Speech Corpus Tools: A System for Representing, Integrating, and Querying Speech Corpora</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1390.PDF</link>
        <description>Speech datasets from many languages, styles, and sources exist in the
world, representing significant potential for scientific studies of
speech &amp;#8212; particularly given structural similarities among all
speech datasets. However, studies using multiple speech corpora remain
difficult in practice, due to corpus size, complexity, and differing
formats. We introduce open-source software for  unified corpus analysis:
integrating speech corpora and querying across them. Corpora are stored
in a custom &amp;#8216;polyglot persistence&amp;#8217; scheme that combines
three sub-databases mirroring different data types: a Neo4j graph database
to represent temporal annotation graph structure, and SQL and InfluxDB
databases to represent meta- and acoustic data. This scheme abstracts
away from the idiosyncratic formats of different speech corpora, while
mirroring the structure of different data types improves speed and
scalability. A Python API and a GUI both allow for: enriching the database
with positional, hierarchical, temporal, and signal measures (e.g.
utterance boundaries, f0) that are useful for linguistic analysis;
querying the database using a simple query language; and exporting
query results to standard formats for further analysis. We describe
the software, summarize two case studies using it to examine effects
on pitch and duration across languages, and outline planned future
development.
</description>
    </item>
    
    <item>
        <title>Mapping Across Feature Spaces in Forensic Voice Comparison: The Contribution of Auditory-Based Voice Quality to (Semi-)Automatic System Testing</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1508.PDF</link>
        <description>In forensic voice comparison, there is increasing focus on the integration
of automatic and phonetic methods to improve the validity and reliability
of voice evidence to the courts. In line with this, we present a comparison
of long-term measures of the speech signal to assess the extent to
which they capture complementary speaker-specific information. Likelihood
ratio-based testing was conducted using MFCCs and (linear and Mel-weighted)
long-term formant distributions (LTFDs). Fusing automatic and semi-automatic
systems yielded limited improvement in performance over the baseline
MFCC system, indicating that these measures capture essentially the
same speaker-specific information. The output from the best performing
system was used to evaluate the contribution of auditory-based analysis
of supralaryngeal (filter) and laryngeal (source) voice quality in
system testing. Results suggest that the problematic speakers for the
(semi-)automatic system are, to some extent, predictable from their
supralaryngeal voice quality profiles, with the least distinctive speakers
producing the weakest evidence and most misclassifications. However,
the misclassified pairs were still easily differentiated via auditory
analysis. Laryngeal voice quality may thus be useful in resolving problematic
pairs for (semi-)automatic systems, potentially improving their overall
performance.
</description>
    </item>
    
    <item>
        <title>Effect of Language, Speaking Style and Speaker on Long-Term F0 Estimation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Stability of Prosodic Characteristics Across Age and Gender Groups</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1503.PDF</link>
        <description>The indexical function of speech prosody signals the membership of
a speaker in a social group. The factors of age and gender are relatively
easy to establish but their reflection in speech characteristics can
be less straightforward as they interact with other social aspects.
Therefore, diverse speaker communities should be investigated with
the aim of their subsequent comparison. Our study provides data for
the population of adult speakers of Czech &amp;#8212; a West Slavic language
of Central Europe. The sample consists of six age groups (20 to 80
years of age) with balanced representation of gender. The search for
age and gender related attributes covered both global acoustic descriptors
and linguistically informed prosodic feature extraction. Apart from
commonly used measures and methods we also exploited Legendre polynomials,
k-means clustering and a newly designed Cumulative Slope Index (CSI).
The results specify general deceleration of articulation rate with
age and lowering of F0 in aging Czech women, and reveal an increase
in CSI of both F0 tracks and intensity curves with age. Furthermore,
various melodic shapes were found to be distributed unequally across
the age groups.
</description>
    </item>
    
    <item>
        <title>Electrophysiological Correlates of Familiar Voice Recognition</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1392.PDF</link>
        <description>Our previous work using voice lineups has established that listeners
can recognize with near-perfect accuracy the voice of familiar individuals.
In a forensic perspective, however, there are limitations to the application
of voice lineups in that some witnesses may not wish to recognize the
familiar voice of a parent or close friend or else provide unreliable
responses. Considering this problem, the present study aimed to isolate
the electrophysiological markers of voice familiarity. We recorded
the evoked response potentials (ERPs) of 11 participants as they listened
to a set of similar voices in varying utterances (standards of voice
line ups were used in selecting voices). Within the presented set,
only one voice was familiar to the listener (the voice of a parent,
close friend, etc.). The ERPs showed a marked difference for heard
familiar voices compared to an unfamiliar set. These are the first
findings of a neural marker of voice recognition based on voices that
are actually familiar to a listener and which take into account utterances
rather than isolated vowels. The present results thus indicate that
protocols of near-perfect voice recognition can be devised without
using behavioral responses.
</description>
    </item>
    
    <item>
        <title>Developing an Embosi (Bantu C25) Speech Variant Dictionary to Model Vowel Elision and Morpheme Deletion</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1280.PDF</link>
        <description>This paper investigates vowel elision and morpheme deletion in Embosi
(Bantu C25), an under-resourced language spoken in the Republic of
Congo. We propose that the observed morpheme deletion is morphological,
and that vowel elision is phonological. The study focuses on vowel
elision that occurs across word boundaries between the contact of long/short
vowels (i.e. CV[long] # V[short].CV), and between the contact of short/short
vowels (CV[short] # V[short].CV). Several different categories of morphemes
are explored: (i) prepositions ( ya, mo), (ii) class-noun nominal prefixes
( ba, etc.), (iii) singular subject pronouns ( ng&amp;#225;, n&amp;#596;, wa).
For example, the preposition,  ya, regularly deletes allowing for vowel
elision if vowel contact occurs between the head of the noun phrase
and the previous word. Phonetically motivated speech variants are proposed
in the lexicon used for forced alignment (segmentation) enabling these
phenomena to be quantified in the corpus so as to develop a dictionary
containing relevant phonetic variants.
</description>
    </item>
    
    <item>
        <title>R<SUB>d</SUB> as a Control Parameter to Explore Affective Correlates of the Tense-Lax Continuum</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Cross-Linguistic Distinctions Between Professional and Non-Professional Speaking Styles</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0007.PDF</link>
        <description>This work investigates acoustic and perceptual differences in four
language varieties by using a corpus of professional and non-professional
speaking styles. The professional stimuli are composed of excerpts
of broadcast news and political discourses from six subjects in each
case. The non-professional stimuli are made up of recordings of 10
subjects who read a long story and narrated it subsequently. All this
material was obtained in four language varieties: Brazilian and European
Portuguese, standard French and German. The corpus is balanced for
gender. Eight melodic and intensity parameters were automatically obtained
from excerpts of 10 to 20 seconds. We showed that 6 out of 8 parameters
partially distinguish professional from non-professional style in the
four language varieties. Classification and discrimination tests carried
out with 12 Brazilian listeners using delexicalised speech showed that
these subjects are able to distinguish professional style from non-professional
style with about 2/3 of hits irrespective of language. In comparison,
an automatic classification using an LDA model performed better in
classifying non-professional (96%) against professional styles, but
not in classifying professional (42%) against non-professional styles.
</description>
    </item>
    
    <item>
        <title>Perception and Production of Word-Final /&#x281;/ in French</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Glottal Source Estimation from Coded Telephone Speech Using a Deep Neural Network</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0882.PDF</link>
        <description>In speech analysis, the information about the glottal source is obtained
from speech by using glottal inverse filtering (GIF). The accuracy
of state-of-the-art GIF methods is sufficiently high when the input
speech signal is of high-quality (i.e., with little noise or reverberation).
However, in realistic conditions, particularly when GIF is computed
from coded telephone speech, the accuracy of GIF methods deteriorates
severely. To robustly estimate the glottal source under coded condition,
a deep neural network (DNN)-based method is proposed. The proposed
method utilizes a DNN to map the speech features extracted from the
coded speech to the glottal flow waveform estimated from the corresponding
clean speech. To generate the coded telephone speech, adaptive multi-rate
(AMR) codec is utilized which is a widely used speech compression method.
The proposed glottal source estimation method is compared with two
existing GIF methods, closed phase covariance analysis (CP) and iterative
adaptive inverse filtering (IAIF). The results indicate that the proposed
DNN-based method is capable of estimating glottal flow waveforms from
coded telephone speech with a considerably better accuracy in comparison
to CP and IAIF.
</description>
    </item>
    
    <item>
        <title>Automatic Labelling of Prosodic Prominence, Phrasing and Disfluencies in French Speech by Simulating the Perception of Na&amp;#239;ve and Expert Listeners</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0971.PDF</link>
        <description>We explore the use of machine learning techniques (notably SVM classifiers
and Conditional Random Fields) to automate the prosodic labelling of
French speech, based on modelling and simulating the perception of
prosodic events by na&amp;#239;ve and expert listeners. The models are
based on previous work on the perception of syllabic prominence and
hesitation-related disfluencies, and on an experiment on the real-time
perception of prosodic boundaries. Expert and non-expert listeners
annotated samples from three multi-genre corpora (CPROM, CPROM-PFC,
LOCAS-F). Automatic prosodic annotation is approached as a sequence
labelling problem, drawing on multiple information sources (acoustic
features, lexical and shallow syntactic features) in accordance with
the experimental findings showing that listeners integrate all such
information in their perception of prosodic segmentation and events.
We test combinations of features and machine learning methods, and
we compare the automatic labelling with expert annotation. The result
of this study is a tool that automatically annotates prosodic events
by simulating the perception of expert and na&amp;#239;ve listeners.
</description>
    </item>
    
    <item>
        <title>Don&amp;#8217;t Count on ASR to Transcribe for You: Breaking Bias with Two Crowds</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0164.PDF</link>
        <description>A crowdsourcing approach for collecting high-quality speech transcriptions
is presented. The approach addresses typical weakness of traditional
semi-supervised transcription strategies that show ASR hypotheses to
transcribers to help them cope with unclear or ambiguous audio and
speed up transcriptions. We explain how the traditional methods introduce
bias into transcriptions that make it difficult to objectively measure
system improvements against existing baselines, and suggest a two-stage
crowdsourcing alternative that, first, iteratively collects transcription
hypotheses and, then, asks a different crowd to pick the best of them.
We show that this alternative not only outperforms the traditional
method in a side-by-side comparison, but it also leads to ASR improvements
due to superior quality of acoustic and language models trained on
the transcribed data.
</description>
    </item>
    
    <item>
        <title>Effects of Training Data Variety in Generating Glottal Pulses from Acoustic Features with DNNs</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0363.PDF</link>
        <description>Glottal volume velocity waveform, the acoustical excitation of voiced
speech, cannot be acquired through direct measurements in normal production
of continuous speech. Glottal inverse filtering (GIF), however, can
be used to estimate the glottal flow from recorded speech signals.
Unfortunately, the usefulness of GIF algorithms is limited since they
are sensitive to noise and call for high-quality recordings. Recently,
efforts have been taken to expand the use of GIF by training deep neural
networks (DNNs) to learn a statistical mapping between frame-level
acoustic features and glottal pulses estimated by GIF. This framework
has been successfully utilized in statistical speech synthesis in the
form of the GlottDNN vocoder which uses a DNN to generate glottal pulses
to be used as the synthesizer&amp;#8217;s excitation waveform. In this
study, we investigate how the DNN-based generation of glottal pulses
is affected by training data variety. The evaluation is done using
both objective measures as well as subjective listening tests of synthetic
speech. The results suggest that the performance of the glottal pulse
generation with DNNs is affected particularly by how well the training
corpus suits GIF: processing low-pitched male speech and sustained
phonations shows better performance than processing high-pitched female
voices or continuous speech.
</description>
    </item>
    
    <item>
        <title>Towards Intelligent Crowdsourcing for Audio Data Annotation: Integrating Active Learning in the Real World</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0406.PDF</link>
        <description>In this contribution, we combine the advantages of traditional crowdsourcing
with contemporary machine learning algorithms with the aim of ultimately
obtaining reliable training data for audio processing in a faster,
cheaper and therefore more efficient manner than has been previously
possible. We propose a novel crowdsourcing approach, which brings a
simulated active learning annotation scenario into a real world environment
creating an intelligent and gamified crowdsourcing platform for manual
audio annotation. Our platform combines two active learning query strategies
with an internally calculated trustability score to efficiently reduce
manual labelling efforts. This reduction is achieved in a twofold manner:
first our system automatically decides if an instance requires annotation;
second, it dynamically decides, depending on the quality of previously
gathered annotations, on exactly how many annotations are needed to
reliably label an instance. Results presented indicate that our approach
drastically reduces the annotation load and is considerably more efficient
than conventional methods.
</description>
    </item>
    
    <item>
        <title>Principles for Learning Controllable TTS from Annotated and Latent Variation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0171.PDF</link>
        <description>For building flexible and appealing high-quality speech synthesisers,
it is desirable to be able to accommodate and reproduce fine variations
in vocal expression present in natural speech. Synthesisers can enable
control over such output properties by adding adjustable control parameters
in parallel to their text input. If not annotated in training data,
the values of these control inputs can be optimised jointly with the
model parameters. We describe how this established method can be seen
as approximate maximum likelihood and MAP inference in a latent variable
model. This puts previous ideas of (learned) synthesiser inputs such
as sentence-level control vectors on a more solid theoretical footing.
We furthermore extend the method by restricting the latent variables
to orthogonal subspaces via a sparse prior. This enables us to learn
dimensions of variation present also within classes in coarsely annotated
speech. As an example, we train an LSTM-based TTS system to learn nuances
in emotional expression from a speech database annotated with seven
different acted emotions. Listening tests show that our proposal successfully
can synthesise speech with discernible differences in expression within
each emotion, without compromising the recognisability of synthesised
emotions compared to an identical system without learned nuances.
</description>
    </item>
    
    <item>
        <title>Sampling-Based Speech Parameter Generation Using Moment-Matching Networks</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0362.PDF</link>
        <description>This paper presents sampling-based speech parameter generation using
moment-matching networks for Deep Neural Network (DNN)-based speech
synthesis. Although people never produce exactly the same speech even
if we try to express the same linguistic and para-linguistic information,
typical statistical speech synthesis produces completely the same speech,
i.e., there is no inter-utterance variation in synthetic speech. To
give synthetic speech natural inter-utterance variation, this paper
builds DNN acoustic models that make it possible to randomly sample
speech parameters. The DNNs are trained so that they make the moments
of generated speech parameters close to those of natural speech parameters.
Since the variation of speech parameters is compressed into a low-dimensional
simple prior noise vector, our algorithm has lower computation cost
than direct sampling of speech parameters. As the first step towards
generating synthetic speech that has natural inter-utterance variation,
this paper investigates whether or not the proposed sampling-based
generation deteriorates synthetic speech quality. In evaluation, we
compare speech quality of conventional maximum likelihood-based generation
and proposed sampling-based generation. The result demonstrates the
proposed generation causes no degradation in speech quality.
</description>
    </item>
    
    <item>
        <title>Unit Selection with Hierarchical Cascaded Long Short Term Memory Bidirectional Recurrent Neural Nets</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0428.PDF</link>
        <description>Bidirectional recurrent neural nets have demonstrated state-of-the-art
performance for parametric speech synthesis. In this paper, we introduce
a top-down application of recurrent neural net models to unit-selection
synthesis. A hierarchical cascaded network graph predicts context phone
duration, speech unit encoding and frame-level logF0 information that
serves as targets for the search of units. The new approach is compared
with an existing state-of-art hybrid system that uses Hidden Markov
Models as basis for the statistical unit search.
</description>
    </item>
    
    <item>
        <title>Utterance Selection for Optimizing Intelligibility of TTS Voices Trained on ASR Data</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0465.PDF</link>
        <description>This paper describes experiments in training HMM-based text-to-speech
(TTS) voices on data collected for Automatic Speech Recognition (ASR)
training. We compare a number of filtering techniques designed to identify
the best utterances from a noisy, multi-speaker corpus for training
voices, to exclude speech containing noise and to include speech close
in nature to more traditionally-collected TTS corpora. We also evaluate
the use of automatic speech recognizers for intelligibility assessment
in comparison with crowdsourcing methods. While the goal of this work
is to develop natural-sounding and intelligible TTS voices in Low Resource
Languages (LRLs) rapidly and easily, without the expense of recording
data specifically for this purpose, we focus on English initially to
identify the best filtering techniques and evaluation methods. We find
that, when a large amount of data is available, selecting from the
corpus based on criteria such as standard deviation of f0, fast speaking
rate, and hypo-articulation produces the most intelligible voices.
</description>
    </item>
    
    <item>
        <title>Bias and Statistical Significance in Evaluating Speech Synthesis with Mean Opinion Scores</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Phase Modeling Using Integrated Linear Prediction Residual for Statistical Parametric Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0587.PDF</link>
        <description>The conventional statistical parametric speech synthesis (SPSS) focus
on characteristics of the magnitude spectrum of speech for speech synthesis
by ignoring phase characteristics of speech. In this work, the role
of phase information to improve the naturalness of synthetic speech
is explored. The phase characteristics of excitation signal are estimated
from the integrated linear prediction residual (ILPR) using an all-pass
(AP) filter. The coefficients of the AP filter are estimated by minimizing
an entropy based objective function from the cosine phase of the analytical
signal obtained from ILPR signal. The AP filter coefficients (APCs)
derived from the AP filter are used as features for modeling phase
in SPSS. During synthesis time, to generate the excitation signal,
frame wise generated APCs are used to add the group delay to the impulse
excitation. The proposed method is compared with the group delay based
phase excitation used in the STRAIGHT method. The experimental results
show that proposed phased modeling having a better perceptual synthesis
quality when compared with the STRAIGHT method.
</description>
    </item>
    
    <item>
        <title>Evaluation of a Silent Speech Interface Based on Magnetic Sensing and Deep Learning for a Phonetically Rich Vocabulary</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0802.PDF</link>
        <description>To help people who have lost their voice following total laryngectomy,
we present a speech restoration system that produces audible speech
from articulator movement. The speech articulators are monitored by
sensing changes in magnetic field caused by movements of small magnets
attached to the lips and tongue. Then, articulator movement is mapped
to a sequence of speech parameter vectors using a transformation learned
from simultaneous recordings of speech and articulatory data. In this
work, this transformation is performed using a type of recurrent neural
network (RNN) with fixed latency, which is suitable for real-time processing.
The system is evaluated on a phonetically-rich database with simultaneous
recordings of speech and articulatory data made by non-impaired subjects.
Experimental results show that our RNN-based mapping obtains more accurate
speech reconstructions (evaluated using objective quality metrics and
a listening test) than articulatory-to-acoustic mappings using Gaussian
mixture models (GMMs) or deep neural networks (DNNs). Moreover, our
fixed-latency RNN architecture provides comparable performance to an
utterance-level batch mapping using bidirectional RNNs (BiRNNs).
</description>
    </item>
    
    <item>
        <title>Predicting Head Pose from Speech with a Conditional Variational Autoencoder</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Real-Time Reactive Speech Synthesis: Incorporating Interruptions</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1250.PDF</link>
        <description>The ability to be interrupted and react in a realistic manner is a
key requirement for interactive speech interfaces. While previous systems
have long implemented techniques such as &amp;#8216;barge in&amp;#8217; where
speech output can be halted at word or phrase boundaries, less work
has explored how to mimic human speech output responses to real-time
events like interruptions which require a reaction from the system.
Unlike previous work which has focused on incremental production, here
we explore a novel re-planning approach. The proposed system is versatile
and offers a large range of possible ways to react. A focus group was
used to evaluate the approach, where participants interacted with a
system reading out a text. The system would react to audio interruptions,
either with no reactions, passive reactions, or active negative reactions
(i.e. getting increasingly irritated). Participants preferred a reactive
system.
</description>
    </item>
    
    <item>
        <title>A Neural Parametric Singing Synthesizer</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1420.PDF</link>
        <description>We present a new model for singing synthesis based on a modified version
of the WaveNet architecture. Instead of modeling raw waveform, we model
features produced by a parametric vocoder that separates the influence
of pitch and timbre. This allows conveniently modifying pitch to match
any target melody, facilitates training on more modest dataset sizes,
and significantly reduces training and generation times. Our model
makes frame-wise predictions using mixture density outputs rather than
categorical outputs in order to reduce the required parameter count.
As we found overfitting to be an issue with the relatively small datasets
used in our experiments, we propose a method to regularize the model
and make the autoregressive generation process more robust to prediction
errors. Using a simple multi-stream architecture, harmonic, aperiodic
and voiced/unvoiced components can all be predicted in a coherent manner.
We compare our method to existing parametric statistical and state-of-the-art
concatenative methods using quantitative metrics and a listening test.
While naive implementations of the autoregressive generation algorithm
tend to be inefficient, using a smart algorithm we can greatly speed
up the process and obtain a system that&amp;#8217;s competitive in both
speed and quality.
</description>
    </item>
    
    <item>
        <title>Tacotron: Towards End-to-End Speech Synthesis</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1452.PDF</link>
        <description>A text-to-speech synthesis system typically consists of multiple stages,
such as a text analysis frontend, an acoustic model and an audio synthesis
module. Building these components often requires extensive domain expertise
and may contain brittle design choices. In this paper, we present Tacotron,
an end-to-end generative text-to-speech model that synthesizes speech
directly from characters. Given &amp;#60;text, audio&amp;#62; pairs, the model
can be trained completely from scratch with random initialization.
We present several key techniques to make the sequence-to-sequence
framework perform well for this challenging task. Tacotron achieves
a 3.82 subjective 5-scale mean opinion score on US English, outperforming
a production parametric system in terms of naturalness. In addition,
since Tacotron generates speech at the frame level, it&amp;#8217;s substantially
faster than sample-level autoregressive methods.
</description>
    </item>
    
    <item>
        <title>Siri On-Device Deep Learning-Guided Unit Selection Text-to-Speech System</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/1798.PDF</link>
        <description>This paper describes Apple&amp;#8217;s hybrid unit selection speech synthesis
system, which provides the voices for Siri with the requirement of
naturalness, personality and expressivity. It has been deployed into
hundreds of millions of desktop and mobile devices (e.g. iPhone, iPad,
Mac, etc.) via iOS and macOS in multiple languages. The system is following
the classical unit selection framework with the advantage of using
deep learning techniques to boost the performance. In particular, deep
and recurrent mixture density networks are used to predict the target
and concatenation reference distributions for respective costs during
unit selection. In this paper, we present an overview of the run-time
TTS engine and the voice building process. We also describe various
techniques that enable on-device capability such as preselection optimization,
caching for low latency, and unit pruning for low footprint, as well
as techniques that improve the naturalness and expressivity of the
voice such as the use of long units.
</description>
    </item>
    
    <item>
        <title>An Expanded Taxonomy of Semiotic Classes for Text Normalization</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0402.PDF</link>
        <description>We describe an expanded taxonomy of semiotic classes for text normalization,
building upon the work in [1]. We add a large number of categories
of non-standard words (NSWs) that we believe a robust real-world text
normalization system will have to be able to process. Our new categories
are based upon empirical findings encountered while building text normalization
systems across many languages, for both speech recognition and speech
synthesis purposes. We believe our new taxonomy is useful both for
ensuring high coverage when writing manual grammars, as well as for
eliciting training data to build machine learning-based text normalization
systems.
</description>
    </item>
    
    <item>
        <title>Complex-Valued Restricted Boltzmann Machine for Direct Learning of Frequency Spectra</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/</link>
        <description>'NoneType' object has no attribute 'group'</description>
    </item>
    
    <item>
        <title>Soundtracing for Realtime Speech Adjustment to Environmental Conditions in 3D Simulations</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2002.PDF</link>
        <description>We present a 3D realtime audio engine which utilizes frustum tracing
to create realistic audio auralization, modifying speech in architectural
walkthroughs. All audio effects are computed based on both the geometrical
(e.g. walls, furniture) and acoustical scene properties (e.g. materials,
air attenuation). The sound changes dynamically as we change the point
of perception and sound sources. The engine can be configured to use
as little as 10 percent of available processing power. Our demonstration
will be based on listening radio samples in rooms with similar shape,
but different acoustical properties. The described system is a component
of a virtual reality trainer for firefighters using Oculus Rift. It
allows to conduct dialogues with victims and to locate them based on
sound cues.
</description>
    </item>
    
    <item>
        <title>Vocal-Tract Model with Static Articulators: Lips, Teeth, Tongue, and More</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2027.PDF</link>
        <description>Our physical models of the human vocal tract successfully demonstrate
theories such as the source-filter theory of speech production, mechanisms
such as the relationship between vocal-tract configuration and vowel
quality, and phenomena such as formant frequency estimation. Earlier
models took one of two directions: either simplification, showing only
a few target themes, or diversification, simulating human articulation
more broadly. In this study, we have designed a static, hybrid model.
Each model of this type produces one vowel. However, the model also
simulates the human articulators more broadly, including the lips,
teeth, and tongue. The sagittal block is enclosed with transparent
plates so that the inside of the vocal tract is visible from the outside.
We also colored the articulators to make them more easily identified.
In testing, we confirmed that the vocal-tract models can produce the
target vowel. These models have great potential, with applications
not only in acoustics and phonetics education, but also pronunciation
training in language learning and speech therapy in the clinical setting.
</description>
    </item>
    
    <item>
        <title>Remote Articulation Test System Based on WebRTC</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2038.PDF</link>
        <description>A remote articulation test system with multimedia communication has
been developed in order that outside speech-language-hearing therapists
(STs) can exam pronunciations of the students in special education
classes in regular elementary schools and give advice to their teachers.
The proposed system has video and voice communication and image transmission
functions based on WebRTC. Using image transmission, the ST presents
picture cards for the word test to the student and asks what is depicted.
Using video / voice communication, the ST confirms the student&amp;#8217;s
voice and articulation movement. Compared to our previous system in
which written words were presented, the proposed system enables a more
formal and accurate articulation test.
</description>
    </item>
    
    <item>
        <title>The ModelTalker Project: A Web-Based Voice Banking Pipeline for ALS/MND Patients</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2054.PDF</link>
        <description>The Nemours ModelTalker supports  voice banking for users diagnosed
with ALS/MND and related neurodegenerative diseases. Users record up
to 1600 sentences from which a synthetic voice is constructed. For
the past two years we have focused on extending and refining a web-based
recording tool to support this process. In this demonstration, we illustrate
the features of the web-based pipeline that guides patients through
the process of setting up to record at home, recording a standard speech
inventory, adding custom recordings, and screening alternative versions
of their voice and alternative synthesis parameter settings. Finally,
we summarize results from 352 individuals with a wide range of speaking
ability, who have recently used this voice banking pipeline. 
</description>
    </item>
    
    <item>
        <title>Visible Vowels: A Tool for the Visualization of Vowel Variation</title>
        <link>https://www.isca-speech.org/archive/Interspeech_2017/pdfs/2055.PDF</link>
        <description>This paper presents Visible Vowels, a web app that visualizes variation
in f0, formants and duration. It combines user friendliness with maximum
functionality and flexibility, using a live plot view.
</description>
    </item>
    
</channel>
</rss>